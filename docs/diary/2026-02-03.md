# 2026-02-03: Day 3 — ELF バイナリのロードと実行

Day 2 でユーザーモード (Ring 3) + プロセス分離まで実装した。でもユーザープログラムはカーネルバイナリに Rust 関数として埋め込まれていて、独立したバイナリではなかった。今日はいよいよ外部の ELF バイナリをカーネルにロードして Ring 3 で実行できるようにする。

## 追加タスク: procfs 最小実装

ユーザーランドからカーネル情報を読むために、procfs（カーネル内部情報を疑似ファイルとして見せる仕組み）を最小構成で試す。

### 実装計画（変更ファイル・変更内容）

1. `kernel/src/syscall.rs`
   - `/proc/meminfo` と `/proc/tasks` を読み取りできる procfs の最小実装を追加
   - `SYS_FILE_READ` / `SYS_DIR_LIST` で `/proc` を処理する分岐を追加
2. `kernel/src/shell.rs`
   - selftest に procfs の読み取りテストを追加
3. `docs/diary/2026-02-03.md`
   - 実装ログと学びを追記

## 今日のゴール

1. **ユーザープログラム crate の作成**: `x86_64-unknown-none` ターゲットで独立した ELF64 バイナリを生成
2. **ELF パーサー**: ELF64 ヘッダーとプログラムヘッダーをパースして LOAD セグメントを抽出
3. **ELF ローダー**: LOAD セグメントをプロセスのアドレス空間にマッピングしてデータをコピー
4. **シェルコマンド**: `elf` コマンドで ELF バイナリをロード・実行

## 設計判断

### ELF バイナリの配置方法

UEFI Boot Services を終了するとファイルシステムにアクセスできなくなるため、`include_bytes!` でカーネルバイナリにコンパイル時に埋め込む方式にした。将来的にファイルシステムを実装すればディスクから読み込む方式に切り替えられる。

### ターゲットの選択

ユーザープログラムは `x86_64-unknown-none` ターゲットでビルドする。これは OS を持たないベアメタル環境向けの ELF64 バイナリを出力する。カーネルの `x86_64-unknown-uefi` (PE 形式) とは別のフォーマットで、リンカスクリプトで `.text` セクションを `0x400000` (4MiB) に配置する。

### なぜ 0x400000 なのか

Linux の ELF ローダーがデフォルトで使うベースアドレスが `0x400000`。歴史的にこのアドレスが使われてきた理由は、低アドレス領域（0x0〜0x400000）をカーネルやヌルポインタガードに使えるように空けておくため。今回もこの慣習に従った。

### ABI の違い

重要な設計上のポイントとして、カーネル (`x86_64-unknown-uefi`) は **Microsoft x64 ABI** を使うのに対し、ユーザープログラム (`x86_64-unknown-none`) は **System V AMD64 ABI** を使う。システムコールのアセンブリでは System V 規約（rax=syscall番号, rdi=引数1, rsi=引数2）でレジスタをセットし、カーネル側のハンドラが Microsoft ABI の引数レジスタ（rcx, rdx, r8）に変換する。

## 実装ログ

### ユーザープログラム crate (`user/`)

`user/` ディレクトリに独立した Rust crate を作成した。`#![no_std]` + `#![no_main]` で OS なし環境向けのバイナリを生成する。

エントリポイント `_start` は `extern "C"` リンケージで、System V ABI に従う。`int 0x80` でシステムコールを呼び、SYS_WRITE (1) で "Hello from ELF binary!\n" を出力して SYS_EXIT (60) で終了する。

リンカスクリプト (`linker.ld`) で `.text` を `0x400000` に配置:

```ld
SECTIONS {
    . = 0x400000;
    .text : { *(.text .text.*) }
    .rodata : { *(.rodata .rodata.*) }
    .data : { *(.data .data.*) }
    .bss : { *(.bss .bss.*) }
}
```

`.cargo/config.toml` でデフォルトターゲットとリンカスクリプトを設定:

```toml
[build]
target = "x86_64-unknown-none"
[target.x86_64-unknown-none]
rustflags = ["-C", "link-arg=-Tlinker.ld", "-C", "relocation-model=static"]
```

ビルド結果を `objdump -d` で確認すると、`_start` が 0x400040 に配置されていて、`sys_write` と `sys_exit` のヘルパー関数がそれより前に配置されていた。

### ELF パーサー (`kernel/src/elf.rs`)

ELF64 のヘッダー構造体を `#[repr(C)]` で定義した。ELF フォーマットはバイナリ構造がシンプルで、ヘッダーの先頭 4 バイトが `\x7fELF` というマジックナンバーで始まる。

パーサーは最小限の検証だけ行う:
- マジックナンバー (`\x7fELF`)
- クラス (64-bit = ELFCLASS64)
- エンディアン (Little Endian)
- マシンタイプ (EM_X86_64 = 0x3E)

プログラムヘッダーから `PT_LOAD` (type=1) のセグメントを抽出して、仮想アドレス (vaddr)、ファイルサイズ (filesz)、メモリサイズ (memsz) を返す。BSS 領域は memsz > filesz の差分で表現される（ファイルにはデータがないがメモリ上はゼロ初期化する必要がある）。

### ELF ローダーとページマッピングの難関

ここが今日最もハマったところ。実装自体は一通り完成したのに、ELF バイナリを実行すると毎回ページフォルトが発生した。

#### 症状

`elf` コマンドを実行すると:

```
Running ELF binary in Ring 3...

Page fault in user mode!
  Accessed address: Ok(VirtAddr(0x1b))
  Error code: PageFaultErrorCode(PROTECTION_VIOLATION | CAUSED_BY_WRITE | USER_MODE)
```

アクセスアドレス 0x1b でページフォルト。0x1b は User Data セグメントセレクタの値で、明らかにおかしい。

#### デバッグ過程

まず `iretq` のスタックフレームが正しいか疑った。割り込みスタックフレームを出力したところ:

```
instruction_pointer: 0x400040   (正しい — _start のアドレス)
stack_pointer: 0x804000          (正しい — ユーザースタックトップ)
code_segment: Ring3, index 4     (正しい — User Code セレクタ)
stack_segment: Ring3, index 3    (正しい — User Data セレクタ)
```

`iretq` 自体は成功していた。RSP も 0x804000 で正しい。ではなぜ 0x1b へのアクセスが発生するのか。

次にプロセスのページテーブルを手動で辿って、0x400040 の物理アドレスを確認した:

```
page 0x400000 -> phys 0x400000
code at 0x400040: 00 00 00 00 00 00 00 00 00 00 00 00 ...
```

**コードが全部ゼロ！** 物理アドレス 0x400000 は UEFI のアイデンティティマッピングのアドレスそのものだった。ELF のコードが確保したフレーム (0x105000 等) にコピーされていたはずなのに、ページテーブルはカーネルの元のマッピング (phys=virt=0x400000) を指していた。

#### 根本原因: カーネルとのページテーブル共有

`map_user_pages_in_process()` が仮想アドレス 0x400000 の新しいフレームをマッピングしようとしたとき、L4 → L3 → L2 → L1 の中間テーブルはカーネルのアイデンティティマッピングから共有されたものだった。共有テーブルの L1 エントリを直接書き換えると、**カーネルのページテーブルも壊れてしまう**。しかも L1 エントリには既にカーネルのマッピング (phys=0x400000) があったので、「既にマッピング済み」と判定されて新しいフレームが設定されなかった。

#### 修正: 分岐コピーパターンの導入

Day 2 の `set_user_accessible_in_process()` で使った「分岐コピー」パターンを `map_user_pages_in_process()` にも導入した。カーネルと共有しているテーブルを検出して、新しいフレームにコピーしてからプロセス固有の変更を行う:

1. プロセスの L4[0] がカーネルの L3 を指している → L3 テーブルを新フレームにコピー
2. コピーした L3 の中で、L2 もカーネルと共有 → L2 テーブルもコピー
3. L2 の中の L1 もカーネルと共有 → L1 テーブルもコピー
4. コピーした L1 のエントリに新しいデータフレームを設定

これでカーネルのテーブルに影響を与えずに、プロセス固有のマッピングを作成できる。

#### もう一つのバグ: 同一ページに複数セグメント

ELF パーサーの出力を見ると:

```
[0] vaddr=0x400000 filesz=0x57 memsz=0x57
[1] vaddr=0x400057 filesz=0x45 memsz=0x45
```

2 つの LOAD セグメントが同じページ (0x400000-0x400FFF) に含まれている。`map_user_pages_in_process()` をセグメントごとに別々に呼ぶと、2回目の呼び出しで同じページに新しいフレームを割り当ててしまい、1回目にコピーしたデータが消える。

これを防ぐため、前回の呼び出しで確保済みのフレームリストを渡し、既にプロセス専用のフレームがマッピングされている場合は再利用するようにした。

#### jump_to_usermode の改善

もう一つの修正として、`jump_to_usermode` アセンブリ関数の 5 番目の引数 (`user_ss`) をスタック経由で渡す方式を廃止した。Microsoft x64 ABI では 5 番目以降の引数はスタックの `[rsp+40]` に置かれるが、コンパイラの最適化によってスタックレイアウトが変わると正しい値を読めない可能性がある。

代わりに、GDT の配置順序 (User Data → User Code) から `user_ss = user_cs - 8` が常に成り立つことを利用して、アセンブリ内で計算するようにした:

```asm
mov rax, rdx      ; rax = user_cs
sub rax, 8        ; rax = user_ss = user_cs - 8
push rax           ; SS
push r9            ; RSP
push r8            ; RFLAGS
push rdx           ; CS
push rcx           ; RIP
iretq
```

### ビルドシステム統合

Makefile に `build-user` ターゲットを追加し、`build` が `build-user` に依存するようにした。`make build` で user → kernel の順にビルドされ、`include_bytes!` が自動的に最新の ELF バイナリを埋め込む。

CI (`.github/workflows/build.yml`) にも `x86_64-unknown-none` ターゲットの追加と user crate のビルドステップを追加した。

## 最終結果

全コマンドが正常に動作:

```
sabos> usermode
Hello from Ring 3!

sabos> usertest
Page fault in user mode!
Protection test passed!

sabos> isolate
Process A: CR3 = 0x104000 ... Hello from Ring 3!
Process B: CR3 = 0x108000 ... Hello from Ring 3!
Frames reclaimed: 8

sabos> elf
Entry point: 0x400040
LOAD segments: 2
Hello from ELF binary!
Frames: before=21976, after=21976, reclaimed=0

sabos> mem
Frames: 21980 total, 4 allocated, 21976 free
```

`elf` コマンドの `reclaimed=0` は「ELF プロセスで確保したフレームが全て正しく解放されてプロセス作成前と同じ状態に戻った」ことを意味する（before == after なので差分がゼロ）。メモリリークなし。

![ELF バイナリ実行](images/elf-hello.png)

## 学んだこと

- **ページテーブルの共有は危険**: プロセスのページテーブルがカーネルと中間テーブルを共有している場合、直接エントリを変更するとカーネルのマッピングが壊れる。変更が必要な場合は必ず分岐コピーしてからプロセス固有のテーブルを操作する
- **ELF のセグメントは同じページに重なることがある**: 小さなバイナリでは .text と .rodata が同一ページに収まる。ページ単位でフレームを管理する場合、セグメントをまたぐ重複に注意が必要
- **ABI の違いは常に意識する**: UEFI カーネル (Microsoft ABI) と ELF ユーザープログラム (System V ABI) では引数レジスタの規約が異なる。アセンブリでの引数渡しはスタック経由を避け、レジスタだけで完結させるのが安全

## 次にやりたいこと

- **ファイルシステム**: FAT32 を実装して、ディスクから ELF バイナリを読み込めるようにする
- **複数ユーザープロセスの同時実行**: タスクスケジューラとユーザーモードプロセスを統合する
- **仮想アドレス空間のレイアウト**: カーネル空間を上位アドレス、ユーザー空間を下位アドレスに配置する正式なレイアウトに移行する
- **mmap / brk**: ユーザープログラムが動的にメモリを確保できるシステムコールを追加する

---

# Day 4 — virtio-blk ドライバと FAT16 ファイルシステム

Day 3 まででカーネルに ELF バイナリを `include_bytes!` で埋め込んでロード・実行できるようになった。でもこれでは新しいプログラムを追加するたびにカーネルを再ビルドする必要がある。本物の OS ではディスクからプログラムを読み込むのが当然。今日は QEMU の virtio-blk デバイスを通じて FAT16 ディスクからファイルを読み出せるようにする。

## 今日のゴール

1. **PCI 列挙の仕上げ**: 既に書いた PCI バス列挙コードをコミットする
2. **virtio-blk ドライバ**: PCI で見つけた virtio-blk デバイスを初期化し、ブロック（セクタ）単位で読み取りできるようにする
3. **FAT16 ファイルシステム**: virtio-blk の上に FAT16 ドライバを実装し、ファイル一覧とファイル読み取りを可能にする
4. **ディスクから ELF 実行**: FAT16 上の ELF バイナリを読み込んでユーザーモードで実行する

## 実装計画

### Task 1: PCI 列挙のコミット

既にある未コミットのコードをコミットする。変更点:
- `kernel/src/pci.rs` (新規) — PCI Configuration Space アクセス、バス列挙、virtio-blk 探索
- `kernel/src/shell.rs` — `lspci` / `halt` コマンド追加
- `kernel/src/main.rs` — `mod pci;` 追加
- `Makefile` — `disk-img` ターゲット追加（FAT16 32MB + virtio-blk）
- `setup-ubuntu.sh` — `mtools` / `dosfstools` 追加

### Task 2: virtio-blk ドライバ

virtio は仮想化環境でホスト-ゲスト間の効率的な I/O を実現するための標準インターフェース。QEMU の `-drive if=virtio` で使われる。今回は virtio legacy (v0.9.5) インターフェースを実装する。

**変更ファイル:**
- `kernel/src/virtio_blk.rs` (新規) — virtio-blk ドライバ
  - BAR0 から I/O ポートベースアドレスを取得
  - デバイス初期化（ステータスネゴシエーション）
  - Virtqueue のセットアップ（Descriptor Table + Available Ring + Used Ring）
  - ブロック読み取り関数 `read_block(sector, buf)`
- `kernel/src/main.rs` — `mod virtio_blk;` 追加
- `kernel/src/shell.rs` — `blkread` テストコマンド追加

### Task 3: FAT16 ファイルシステム

FAT16 は DOS/Windows 時代から使われているシンプルなファイルシステム。構造が単純で実装しやすい。

**変更ファイル:**
- `kernel/src/fat16.rs` (新規) — FAT16 ドライバ
  - BPB (BIOS Parameter Block) のパース — セクタサイズ、クラスタサイズ、FAT の位置等
  - ルートディレクトリエントリの読み取り
  - FAT テーブルを辿ってクラスタチェーンを読むファイル読み取り
- `kernel/src/main.rs` — `mod fat16;` 追加
- `kernel/src/shell.rs` — `ls` コマンド（ファイル一覧）、`cat <file>` コマンド（ファイル内容表示）

### Task 4: ディスクから ELF 実行

**変更ファイル:**
- `kernel/src/shell.rs` — `run <file>` コマンド追加
  - FAT16 からファイルを読み込み
  - ELF パース → プロセス作成 → Ring 3 実行 → 破棄

## 実装ログ

### Task 1: PCI バス列挙 ✅

まずは virtio-blk デバイスを見つけるために PCI バスの列挙機能を実装した。

#### PCI とは

PCI (Peripheral Component Interconnect) は PC のデバイスを接続するバス規格。CPU から各デバイスの設定情報（ベンダー ID、デバイス ID、BAR 等）を読み書きできる。PCI Type 1 アクセス方式では、I/O ポート 0xCF8 (CONFIG_ADDRESS) にアドレスを書き込み、0xCFC (CONFIG_DATA) からデータを読み書きする。

CONFIG_ADDRESS のビット構造:
- bit 31: Enable bit (1 で有効)
- bit 23:16: バス番号 (0〜255)
- bit 15:11: デバイス番号 (0〜31)
- bit 10:8: ファンクション番号 (0〜7)
- bit 7:2: レジスタ番号 (4バイトアライン)

Configuration Space の主要レジスタ:
- 0x00: ベンダー ID (16bit) + デバイス ID (16bit)
- 0x08: クラスコード（デバイスの大分類）
- 0x10〜0x24: BAR0〜BAR5 (Base Address Register)

#### 実装した関数

- `pci_config_read32()` / `pci_config_read16()`: 設定情報の読み取り
- `read_bar()`: BAR の読み取り
- `enumerate_bus()`: バス 0 の全デバイス列挙
- `find_virtio_blk()`: virtio-blk デバイスの探索 (vendor=0x1AF4, device=0x1001)

マルチファンクションデバイスの判定も実装した。ヘッダータイプ (offset 0x0E) の bit 7 が 1 ならファンクション 1〜7 もスキャンする。

#### シェルコマンド

- `lspci`: PCI デバイス一覧を表示（BDF、ベンダー ID、デバイス ID、クラスコード）
- `halt`: CPU を停止 (`cli` + `hlt` ループで二度と復帰しない)

#### ビルドシステム更新

- Makefile に `disk-img` ターゲットを追加: 32MB FAT16 イメージを作成し、テストファイル (HELLO.TXT) とユーザープログラム (HELLO.ELF) を書き込む
- QEMU コマンドに `-drive if=virtio,format=raw,file=disk.img` を追加して virtio-blk デバイスとして接続
- setup-ubuntu.sh に `mtools` / `dosfstools` を追加（FAT16 イメージの作成に必要）

### Task 2: virtio-blk ドライバ ✅

virtio は仮想化環境でホストとゲスト間の効率的な I/O を実現するための標準インターフェース仕様。物理ハードウェアのエミュレーションよりオーバーヘッドが小さい。QEMU で `-drive if=virtio` を指定すると、ゲスト OS からは PCI バス上の virtio デバイス (vendor=0x1AF4) として見える。

今回実装したのは virtio legacy (v0.9.5) インターフェース。QEMU のデフォルトは legacy デバイス (device_id=0x1001) を使う。

#### PCI Transport と BAR0

virtio デバイスの設定レジスタは BAR0（Base Address Register 0）を通じてアクセスする。BAR0 が I/O ポート空間にマップされている場合（bit 0 = 1）、`in` / `out` 命令でレジスタを読み書きする。主要なレジスタ:

- Device Features (0x00): デバイスが対応する機能ビット
- Guest Features (0x04): ゲストが使いたい機能ビット
- Queue Address (0x08): Virtqueue の物理ページ番号
- Queue Size (0x0C): Virtqueue のエントリ数
- Queue Notify (0x10): デバイスへの通知
- Device Status (0x12): 初期化ステータス

初期化シーケンスは Reset → ACKNOWLEDGE → DRIVER → Feature negotiation → Virtqueue setup → DRIVER_OK。

#### Virtqueue (Split Virtqueue)

virtio の I/O は Virtqueue を通じて行われる。Virtqueue は 3 つのデータ構造で構成される:

1. **Descriptor Table**: I/O バッファの物理アドレス・長さ・フラグの配列。複数のディスクリプタを `next` フィールドでチェーンできる
2. **Available Ring**: ゲスト→デバイスへの「新しいリクエストがある」通知リング
3. **Used Ring**: デバイス→ゲストへの「リクエスト完了」通知リング

virtio-blk のブロック読み取りは 3 つのディスクリプタをチェーンする:
- [0] リクエストヘッダー (type=IN, sector=N)
- [1] データバッファ (512 バイト、デバイスが書き込む)
- [2] ステータスバイト (1 バイト、0=OK)

Available Ring にチェーンの先頭を書いて Queue Notify で通知し、Used Ring をポーリングして完了を待つ。

#### ハマったポイント: ページアライメント

最初の実装では Virtqueue のメモリを `Vec<u8>` で確保していたが、アドレスが `0x59f6e90` のようにページアラインされていなかった。legacy virtio の Queue Address レジスタは「物理アドレス ÷ 4096」を受け取るため、アラインされていないと下位ビットが切り捨てられて正しいアドレスにならない。

`alloc::alloc::alloc_zeroed` に `Layout::from_size_align(size, 4096)` を渡してページアラインのメモリを確保することで解決。修正後のアドレスは `0x59f6000` で、ページアラインが保証された。

#### テスト結果

```
Initializing virtio-blk... OK (65536 sectors, 32 MiB)
```

セクタ 0 の読み取りに成功し、FAT16 の BPB シグネチャ (0x55AA) を確認できた。

### Task 3: FAT16 ファイルシステム ✅

FAT (File Allocation Table) は DOS 時代から使われているシンプルなファイルシステム。FAT16 は最大 2GB のボリュームをサポートし、構造が単純で実装しやすい。

#### ディスクレイアウト

FAT16 のディスクは以下の領域に分かれる:

```
[ブートセクタ (BPB)]   ← セクタ 0
[予約セクタ...]         ← reserved_sectors 個分
[FAT #1]               ← num_fats × fat_size_16 セクタ
[FAT #2 (バックアップ)]
[ルートディレクトリ]    ← root_entry_count × 32 バイト
[データ領域]            ← クラスタ 2 から始まる
```

BPB (BIOS Parameter Block) はセクタ 0 の固定オフセットにあるパラメータ群で、セクタサイズ・クラスタサイズ・FAT の位置等を定義する。今回のディスクでは bytes_per_sector=512, sectors_per_cluster=4, reserved_sectors=4, num_fats=2, root_entry_count=512, fat_size_16=64 だった。

#### FAT テーブルとクラスタチェーン

FAT16 の各エントリは 16 ビット。ファイルのデータはクラスタ（連続するセクタのグループ）単位で管理される。ファイルが複数クラスタにまたがる場合、FAT テーブルがリンクリストのように次のクラスタ番号を示す。0xFFF8〜0xFFFF はチェーン終端を意味する。

#### ディレクトリエントリ

各エントリは 32 バイト固定長。ファイル名は 8.3 形式（名前 8 バイト + 拡張子 3 バイト、スペースでパディング）。先頭バイトが 0x00 ならディレクトリ終端、0xE5 なら削除済み。属性フラグで通常ファイル・ディレクトリ・ボリュームラベル・LFN（長いファイル名）を判別する。

#### テスト結果

```
sabos> ls
  HELLO.TXT           18
  HELLO.ELF         9392
  NVVARS           10391
  3 file(s)
sabos> cat HELLO.TXT
Hello from FAT16!
```

### Task 4: ディスクから ELF 実行 ✅

FAT16 の `read_file()` で読んだバイト列を既存の ELF パーサー (`elf.rs`) と ELF ローダー (`usermode.rs`) に渡すだけで動いた。Day 3 で `include_bytes!` 埋め込みデータに対して作ったコードが、ディスクから読んだデータに対してもそのまま使える。

```
sabos> run HELLO.ELF
Loading HELLO.ELF from disk...
  Loaded 9392 bytes
  Entry point: 0x400040
  LOAD segments: 2
Running in Ring 3...
Hello from ELF binary!
Program exited.
Frames: before=18896, after=18896, reclaimed=0
```

メモリリークなし (before == after)。プロセスが確保したページテーブルフレームと ELF データフレームが正しく解放されている。

![FAT16 からの ELF 実行](images/fat16-run.png)

### おまけ: sleep デモのハング修正

Day 2 で実装した sleep デモが動作しなくなっていた問題を修正した。原因は `yield_now()` で全タスクが Sleeping のとき、割り込みを有効→即無効のタイトループになり、タイマー割り込みが発火する隙がなかったこと。

修正: `yield_now()` で切り替え先がない場合、`enable_and_hlt()` で CPU を停止して次の割り込み（タイマー）を待つようにした。`enable_and_hlt()` はアトミックに sti + hlt を実行するため、割り込みの取りこぼしを防ぐ。

## 学んだこと

- **virtio の Virtqueue はページアラインが必須**: legacy virtio の Queue Address レジスタは物理ページ番号を受け取るため、4096 バイト境界に揃ったメモリが必要。ヒープアロケータのデフォルトアラインメント (8 バイト) では不十分
- **FAT16 はシンプルで実装しやすい**: BPB のパース、FAT チェーンの追跡、ディレクトリエントリの読み取りの 3 要素だけで基本的なファイル読み取りが実現できる
- **抽象化が効いている**: Day 3 で作った ELF パーサー/ローダーは `&[u8]` スライスを受け取る設計にしていたので、データの取得元が `include_bytes!` でもディスクでも変更なく使えた
- **enable_and_hlt() の重要性**: 割り込み有効化と hlt をアトミックに実行しないと、その隙間で割り込みを取りこぼす可能性がある。OS カーネルのスケジューラでは特に重要

### Task 5: FAT16 サブディレクトリ対応 ✅

ルートディレクトリのファイルしか読めないのは不便なので、サブディレクトリにもアクセスできるようにした。

#### FAT16 のサブディレクトリ構造

FAT16 のルートディレクトリは固定位置・固定サイズでデータ領域の直前に配置される（root_entry_count × 32 バイト）。一方、サブディレクトリはデータ領域にクラスタチェーンとして配置され、通常のファイルと同じようにクラスタを辿って読む。

サブディレクトリのディレクトリエントリには特殊なエントリが含まれる:
- `.` (カレントディレクトリ): 自分自身の先頭クラスタ
- `..` (親ディレクトリ): 親の先頭クラスタ（ルートの場合はクラスタ 0）

#### 実装した関数

- `list_subdir(first_cluster)`: サブディレクトリのエントリ一覧を取得。クラスタチェーンを辿る
- `parse_dir_entries()`: ディレクトリエントリのパース処理を共通化（ルートとサブディレクトリで共用）
- `list_dir(path)`: パスを解析してディレクトリ一覧を返す。"/" 区切りのパスを処理して階層を辿る
- `find_entry(path)`: パスを解析してファイル/ディレクトリのエントリを探す
- `read_file(path)`: パスを受け取るように変更（`find_entry()` を内部で使用）

#### シェルコマンドの更新

- `ls [path]`: 引数なしはルート、引数ありはそのパスのディレクトリを表示
  - サブディレクトリには `.` と `..` が含まれるが、見づらいので表示から除外
- `cat <path>`: `/SUBDIR/FILE.TXT` のようなパスでファイルを読める
- `run <path>`: `/SUBDIR/APP.ELF` のようなパスで ELF を実行できる

#### テスト結果

```
sabos> ls
Directory: /
  Name          Size     Attr
  ------------- -------- ----
  HELLO.TXT           18
  HELLO.ELF         9392
  NVVARS           10391
  SUBDIR               0 <DIR>
  4 file(s)
sabos> ls /SUBDIR
Directory: /SUBDIR
  Name          Size     Attr
  ------------- -------- ----
  TEST.TXT            26
  APP.ELF           9392
  2 file(s)
sabos> cat /SUBDIR/TEST.TXT
Hello from subdirectory!

sabos> run /SUBDIR/APP.ELF
Loading /SUBDIR/APP.ELF from disk...
FAT16: reading file 'APP.ELF', cluster=16, size=9392
  Loaded 9392 bytes
  Entry point: 0x400040
  LOAD segments: 2
Running in Ring 3...
Hello from ELF binary!
Program exited.
```

ネストしたサブディレクトリ（`/A/B/C/FILE.TXT`）も同じロジックで辿れる。

### Task 6: virtio-blk 書き込み対応 ✅

読み取り専用だった virtio-blk ドライバに書き込み機能を追加した。

#### virtio-blk の読み書きの違い

virtio-blk のリクエストは読み取り (IN) と書き込み (OUT) で構造がほぼ同じ。違いは:

1. **リクエストタイプ**: `VIRTIO_BLK_T_IN` (0) vs `VIRTIO_BLK_T_OUT` (1)
2. **データバッファのフラグ**: 読み取りは `VIRTQ_DESC_F_WRITE`（デバイスが書き込む）、書き込みはフラグなし（デバイスが読む）

`read_sector` のコードをほぼコピーして、この 2 点を変更するだけで `write_sector` が完成した。

#### テストコマンド

`blkwrite <sector>` コマンドを追加してテスト。セクタにテストパターンを書き込み、読み返して一致を確認する。

```
sabos> blkwrite 200
Sector 200 written successfully!
Verified: read-back matches written data.
```

### Task 7: FAT16 ファイル作成・削除 ✅

virtio-blk の書き込みを使って、FAT16 ファイルシステムにファイルを作成・削除する機能を実装した。

#### 実装した関数

- `write_sector()` - セクタ書き込み（内部ヘルパー）
- `write_fat_entry()` - FAT テーブルのエントリを更新（FAT #1 と #2 の両方）
- `find_free_cluster()` - FAT テーブルをスキャンして空きクラスタ (0x0000) を探す
- `allocate_clusters()` - 必要な数のクラスタを確保してチェーンを作成
- `free_cluster_chain()` - クラスタチェーンを解放
- `add_root_dir_entry()` - ルートディレクトリに新しいエントリを追加
- `create_file()` - ファイルを作成してデータを書き込む
- `delete_file()` - ファイルを削除（ディレクトリエントリを 0xE5 でマーク、クラスタ解放）

#### FAT16 ファイル作成の手順

1. 同名ファイルが存在しないか確認
2. 必要なクラスタ数を計算（ファイルサイズ ÷ クラスタサイズ）
3. 空きクラスタを確保してチェーンを作成
4. データをクラスタに書き込む
5. ルートディレクトリにエントリを追加（8.3 形式のファイル名）

#### シェルコマンド

- `write <name> <text>` - ファイル作成（例: `write TEST.TXT Hello`）
- `rm <name>` - ファイル削除

#### テスト結果

```
sabos> write A.TXT Hi
FAT16: creating file 'A.TXT', size=3, clusters=1
FAT16: file created successfully, first_cluster=21
File 'A.TXT' created (3 bytes)

sabos> ls
  A.TXT                3
  5 file(s)

sabos> cat A.TXT
Hi

sabos> rm A.TXT
FAT16: file 'A.TXT' deleted
File 'A.TXT' deleted
```

ファイルの作成・読み取り・削除が一通り動作している。

## 学んだこと

- **virtio の Virtqueue はページアラインが必須**: legacy virtio の Queue Address レジスタは物理ページ番号を受け取るため、4096 バイト境界に揃ったメモリが必要。ヒープアロケータのデフォルトアラインメント (8 バイト) では不十分
- **FAT16 はシンプルで実装しやすい**: BPB のパース、FAT チェーンの追跡、ディレクトリエントリの読み取りの 3 要素だけで基本的なファイル読み取りが実現できる
- **FAT16 のルートディレクトリとサブディレクトリの違い**: ルートは固定位置・固定サイズ、サブディレクトリはクラスタチェーン。この違いを意識して共通処理を抽出すると実装がきれいになる
- **抽象化が効いている**: Day 3 で作った ELF パーサー/ローダーは `&[u8]` スライスを受け取る設計にしていたので、データの取得元が `include_bytes!` でもディスクでも変更なく使えた
- **enable_and_hlt() の重要性**: 割り込み有効化と hlt をアトミックに実行しないと、その隙間で割り込みを取りこぼす可能性がある。OS カーネルのスケジューラでは特に重要
- **FAT16 書き込みは読み取りの延長**: FAT テーブルの更新、空きクラスタの探索、ディレクトリエントリの追加という基本操作の組み合わせでファイル作成が実現できる
- **virtio-blk の読み書きはほぼ同じ**: リクエストタイプとバッファフラグの違いだけで、コードの大部分を共有できる

### Task 8: virtio-net ドライバとネットワークスタック ✅

ネットワーク通信の基盤を実装した。virtio-blk と同じ virtio フレームワークを使うので、経験を活かせた。

#### virtio-net ドライバ

virtio-blk との主な違い:
- 2 つの Virtqueue: receiveq (queue 0) と transmitq (queue 1)
- パケットの前に virtio-net ヘッダー (10 バイト) が付く
- MAC アドレスを device config から読み取る

初期化シーケンスは virtio-blk とほぼ同じで、2 つの Virtqueue をセットアップする点だけ異なる。受信バッファは 16 個（各 2048 バイト）を事前に確保して receiveq に登録しておく。

```
virtio-net found at PCI 00:02.0
virtio-net I/O base: 0x60e0
virtio-net MAC: 52:54:00:12:34:56
virtio-net status after init: 0x7
OK (MAC 52:54:00:12:34:56)
```

#### ネットワークプロトコルスタック

最小限のプロトコルスタックを実装:

1. **Ethernet**: 14 バイトのヘッダー（宛先 MAC + 送信元 MAC + EtherType）をパース
2. **ARP**: IP アドレスから MAC アドレスを解決。ARP Request を受信したら Reply を返す
3. **IPv4**: ヘッダー（20 バイト）をパースして、宛先 IP が自分宛かチェック
4. **ICMP**: Echo Request (ping) を受信したら Echo Reply を返す

チェックサム計算は RFC 1071 に従って 16 ビット 1 の補数の和の 1 の補数を計算。IP ヘッダーと ICMP パケットの両方でチェックサムを設定する必要がある。

#### QEMU のネットワーク設定

QEMU の user mode networking (SLIRP) を使用:
- ゲストの IP: 10.0.2.15
- ゲートウェイ: 10.0.2.2
- DNS: 10.0.2.3

ホストからゲストへの直接 ping は SLIRP の制限でできないが、ゲスト内で ARP/ICMP の処理が動作していることはシリアルログで確認できる。

#### シェルコマンド

- `ip` - IP 設定を表示（IP アドレス、ゲートウェイ、MAC アドレス）
- `netpoll [秒数]` - 指定秒数（デフォルト 10 秒）ネットワークをポーリング

```
sabos> ip
IP Configuration:
  IP Address: 10.0.2.15
  Gateway:    10.0.2.2
  MAC:        52:54:00:12:34:56
```

## 学んだこと

- **virtio の Virtqueue はページアラインが必須**: legacy virtio の Queue Address レジスタは物理ページ番号を受け取るため、4096 バイト境界に揃ったメモリが必要。ヒープアロケータのデフォルトアラインメント (8 バイト) では不十分
- **FAT16 はシンプルで実装しやすい**: BPB のパース、FAT チェーンの追跡、ディレクトリエントリの読み取りの 3 要素だけで基本的なファイル読み取りが実現できる
- **FAT16 のルートディレクトリとサブディレクトリの違い**: ルートは固定位置・固定サイズ、サブディレクトリはクラスタチェーン。この違いを意識して共通処理を抽出すると実装がきれいになる
- **抽象化が効いている**: Day 3 で作った ELF パーサー/ローダーは `&[u8]` スライスを受け取る設計にしていたので、データの取得元が `include_bytes!` でもディスクでも変更なく使えた
- **enable_and_hlt() の重要性**: 割り込み有効化と hlt をアトミックに実行しないと、その隙間で割り込みを取りこぼす可能性がある。OS カーネルのスケジューラでは特に重要
- **FAT16 書き込みは読み取りの延長**: FAT テーブルの更新、空きクラスタの探索、ディレクトリエントリの追加という基本操作の組み合わせでファイル作成が実現できる
- **virtio-blk の読み書きはほぼ同じ**: リクエストタイプとバッファフラグの違いだけで、コードの大部分を共有できる
- **virtio-net は virtio-blk の応用**: 同じ virtio フレームワークを使うので、virtio-blk の実装経験があれば比較的スムーズに実装できる。違いは Virtqueue が 2 つある点と、パケットの前にヘッダーが付く点
- **ネットワークプロトコルは階層構造**: Ethernet → ARP/IP → ICMP と階層を意識すると実装が整理される

### Task 9: マルチプロセス対応 ✅

TCP サーバーを実装するためにはマルチプロセス（複数のユーザープロセスを同時に実行する能力）が必要。Day 2 で実装したカーネルタスク用のスケジューラを拡張して、ユーザープロセスもタスクとして管理できるようにした。

#### 設計

**課題:**
- 既存の `scheduler.rs` はカーネルタスク（`fn()` を実行する軽量スレッド）を管理
- 既存の `usermode.rs` はブロッキング実行（SYS_EXIT まで戻らない）

**解決策:**
ユーザープロセスをカーネルタスクとしてスケジューラに登録し、コンテキストスイッチ時に CR3（ページテーブル）も一緒に切り替える。

#### 実装した変更

**1. Task 構造体の拡張 (scheduler.rs)**
```rust
pub struct Task {
    // 既存フィールド
    pub id: u64,
    pub name: String,  // &'static str から String に変更（動的な名前対応）
    pub state: TaskState,
    pub context: Context,
    _stack: Option<Box<[u8]>>,
    // 新規追加
    pub cr3: Option<PhysFrame<Size4KiB>>,  // ユーザープロセスのページテーブル
    pub user_process_info: Option<UserProcessInfo>,
    pub is_user: bool,
}
```

**2. コンテキストスイッチで CR3 も切り替え**
```asm
context_switch:
    ; 既存のレジスタ保存処理
    push rbp
    ...
    mov [rcx], rsp    ; 現在の rsp を保存
    mov rsp, rdx      ; 新しい rsp に切り替え
    mov cr3, r8       ; CR3 を切り替え（新規追加！）
    ; 既存のレジスタ復元処理
    pop r15
    ...
    ret
```

**3. spawn_user() 関数**
```rust
pub fn spawn_user(name: &str, elf_data: &[u8]) -> Result<u64, &'static str> {
    // 1. ELF からユーザープロセスを作成
    // 2. カーネルタスクとしてスケジューラに登録
    // 3. user_task_trampoline をエントリに設定
}
```

**4. user_task_trampoline (アセンブリ)**
新規タスクが初めてスケジュールされたとき:
1. 割り込みを有効化（sti）
2. タスク ID からユーザープロセス情報を取得
3. TSS rsp0 を設定
4. iretq で Ring 3 に遷移

**5. シェルコマンド**
- `spawn <file>` - ELF をバックグラウンドで実行
- `ps` - TYPE 列で kernel/user を表示

#### 動作確認

```
sabos> spawn HELLO.ELF
Loading HELLO.ELF from disk...
  Loaded 9392 bytes
[scheduler] spawned user task 7 'HELLO.ELF' (entry: 0x400040)
Process 'HELLO.ELF' spawned as task 7 (background)
Use 'ps' to see running tasks.
sabos> Hello from ELF binary!  ← バックグラウンドで出力
ps
  ID  STATE       TYPE    NAME
  --  ----------  ------  ----------
   0  Running     kernel  kernel
   1  Finished    kernel  demo_a
   ...
   7  Finished    user    HELLO.ELF  ← ユーザープロセス！
  Total: 8 tasks (1 active)
```

`spawn` コマンドで起動したプロセスはバックグラウンドで実行され、シェルは即座にプロンプトを返す。プロセスの出力（"Hello from ELF binary!"）はプロンプトの後に表示される。これは正常な動作で、プロセスがスケジューラによって実行されていることを示している。

#### コンテキストスイッチの流れ

1. シェル（task 0）が `spawn` でユーザープロセス（task 7）を登録
2. 次の `yield_now()` または `preempt()` で task 7 にスイッチ
3. `context_switch` で RSP と CR3 を同時に切り替え
4. `user_task_trampoline` → `user_task_entry_wrapper` → `iretq` で Ring 3 に遷移
5. ユーザーコードが実行される
6. タイマー割り込みで `preempt()` → シェル（task 0）に戻る
7. シェルは次のコマンドを受け付けられる
8. `SYS_EXIT` → `exit_usermode()` → `user_task_exit_handler` → タスク Finished

## 学んだこと

- **virtio の Virtqueue はページアラインが必須**: legacy virtio の Queue Address レジスタは物理ページ番号を受け取るため、4096 バイト境界に揃ったメモリが必要。ヒープアロケータのデフォルトアラインメント (8 バイト) では不十分
- **FAT16 はシンプルで実装しやすい**: BPB のパース、FAT チェーンの追跡、ディレクトリエントリの読み取りの 3 要素だけで基本的なファイル読み取りが実現できる
- **FAT16 のルートディレクトリとサブディレクトリの違い**: ルートは固定位置・固定サイズ、サブディレクトリはクラスタチェーン。この違いを意識して共通処理を抽出すると実装がきれいになる
- **抽象化が効いている**: Day 3 で作った ELF パーサー/ローダーは `&[u8]` スライスを受け取る設計にしていたので、データの取得元が `include_bytes!` でもディスクでも変更なく使えた
- **enable_and_hlt() の重要性**: 割り込み有効化と hlt をアトミックに実行しないと、その隙間で割り込みを取りこぼす可能性がある。OS カーネルのスケジューラでは特に重要
- **FAT16 書き込みは読み取りの延長**: FAT テーブルの更新、空きクラスタの探索、ディレクトリエントリの追加という基本操作の組み合わせでファイル作成が実現できる
- **virtio-blk の読み書きはほぼ同じ**: リクエストタイプとバッファフラグの違いだけで、コードの大部分を共有できる
- **virtio-net は virtio-blk の応用**: 同じ virtio フレームワークを使うので、virtio-blk の実装経験があれば比較的スムーズに実装できる。違いは Virtqueue が 2 つある点と、パケットの前にヘッダーが付く点
- **ネットワークプロトコルは階層構造**: Ethernet → ARP/IP → ICMP と階層を意識すると実装が整理される
- **コンテキストスイッチと CR3 の同時切り替え**: ユーザープロセスはそれぞれ独自のページテーブルを持つため、タスクを切り替えるときに CR3 も一緒に切り替える必要がある。アセンブリで rsp と cr3 を連続して切り替えることで、アドレス空間の遷移がアトミックに行われる

### Task 10: UDP/DNS クライアント ✅

TCP クライアントを実装する前に、より単純な UDP プロトコルで DNS クライアントを実装した。DNS は UDP ポート 53 を使うため、UDP プロトコルと DNS プロトコルの両方を実装する必要がある。

#### UDP プロトコル

UDP (User Datagram Protocol) は IP の上に乗る軽量なトランスポートプロトコル。TCP と違ってコネクションレスで、信頼性保証（再送・順序保証）がない代わりにシンプル。DNS のような単発のリクエスト/レスポンスに適している。

UDP ヘッダーは 8 バイト:
- src_port (2 bytes): 送信元ポート
- dst_port (2 bytes): 宛先ポート
- length (2 bytes): UDP ヘッダー + ペイロードの長さ
- checksum (2 bytes): チェックサム（IPv4 では省略可能で 0 にできる）

```rust
#[repr(C, packed)]
pub struct UdpHeader {
    pub src_port: [u8; 2],
    pub dst_port: [u8; 2],
    pub length: [u8; 2],
    pub checksum: [u8; 2],
}
```

`send_udp_packet()` 関数で UDP パケットを送信:
1. Ethernet ヘッダー（14 バイト）+ virtio-net ヘッダー（10 バイト）
2. IP ヘッダー（20 バイト、protocol = 17 = UDP）
3. UDP ヘッダー（8 バイト）
4. ペイロード

#### DNS プロトコル

DNS (Domain Name System) はドメイン名から IP アドレスを解決するプロトコル。クエリとレスポンスの両方が同じフォーマットを使う。

DNS ヘッダーは 12 バイト:
- id (2 bytes): トランザクション ID（リクエストとレスポンスを紐付ける）
- flags (2 bytes): QR（0=クエリ/1=レスポンス）、OPCODE、AA、TC、RD、RA、RCODE
- qdcount (2 bytes): 質問セクションのエントリ数
- ancount (2 bytes): 回答セクションのエントリ数
- nscount (2 bytes): 権威セクションのエントリ数
- arcount (2 bytes): 追加セクションのエントリ数

ドメイン名はラベル形式でエンコード:
- 各ラベルの前に長さバイト
- 例: "google.com" → `\x06google\x03com\x00`
- 最後は 0x00 で終端

クエリセクション:
- QNAME: ドメイン名（ラベル形式）
- QTYPE: レコードタイプ（A=1 は IPv4 アドレス）
- QCLASS: クラス（IN=1 はインターネット）

レスポンスの回答セクション:
- NAME: ドメイン名（圧縮ポインタ 0xC00C でヘッダー直後を指すことが多い）
- TYPE: レコードタイプ
- CLASS: クラス
- TTL: Time To Live（4 バイト）
- RDLENGTH: RDATA の長さ
- RDATA: A レコードの場合は 4 バイトの IP アドレス

```rust
pub fn dns_lookup(domain: &str) -> Result<[u8; 4], &'static str> {
    // 1. DNS クエリパケットを構築
    let query = build_dns_query(domain);
    // 2. UDP で DNS サーバー (10.0.2.3:53) に送信
    send_udp_packet(DNS_SERVER_IP, 53, 12345, &query);
    // 3. レスポンスを待つ（最大 3 秒、ポーリング）
    // 4. A レコードの IP アドレスをパースして返す
}
```

#### QEMU SLIRP のネットワーク

QEMU の user mode networking (SLIRP) では:
- ゲスト IP: 10.0.2.15
- DNS サーバー: 10.0.2.3（SLIRP がホストの DNS に転送）

ゲストから 10.0.2.3 に DNS クエリを送ると、SLIRP がホスト OS の DNS 解決を使って応答を返す。

#### 動作確認

```
sabos> dns google.com
Resolving 'google.com'...
dns: sending query for 'google.com'
net: ARP Request for 10.0.2.15 from 10.0.2.2
net: sent ARP Reply
net: UDP packet from port 53 to port 12345, len=44
dns: response with 1 questions, 1 answers
dns: resolved to 142.250.199.142
google.com -> 142.250.199.142
```

1. DNS クエリを送信すると、まず SLIRP から ARP Request が来る
2. カーネルが ARP Reply を返す
3. その後 DNS レスポンス（UDP ポート 53 → 12345）が届く
4. A レコードをパースして IP アドレスを表示

DNS クエリ→レスポンスの往復が正常に動作している。これで TCP クライアントの実装前提条件が整った。

### Task 11: 自動テストフレームワーク ✅

開発サイクルを高速化するため、自動テストの仕組みを導入した。

#### selftest コマンド

シェルに `selftest` コマンドを追加。各サブシステムを自動テストして結果を表示する:

```
sabos> selftest
=== SELFTEST START ===
[PASS] memory_allocator
[PASS] paging
[PASS] scheduler
[PASS] virtio_blk
[PASS] fat16
[PASS] network_dns
=== SELFTEST END: 6/6 PASSED ===
```

テスト内容:
- **memory_allocator**: Box と Vec のアロケーション/解放
- **paging**: アドレス変換（アイデンティティマッピングの確認）
- **scheduler**: タスク spawn と完了待ち
- **virtio_blk**: セクタ 0 読み取りと FAT16 シグネチャ確認
- **fat16**: HELLO.TXT の読み取りと内容確認
- **network_dns**: example.com の DNS lookup

出力形式は CI でパースしやすいように、最後の行で `PASSED` または `FAILED` を明示する。

#### テストランナースクリプト

`scripts/run-selftest.sh` を作成:
1. QEMU をバックグラウンドで起動
2. シェルプロンプトを待つ
3. telnet monitor 経由で `selftest` コマンドを送信
4. 結果を待ってパース
5. PASS/FAIL に応じて終了コードを返す

```bash
make test  # ← これだけでテスト実行
```

#### GitHub Actions 統合

`.github/workflows/build.yml` を拡張して test ジョブを追加:
- build ジョブ成功後に test ジョブを実行
- Ubuntu で QEMU + OVMF + mtools をインストール
- `make test` で selftest を実行
- テスト失敗時は CI も失敗

これで push/PR のたびに自動でテストが走る。リグレッションを早期に検出できる。

### Task 12: TCP クライアントと HTTP GET ✅

UDP/DNS クライアントの実装に続いて、TCP プロトコルスタックを実装した。これで HTTP リクエストを送信できるようになった。

#### TCP プロトコル

TCP (Transmission Control Protocol) はコネクション指向の信頼性のあるストリームプロトコル。UDP と違って以下の特徴がある:

- **コネクション確立** (3-way ハンドシェイク)
- **シーケンス番号と確認応答** (seq/ack) による信頼性保証
- **状態管理** (CLOSED → SYN_SENT → ESTABLISHED → FIN_WAIT → ...)
- **フロー制御** (ウィンドウサイズ)

TCP ヘッダーは 20 バイト (オプションなし):
```
┌─────────────────────────────────────────┐
│    Source Port    │  Destination Port  │  4 bytes
├─────────────────────────────────────────┤
│              Sequence Number             │  4 bytes
├─────────────────────────────────────────┤
│           Acknowledgment Number          │  4 bytes
├─────────────────────────────────────────┤
│ Offset │ Reserved │  Flags  │  Window   │  4 bytes
├─────────────────────────────────────────┤
│     Checksum      │    Urgent Pointer   │  4 bytes
└─────────────────────────────────────────┘
```

TCP フラグ:
- FIN (0x01): コネクション終了
- SYN (0x02): コネクション開始
- RST (0x04): コネクションリセット
- PSH (0x08): プッシュ（即座に配送）
- ACK (0x10): 確認応答

#### 3-way ハンドシェイク

```
Client                    Server
  │                         │
  │  ──── SYN (seq=x) ────→ │
  │                         │
  │ ←── SYN-ACK (seq=y, ───  │
  │      ack=x+1)           │
  │                         │
  │ ──── ACK (seq=x+1, ───→ │
  │      ack=y+1)           │
  │                         │
  │     [ESTABLISHED]       │
```

#### TCP チェックサム

TCP チェックサムは UDP と違って必須。計算には「疑似ヘッダー」を含める:
- 送信元 IP (4 bytes)
- 宛先 IP (4 bytes)
- ゼロ (1 byte)
- プロトコル番号 (1 byte, TCP=6)
- TCP 長 (2 bytes)
- TCP ヘッダー + データ

```rust
fn calculate_tcp_checksum(
    src_ip: &[u8; 4],
    dst_ip: &[u8; 4],
    tcp_header: &TcpHeader,
    payload: &[u8],
) -> u16 {
    // 疑似ヘッダー + TCP ヘッダー + ペイロードの
    // 16ビット 1の補数の和の1の補数を計算
}
```

#### 実装した関数

- `tcp_connect(ip, port)`: 3-way ハンドシェイクでコネクション確立
- `tcp_send(data)`: データ送信
- `tcp_recv(timeout)`: データ受信（ポーリング）
- `tcp_close()`: FIN/ACK でコネクション終了
- `handle_tcp()`: 受信パケット処理（状態遷移）

#### http コマンド

```
sabos> http example.com /
Resolving example.com...
Resolved to 104.18.26.120
Connecting to 104.18.26.120:80...
Connected!
Sending HTTP request...
Receiving response...
--- Response ---
HTTP/1.1 200 OK
Date: Tue, 03 Feb 2026 11:52:20 GMT
Content-Type: text/html
...
<!doctype html><html lang="en"><head><title>Example Domain</title>...
--- End ---
```

DNS 解決 → TCP 接続 → HTTP GET → レスポンス受信 → コネクション終了の一連の流れが動作。

## 学んだこと

- **virtio の Virtqueue はページアラインが必須**: legacy virtio の Queue Address レジスタは物理ページ番号を受け取るため、4096 バイト境界に揃ったメモリが必要。ヒープアロケータのデフォルトアラインメント (8 バイト) では不十分
- **FAT16 はシンプルで実装しやすい**: BPB のパース、FAT チェーンの追跡、ディレクトリエントリの読み取りの 3 要素だけで基本的なファイル読み取りが実現できる
- **FAT16 のルートディレクトリとサブディレクトリの違い**: ルートは固定位置・固定サイズ、サブディレクトリはクラスタチェーン。この違いを意識して共通処理を抽出すると実装がきれいになる
- **抽象化が効いている**: Day 3 で作った ELF パーサー/ローダーは `&[u8]` スライスを受け取る設計にしていたので、データの取得元が `include_bytes!` でもディスクでも変更なく使えた
- **enable_and_hlt() の重要性**: 割り込み有効化と hlt をアトミックに実行しないと、その隙間で割り込みを取りこぼす可能性がある。OS カーネルのスケジューラでは特に重要
- **FAT16 書き込みは読み取りの延長**: FAT テーブルの更新、空きクラスタの探索、ディレクトリエントリの追加という基本操作の組み合わせでファイル作成が実現できる
- **virtio-blk の読み書きはほぼ同じ**: リクエストタイプとバッファフラグの違いだけで、コードの大部分を共有できる
- **virtio-net は virtio-blk の応用**: 同じ virtio フレームワークを使うので、virtio-blk の実装経験があれば比較的スムーズに実装できる。違いは Virtqueue が 2 つある点と、パケットの前にヘッダーが付く点
- **ネットワークプロトコルは階層構造**: Ethernet → ARP/IP → ICMP と階層を意識すると実装が整理される
- **コンテキストスイッチと CR3 の同時切り替え**: ユーザープロセスはそれぞれ独自のページテーブルを持つため、タスクを切り替えるときに CR3 も一緒に切り替える必要がある。アセンブリで rsp と cr3 を連続して切り替えることで、アドレス空間の遷移がアトミックに行われる
- **UDP は TCP より単純**: コネクション管理がないのでステートレスに実装できる。DNS のような単発のリクエスト/レスポンスには最適
- **DNS のラベル形式**: ドメイン名をドット区切りではなく「長さ+文字列」の連続で表現する。パースもシリアライズも単純なループで実装できる
- **自動テストは早めに導入すべき**: 手動テストは面倒で漏れやすい。selftest コマンド + CI の組み合わせで、変更のたびに自動検証される安心感がある
- **QEMU の telnet monitor は便利**: sendkey コマンドでキーボード入力をシミュレートできるので、対話的なテストも自動化できる
- **TCP は状態管理が肝**: 3-way ハンドシェイク、データ転送、コネクション終了それぞれで状態遷移が必要。状態を明示的に `enum TcpState` で管理することでコードの見通しが良くなる
- **TCP チェックサムは疑似ヘッダーを含む**: IP ヘッダーチェックサムとは計算方法が異なり、送信元/宛先 IP を含めた「疑似ヘッダー」から計算する

## 次にやりたいこと

- **TCP サーバー**: accept() ループでクライアントからの接続を待ち受ける
- ~~**シェルのユーザーランド移行**: マイクロカーネル化に向けてシェルをユーザー空間に移す~~ ← Phase 1-4 完了！
- **IPC 基盤の設計**: 型安全なメッセージパッシングの仕組み
- **仮想アドレス空間のレイアウト**: カーネル空間を上位アドレス、ユーザー空間を下位アドレスに配置する

---

# Day 5 — シェルのユーザーランド移行（Phase 1-4）

CLAUDE.md のマイクロカーネル目標に向けて、カーネル空間で動いていたシェルをユーザー空間に移行する作業を開始した。今日は Phase 1〜4 を完了し、ユーザー空間でファイル操作までできるシェルが動くようになった。

## 移行計画

カーネルのシェル（27コマンド）を段階的にユーザー空間に移行する。CLAUDE.md の設計原則（型安全性、null 終端排除）に従い、動く状態を保ちながら進める。

```
Phase 1: 型安全なシステムコール基盤
Phase 2: コンソール入力 (SYS_READ)
Phase 3: 最小限のユーザー空間シェル (echo, help, clear, exit)
Phase 4: ファイルシステムアクセス (ls, cat, write, rm)
Phase 5: カーネル情報取得 (mem, ps, ip)
Phase 6: プロセス実行 (run, spawn)
Phase 7: ネットワーク (dns, http)
Phase 8: システム制御と完成 (halt)
```

## Phase 1: 型安全なシステムコール基盤 ✅

CLAUDE.md の設計原則「null 終端文字列をカーネル API から排除する」を実装した。

### UserPtr<T> と UserSlice<T>

ユーザー空間から渡されるポインタを型安全にラップする新しい型を追加した:

```rust
// kernel/src/user_ptr.rs

/// ユーザー空間の単一ポインタ
pub struct UserPtr<T> {
    addr: u64,
    _marker: PhantomData<*const T>,
}

/// ユーザー空間のスライス（ポインタ + 長さ）
/// SABOS の「null 終端文字列を排除」の中核
pub struct UserSlice<T> {
    addr: u64,
    len: usize,
    _marker: PhantomData<*const T>,
}
```

`from_raw()` でアドレスを検証:
- null チェック
- ユーザー空間の範囲チェック（0〜0x7FFFFFFFFFFF）
- アラインメントチェック
- オーバーフローチェック

### SyscallError

エラーを型付きで表現:

```rust
pub enum SyscallError {
    NullPointer,
    InvalidAddress,
    MisalignedPointer,
    BufferOverflow,
    InvalidUtf8,
    FileNotFound,
    UnknownSyscall,
    Other,
}
```

`to_errno()` で Linux 風のエラーコード（負の値）に変換してユーザー空間に返す。

### syscall.rs の改修

SYS_WRITE を `UserSlice<u8>` 形式に改修:

```rust
fn sys_write(arg1: u64, arg2: u64) -> Result<u64, SyscallError> {
    let len = arg2 as usize;
    // 型安全に検証してからアクセス
    let user_slice = UserSlice::<u8>::from_raw(arg1, len)?;
    let s = user_slice.as_str_lossy();
    crate::kprint!("{}", s);
    Ok(len as u64)
}
```

### user/src/syscall.rs（新規）

ユーザー空間用のシステムコールライブラリを作成:

```rust
// 低レベル関数
unsafe fn syscall2(nr: u64, arg1: u64, arg2: u64) -> u64 { ... }

// 高レベル API
pub fn write(buf: &[u8]) -> SyscallResult { ... }
pub fn exit() -> ! { ... }
```

## Phase 2: コンソール入力 (SYS_READ) ✅

ユーザー空間からキーボード入力を読み取れるようにした。

### kernel/src/console.rs（新規）

コンソール入力管理:

```rust
static INPUT_BUFFER: Mutex<VecDeque<char>> = Mutex::new(VecDeque::new());

/// キーボード割り込みから呼ばれる
pub fn push_input_char(c: char) { ... }

/// ブロッキング読み取り
pub fn read_input_blocking() -> char {
    loop {
        if let Some(c) = read_input_nonblocking() {
            return c;
        }
        crate::scheduler::yield_now();  // CPU を譲る
    }
}
```

### キーボード割り込みの更新

キーボードハンドラで console モジュールにも通知:

```rust
// interrupts.rs
DecodedKey::Unicode(character) => {
    KEY_QUEUE.lock().push_back(character);        // カーネルシェル用
    crate::console::push_input_char(character);   // ユーザー空間 SYS_READ 用
}
```

### SYS_READ (0)

```rust
fn sys_read(arg1: u64, arg2: u64) -> Result<u64, SyscallError> {
    let user_slice = UserSlice::<u8>::from_raw(arg1, len)?;
    let buf = user_slice.as_mut_slice();
    let bytes_read = crate::console::read_input(buf, len);
    Ok(bytes_read as u64)
}
```

## Phase 3: 最小限のユーザー空間シェル ✅

echo, help, clear, exit の 4 コマンドを持つシェルをユーザー空間で実装した。

### SYS_CLEAR_SCREEN (2)

```rust
fn sys_clear_screen() -> Result<u64, SyscallError> {
    crate::framebuffer::clear_global_screen();
    Ok(0)
}
```

### user/src/shell.rs（新規）

```rust
pub fn run() -> ! {
    print_welcome();
    let mut line_buf = [0u8; LINE_BUFFER_SIZE];

    loop {
        syscall::write_str("user> ");
        let line_len = read_line(&mut line_buf);
        if line_len == 0 { continue; }
        execute_command(&line_buf[..line_len]);
    }
}
```

`read_line()` でエコーバックとバックスペースを処理。コマンドは `match` でディスパッチ。

### プロンプトの変更

カーネルシェルは `sabos>` だが、ユーザー空間シェルは `user>` にして区別できるようにした。

## Phase 4: ファイルシステムアクセス ✅

ls, cat, write, rm をユーザー空間で動かすためのシステムコールを追加した。

### 4引数システムコールへの拡張

ファイルシステムのシステムコールには (path_ptr, path_len, buf_ptr, buf_len) の 4 引数が必要。アセンブリを拡張:

```asm
; int 0x80 のレジスタ規約:
;   rax = syscall番号, rdi = arg1, rsi = arg2, rdx = arg3, r10 = arg4

; syscall_dispatch(nr, arg1, arg2, arg3, arg4) を呼ぶ
mov r9, rdx    ; arg3 → 第4引数 (r9)
mov r8, rsi    ; arg2 → 第3引数 (r8)
mov rdx, rdi   ; arg1 → 第2引数 (rdx)
mov rcx, rax   ; syscall_nr → 第1引数 (rcx)
; arg4 (r10) はスタック経由
```

### ファイルシステムシステムコール

```
SYS_FILE_READ (10):  ファイル全体を読み取り
SYS_FILE_WRITE (11): ファイル作成・上書き
SYS_FILE_DELETE (12): ファイル削除
SYS_DIR_LIST (13):   ディレクトリ一覧（改行区切り）
```

例: `sys_file_read`:

```rust
fn sys_file_read(arg1: u64, arg2: u64, arg3: u64, arg4: u64) -> Result<u64, SyscallError> {
    // パスを取得
    let path_slice = UserSlice::<u8>::from_raw(arg1, arg2 as usize)?;
    let path = path_slice.as_str().map_err(|_| SyscallError::InvalidUtf8)?;

    // バッファを取得
    let buf_slice = UserSlice::<u8>::from_raw(arg3, arg4 as usize)?;
    let buf = buf_slice.as_mut_slice();

    // FAT16 から読み取り
    let fat16 = crate::fat16::Fat16::new().map_err(|_| SyscallError::Other)?;
    let data = fat16.read_file(path).map_err(|_| SyscallError::FileNotFound)?;

    let copy_len = core::cmp::min(data.len(), buf.len());
    buf[..copy_len].copy_from_slice(&data[..copy_len]);
    Ok(copy_len as u64)
}
```

### ユーザー空間シェルのコマンド

```rust
// shell.rs
match cmd {
    "echo" => cmd_echo(args),
    "help" => cmd_help(),
    "clear" => cmd_clear(),
    "exit" => cmd_exit(),
    "ls" => cmd_ls(args),
    "cat" => cmd_cat(args),
    "write" => cmd_write(args),
    "rm" => cmd_rm(args),
    ...
}
```

## 現在のシステムコール番号

```
コンソール I/O:
  0  SYS_READ
  1  SYS_WRITE
  2  SYS_CLEAR_SCREEN

ファイルシステム:
  10 SYS_FILE_READ
  11 SYS_FILE_WRITE
  12 SYS_FILE_DELETE
  13 SYS_DIR_LIST

終了:
  60 SYS_EXIT
```

## 動作確認

ユーザー空間シェルは `run HELLO.ELF` で起動できる（ELF バイナリに埋め込まれている）:

```
sabos> run HELLO.ELF
Loading HELLO.ELF from disk...

=================================
  SABOS User Shell
=================================
Type 'help' for available commands.

user> ls
HELLO.TXT
HELLO.ELF
NVVARS
SUBDIR/
user> cat HELLO.TXT
Hello from FAT16!
user> echo Hello World
Hello World
user> exit
Goodbye!
Program exited.
sabos>
```

## 学んだこと

- **型安全なシステムコール境界**: `UserSlice<T>` でユーザー空間ポインタを検証してから使うパターンは、バッファオーバーフローを構造的に防ぐ。CLAUDE.md の設計原則がコードに反映できた
- **レジスタ規約の変換は面倒だが重要**: Linux 風の int 0x80 (rdi/rsi/rdx/r10) と Microsoft x64 ABI (rcx/rdx/r8/r9) の変換をアセンブリで行う。間違えるとデータが化ける
- **ブロッキング読み取りと yield**: `read_input_blocking()` で入力待ちの間 `yield_now()` を呼ぶことで、他のタスクに CPU を譲りつつ入力を待てる。協調的マルチタスキングのパターン
- **カーネルシェルとの共存**: 既存のカーネルシェルを壊さずにユーザー空間シェルを追加できた。KEY_QUEUE と console モジュールの両方に入力を通知することで、どちらのシェルも動く

---

# Day 6 — シェルのユーザーランド移行（Phase 5-8 完成）

Phase 1-4 に続いて、残りの Phase 5-8 を完成させた。これでユーザー空間シェルが 17 コマンドを持ち、カーネル起動時に自動起動するようになった。

## Phase 5: カーネル情報取得 (mem, ps, ip) ✅

カーネル内部の情報をユーザー空間から取得するシステムコールを追加した。

### システムコール

```
SYS_GET_MEM_INFO (20): メモリ情報を文字列で取得
SYS_GET_TASK_LIST (21): タスク一覧を文字列で取得
SYS_GET_NET_INFO (22): ネットワーク情報を文字列で取得
```

### SliceWriter

カーネル内部で `write!` マクロを使ってユーザー空間バッファに書き込むためのヘルパーを追加:

```rust
struct SliceWriter<'a> {
    buf: &'a mut [u8],
    pos: usize,
}

impl<'a> fmt::Write for SliceWriter<'a> {
    fn write_str(&mut self, s: &str) -> fmt::Result {
        let bytes = s.as_bytes();
        let remaining = &mut self.buf[self.pos..];
        let copy_len = core::cmp::min(bytes.len(), remaining.len());
        remaining[..copy_len].copy_from_slice(&bytes[..copy_len]);
        self.pos += copy_len;
        Ok(())
    }
}
```

これで `write!(writer, "Frames: {} total, ...", total)` のように自然に書ける。

### 動作確認

```
user> mem
Frames: 18003 total, 183 allocated, 17820 free
user> ps
ID  STATE       TYPE    NAME
0   Running     kernel  kernel
1   Finished    kernel  demo_a
...
user> ip
IP Configuration:
  IP Address: 10.0.2.15
  Gateway:    10.0.2.2
  MAC:        52:54:00:12:34:56
```

## Phase 6: プロセス実行 (run, spawn, sleep) ✅

ユーザー空間から ELF バイナリを実行できるようにした。

### システムコール

```
SYS_EXEC (30):  現在のプロセスを置き換えて実行
SYS_SPAWN (31): バックグラウンドで実行
SYS_YIELD (32): CPU を譲る
SYS_SLEEP (33): 指定ミリ秒スリープ
```

### SYS_EXEC の実装

既存の `run` コマンドは「新しいプロセスを作成 → Ring 3 実行 → プロセス破棄」という流れだったが、ユーザー空間から呼ぶには工夫が必要。現在のユーザープロセスを破棄して新しい ELF で置き換える:

```rust
fn sys_exec(path_ptr: u64, path_len: u64) -> Result<u64, SyscallError> {
    // 1. パスを取得
    let path = UserSlice::<u8>::from_raw(path_ptr, path_len as usize)?.as_str()?;

    // 2. FAT16 から ELF を読み込み
    let elf_data = fat16.read_file(path)?;

    // 3. 現在のユーザープロセスを破棄して新しい ELF で置き換え
    usermode::exec_elf(&elf_data)?;

    // ここには戻らない（新しいプログラムが実行される）
    unreachable!()
}
```

### 数値ユーティリティ

ユーザー空間シェルで数値を扱うために、`alloc` なしで動くユーティリティを追加:

```rust
// 数値を文字列バッファに書き込む
fn write_number(buf: &mut [u8], n: u64) -> usize { ... }

// 文字列を数値にパース
fn parse_u64(s: &str) -> Option<u64> { ... }
```

### 動作確認

```
user> run /HELLO.ELF
Loading /HELLO.ELF...
Hello from ELF binary!
user> spawn /HELLO.ELF
Spawned task 7: /HELLO.ELF
Hello from ELF binary!
user> sleep 1000
Sleeping for 1000 ms...
Done.
```

## Phase 7: ネットワーク (dns, http) ✅

TCP/IP 関連のシステムコールを追加した。

### システムコール

```
SYS_DNS_LOOKUP (40): DNS 解決 → IP アドレスを返す
SYS_TCP_CONNECT (41): TCP 接続確立
SYS_TCP_SEND (42): TCP データ送信
SYS_TCP_RECV (43): TCP データ受信
SYS_TCP_CLOSE (44): TCP 接続終了
```

### IP アドレスの表現

IP アドレスを文字列ではなく 4 バイトの配列で扱う:

```rust
// ユーザー空間
pub fn dns_lookup(domain: &str, result_ip: &mut [u8; 4]) -> SyscallResult { ... }
pub fn tcp_connect(ip: &[u8; 4], port: u16) -> SyscallResult { ... }

// シェルでの表示
fn write_ip(buf: &mut [u8], ip: &[u8; 4]) -> usize {
    // 10.0.2.15 形式で書き込む
}
```

### http コマンドの実装

```rust
fn cmd_http(args: &str) {
    // 引数をパース: "example.com /" または "example.com"
    let (host, path) = parse_http_args(args);

    // 1. DNS 解決
    let mut ip = [0u8; 4];
    syscall::dns_lookup(host, &mut ip)?;

    // 2. TCP 接続
    syscall::tcp_connect(&ip, 80)?;

    // 3. HTTP GET リクエスト送信
    let request = format!("GET {} HTTP/1.0\r\nHost: {}\r\n\r\n", path, host);
    syscall::tcp_send(request.as_bytes())?;

    // 4. レスポンス受信・表示
    loop {
        let n = syscall::tcp_recv(&mut buf)?;
        if n == 0 { break; }
        syscall::write(&buf[..n]);
    }

    // 5. 接続終了
    syscall::tcp_close()?;
}
```

### 動作確認

```
user> dns google.com
Resolving google.com...
google.com -> 142.250.199.142
user> http example.com /
Connecting to 93.184.215.14:80...
HTTP/1.0 200 OK
...
<!doctype html>...
```

## Phase 8: システム制御と完成 ✅

### SYS_HALT (50)

```rust
fn sys_halt() -> Result<u64, SyscallError> {
    kprintln!("System halt requested by user process.");
    loop {
        unsafe {
            core::arch::asm!("cli");
            core::arch::asm!("hlt");
        }
    }
}
```

### カーネル起動時にユーザーシェルを自動起動

`kernel/src/main.rs` を修正して、起動時にユーザーシェルを実行:

```rust
// --- ユーザーシェルの起動 ---
kprintln!("\nStarting user shell...\n");
let elf_data = usermode::get_user_elf_data();
match usermode::create_elf_process(elf_data) {
    Ok((process, entry_point, user_stack_top)) => {
        usermode::run_elf_process(&process, entry_point, user_stack_top);
        usermode::destroy_user_process(process);
        kprintln!("User shell exited.");
    }
    Err(e) => {
        kprintln!("Failed to start user shell: {}", e);
    }
}
// ユーザーシェル終了後はカーネルシェルにフォールバック
```

### selftest スクリプトの修正

ユーザーシェルが先に起動するようになったため、テストスクリプトを更新:

```bash
# 1. ユーザーシェルプロンプト (user>) を待つ
# 2. exit でユーザーシェルを終了
# 3. カーネルシェルプロンプト (sabos>) を待つ
# 4. selftest を実行
```

## 完成したシステムコール一覧

```
コンソール I/O (0-9):
  0  SYS_READ
  1  SYS_WRITE
  2  SYS_CLEAR_SCREEN

ファイルシステム (10-19):
  10 SYS_FILE_READ
  11 SYS_FILE_WRITE
  12 SYS_FILE_DELETE
  13 SYS_DIR_LIST

システム情報 (20-29):
  20 SYS_GET_MEM_INFO
  21 SYS_GET_TASK_LIST
  22 SYS_GET_NET_INFO

プロセス管理 (30-39):
  30 SYS_EXEC
  31 SYS_SPAWN
  32 SYS_YIELD
  33 SYS_SLEEP

ネットワーク (40-49):
  40 SYS_DNS_LOOKUP
  41 SYS_TCP_CONNECT
  42 SYS_TCP_SEND
  43 SYS_TCP_RECV
  44 SYS_TCP_CLOSE

システム制御 (50-59):
  50 SYS_HALT

終了 (60-69):
  60 SYS_EXIT
```

## ユーザー空間シェルのコマンド一覧

| カテゴリ | コマンド |
|---------|---------|
| 基本 | echo, help, clear, exit |
| ファイル | ls, cat, write, rm |
| システム情報 | mem, ps, ip |
| プロセス | run, spawn, sleep |
| ネットワーク | dns, http |
| システム制御 | halt |

計 17 コマンド。

## カーネルシェルに残したコマンド

開発・デバッグ用途のコマンドはカーネルシェルに残した:

- `selftest` - 自動テスト実行
- `panic` - カーネルパニックのテスト
- `blkread`, `blkwrite` - 低レベルブロックデバイス操作
- `lspci` - PCI デバイス一覧
- `usermode`, `usertest`, `isolate` - ユーザーモードテスト
- `elf` - ELF パーステスト
- `netpoll` - ネットワークポーリング

## 学んだこと

- **POSIX 互換を捨てると設計が簡単になる**: SABOS では `(ptr, len)` 形式でバッファを渡す規約を徹底した。null 終端文字列のバグを心配する必要がない
- **SliceWriter パターン**: `fmt::Write` を実装することで、カーネル内部で `write!` マクロを使ってユーザー空間バッファに安全に書き込める
- **カーネル/ユーザーの役割分担**: ユーザー空間でできることはユーザー空間で行い、カーネルは最小限の特権操作だけを提供するという設計が自然に進んだ
- **段階的な移行**: Phase 1 から順番に進めることで、各段階で動く状態を保てた。一度に全部移行しようとすると収拾がつかなくなる
- **selftest の重要性**: 各 Phase 完了後に `make test` で確認することで、リグレッションを即座に検出できた

## 次にやりたいこと

- **IPC 基盤の設計**: 型安全なメッセージパッシングの仕組み
- **ファイルシステムサービスのユーザー空間移行**: FAT16 ドライバをユーザー空間で動かす
- **ネットワークスタックのユーザー空間移行**: TCP/IP をユーザー空間のサービスとして実装
- **selftest のユーザー空間移行**: テストもユーザー空間で動かす

---

# 追加タスク: procfs 最小実装（追記）

ユーザーランドの `cat /proc/meminfo` と `cat /proc/tasks` を試せるように、procfs の最小構成を実装した。procfs は「カーネル内部の情報を疑似ファイルとして見せる仕組み」で、今回の範囲では `/proc` 以下に 2 つの固定ファイルだけ用意している。

## 実装内容

- `/proc` ディレクトリに `meminfo` / `tasks` を用意
- `SYS_FILE_READ` / `SYS_DIR_LIST` が `/proc` を特別扱いして読み取りを処理
- `procfs` は読み取り専用（write/rm は `EINVAL`）
- procfs の出力は JSON 形式に統一
- selftest に procfs の読み取り確認を追加

## 学んだこと

- **疑似ファイルシステムの価値**: procfs のように「情報をファイルとして見せる」仕組みは、ユーザー空間からの観察を統一できる。システムコールの追加だけでなく、情報提供の“形”を揃える設計が大事だと分かった

## メモ

- procfs は将来的にカーネルから追い出せるように TODO.md に明記しておいた（ユーザー空間サービス化の余地を残す）

---

# 追加タスク: procfs JSON 出力に合わせたユーザーシェル更新

procfs の出力を JSON に統一したので、ユーザー空間の `mem` / `ps` コマンドも JSON を読むように更新した。ここでいう JSON は「キーと値を `{"key":value}` で表現するテキスト形式」で、機械処理しやすいのが利点。

## 実装内容

- `user/src/shell.rs` の `mem` / `ps` で JSON を最小パースして表示
- 既存の CSV 解析は削除
- JSON 文字列の配列（`tasks`）から各オブジェクトを抽出して表示

## 学んだこと

- **JSON でも最小パーサで十分**: 今回は完全な JSON パーサは作らず、必要なキーだけを探す簡易パーサで実用になった。フォーマットを固定しておくと実装負担が減る

---

# 追加タスク: ファイル open 設計（非 POSIX / capability 前提）

SABOS の方針として POSIX 互換は目指さない。また FD を整数に固定せず、将来的に capability 的な概念を導入できるように設計する。さらに FAT16 以外のファイルシステム（例: ext4）も増える可能性があるので、VFS を前提に open を考える。

## 実装計画（変更ファイル・変更内容）

1. `docs/diary/2026-02-03.md`
   - open/handle の設計方針を文章化
   - capability や VFS との関係を明記

## 設計方針

### 1) Handle は「不透明 + 偽造困難」

ユーザー空間に返すハンドルは単なる整数ではなく、`id` と `token` を持つ構造体にする。`token` はカーネルが生成するランダム値で、偽造を防ぐ（capability っぽい性質）。

```rust
#[repr(C)]
pub struct Handle {
    id: u64,     // カーネル内テーブルのインデックス
    token: u64,  // 偽造防止用のランダム値
}
```

### 2) 最小 syscalls セット（非 POSIX）

まずは読み取り中心の最小セットから始める。

```
SYS_OPEN(path_ptr, path_len, flags, rights) -> Handle
SYS_READ(handle_ptr, buf_ptr, len) -> usize
SYS_WRITE(handle_ptr, buf_ptr, len) -> usize
SYS_CLOSE(handle_ptr) -> 0
SYS_STAT(handle_ptr, buf_ptr, len) -> usize  // JSON など
```

- `Handle` は `UserPtr<Handle>` で受け取る
- `flags` / `rights` はビットフラグ（READ/WRITE/ENUM/EXEC など）

### 3) VFS 前提の設計

- `open` は VFS のパス解決を通す
- VFS は `FsDriver` を持ち、FAT16 / procfs / 将来 ext4 を差し替え可能にする
- `Handle` は `FsDriver` と内部参照（inode/dirent 相当）を持つ

### 4) procfs との整合

- procfs は書き込み禁止（設計ルール）
- `open("/proc/meminfo", READ)` は OK
- `write` は「書き込み禁止」の明示エラーを返す（例: `SyscallError::ReadOnly`）

## 学んだこと

- **FD を捨てると設計の自由度が上がる**: Handle を capability 的に扱えるので、IPC でのハンドル移譲とも相性が良い

---

# 追加タスク: open/read/close の最小実装

設計メモに沿って、非 POSIX なファイル open を最小実装する。Handle は `id` + `token` の不透明構造体で、読み取り専用の操作から始める。procfs も Handle で開けるようにする。

## 実装計画（変更ファイル・変更内容）

1. `kernel/src/handle.rs`（新規）
   - Handle 構造体とハンドルテーブルを実装
   - `read/close` の操作を提供（書き込みは ReadOnly）
2. `kernel/src/syscall.rs`
   - `SYS_OPEN` / `SYS_HANDLE_READ` / `SYS_HANDLE_CLOSE` を追加
   - `/proc` と FAT16 を handle 化して open できるようにする
3. `kernel/src/shell.rs`
   - selftest に open/handle 読み取りのテストを追加
4. `user/src/syscall.rs`
   - Handle 構造体と open/read/close のラッパーを追加
5. `user/src/shell.rs`
   - `cat` を handle 経由の読み取りに切り替える

## 実装内容

- `Handle { id, token }` を導入し、`token` で偽造を防ぐ
- `SYS_OPEN` / `SYS_HANDLE_READ` / `SYS_HANDLE_CLOSE` を追加（読み取り専用）
- `/proc/meminfo` と `HELLO.TXT` を open して読めることを selftest で確認
- `cat` コマンドを handle 経由の読み取りに変更

## 学んだこと

- **capability の考え方**: capability は「権限を持った参照（偽造しにくい鍵付きハンドル）」という発想で、FD よりも安全に権限管理できる。Handle に `token` を入れる設計はこの方向性に近い

---

# 追加タスク: 警告の解消と selftest 実行

ビルド時の警告を残さない方針に沿って、未使用コードを整理した。`make test` で selftest を実行し、全テストの PASS を確認した。

## 実装内容

- 使っていない定数/関数/構造体を削除または整理
- 使わない変数は `_` プレフィックスに変更
- selftest: 9/9 PASS を確認

---

# 追加タスク: FAT16 ドライバのユーザー空間移行

FAT16 をカーネルからユーザー空間に移し、`ls/cat/write/rm` をユーザー空間の FAT16 実装で動かすようにした。ブロックデバイス（ディスクの最小入出力単位）を読むために、カーネル側には最小のブロック I/O システムコールを追加した。

## 実装内容

- ユーザー空間に FAT16 実装を追加（`user/src/fat16.rs`）
- ユーザー空間で `Vec` を使うために簡易ヒープアロケータを追加
- ブロック I/O 用 syscalls を追加（`SYS_BLOCK_READ` / `SYS_BLOCK_WRITE`）
- ユーザーシェルの `ls/cat/write/rm` を FAT16 直叩きに切り替え
- selftest にブロック syscalls のテストを追加

## 学んだこと

- **ブロックデバイス**: ディスクの I/O は 512 バイト単位の「セクタ」を読む/書く操作に分解できる。上位のファイルシステムはこの原始操作の上に構築される

---

# 追加タスク: TCP/IP スタックのユーザー空間サービス化

TCP/IP をユーザー空間サービスとして動かすために、まず IPC（タスク間通信）と netd を導入する。第1段階では netd がカーネルの既存ネットワーク syscalls を呼ぶ“代理サービス”として動き、シェルからのネットワーク操作を IPC 経由に切り替える。後続でドライバ/スタック自体をユーザー空間に移す。

## 実装計画（変更ファイル・変更内容）

1. `kernel/src/ipc.rs`（新規）
   - メッセージキューによる IPC を実装
2. `kernel/src/syscall.rs`
   - `SYS_IPC_SEND` / `SYS_IPC_RECV` を追加
   - `SYS_GET_NET_SERVICE_ID` 追加 or netd の起動方式を決める
3. `user/src/bin/netd.rs`（新規）
   - netd を追加（DNS/TCP のリクエストを IPC で受けて処理）
4. `user/src/shell.rs`
   - `dns/http` を IPC 経由に切り替える
5. `Makefile`
   - netd ELF を disk image に追加

## 実装内容

- IPC を追加（`SYS_IPC_SEND` / `SYS_IPC_RECV`）
- `netd` をユーザー空間のサービスとして追加し、DNS/TCP リクエストを IPC で受けて処理
- シェルの `dns/http` を IPC 経由に切り替え
- `NETD.ELF` を disk image に追加して起動できるようにした
- selftest に IPC のテストを追加

## 学んだこと

- **IPC の基本形**: 「宛先ごとのメッセージキュー + 受信待ち」は最小限の IPC として分かりやすい。まずは単純に動かし、将来は権限や型付けを強化する
- **サービス化の段階移行**: いきなりドライバまで移すのは大きいので、まずはユーザー空間に “代理サービス” を立てるのが現実的。ここからスタック本体を移す道筋ができた
- **バンプアロケータ**: free を持たない単純なアロケータ。メモリを一方向に切り出すだけなので実装が簡単で、学習用のユーザー空間には十分

---

# 追記: ビルド成果物の gitignore 追加

## 実装内容

- `esp/` と `kernel/target/` と `user/target/` を `.gitignore` に追加

## 学んだこと

- **ビルド成果物**: コンパイルやリンクで生成されるファイル群のこと。リポジトリに入れないのが基本

---

# 追記: 日記の配置整理（docs/diary へ移動）

## 実装内容

- 開発日記を `docs/diary/` へ移動し、`docs/` は仕様ドキュメント置き場にする方針へ
- `AGENTS.md` にドキュメント一覧を追加
- `README.md` の日記リンクを更新

## 学んだこと

- **ドキュメント導線**: 置き場が整理されていると、仕様と日記の役割が混ざらず検索性も上がる

---

# 追記: procfs JSON 仕様の置き場を整備

## 実装計画（変更ファイル・変更内容）

1. `docs/spec/procfs-json.md` を新規作成し、procfs の JSON 仕様ドラフトを書く
2. `AGENTS.md` に仕様ドキュメントの置き場ルールを追加する

## 実装内容

- `docs/spec/procfs-json.md` に JSON 仕様ドラフトを作成
- `AGENTS.md` に `docs/spec/` を仕様ドキュメント置き場として明記

## 学んだこと

- **スキーマ**: データの構造や意味を定義した設計書のこと。先に枠を決めると実装が揺れにくい

---

# 追記: user シェルの sleep でカーネルクラッシュ修正

## 実装計画（変更ファイル・変更内容）

1. `kernel/src/syscall.rs` — `SYS_SLEEP` の処理で割り込みを有効化してから sleep する

## 実装内容

- `sys_sleep()` で `interrupts::enable()` を呼んでから `scheduler::sleep_ms()` を実行するように修正
- コメントに「syscall は割り込み無効状態で入る」ことを明記して意図を残した

## 学んだこと

- **IF フラグ**: RFLAGS の割り込み許可ビット。`int 0x80` で Ring 0 に入ると IF が落ちるため、割り込みを待つ処理（sleep など）は明示的に IF を立て直す必要がある

---

# 今日の計画: IPC の型安全メッセージパッシング（プロトタイプ）

## 実装計画（変更ファイル・変更内容）

1. `kernel/src/ipc.rs` — Typed IPC の送受信キューを追加し、型の一致を強制する send/recv を実装
2. `kernel/src/shell.rs` — selftest に typed IPC のテストを追加

## 実装内容

- `kernel/src/ipc.rs` に Typed IPC を追加。タスクごとに 1 種類の型だけを受け付けるキューを持たせ、型が一致しない送受信はエラーにした
- `kernel/src/shell.rs` の selftest に `ipc_typed` を追加し、同一タスク間で構造体を送受信できることを確認した

## 学んだこと

- **TypeId**: Rust が型ごとに持つ一意の識別子。実行時に「このキューが受け付ける型は何か」をチェックするのに使える
