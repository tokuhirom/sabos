# 2026-02-08: Day 8 — SYS_MMAP バグ修正 → std::fs 実装で read_to_string が動いた！

## 今日の意気込み

昨日の std 対応 Phase 7 で `user-std` クレートが動くようになったものの、`println!` が使えず `raw_write()` というワークアラウンドで凌いでいた。今日はその根本原因を突き止めて、ちゃんと `println!` / `String` / `Vec` が動くようにしたい。

## SYS_MMAP ハングの原因究明

### 症状のおさらい

- `SYS_MMAP` を exec/spawn で起動したプロセスから呼ぶと制御が返らない
- シェルプロセス自身からの mmap selftest は PASS する
- `SYS_WRITE`（raw_write）は exec プロセスからも正常に動作する

### デバッグ手法

`sys_mmap` → `find_free_mmap_region` → `is_page_mapped` の各関数にデバッグ出力（`kprintln!`）を仕込んで、どの行でハングするか特定した。

最初は大まかなログ（関数の入り口/出口）から始めて、次にページテーブルの各レベル（L4 → L3 → L2 → L1）のエントリ内容を詳細に出力するように段階的にログを増やした。

### 判明した根本原因

```
[is_page_mapped] 0x40000000: L3[1] flags=0x83 addr=0x40000000 unused=false
[is_page_mapped] 0x40000000: L3 HUGE_PAGE -> mapped
```

**L3 エントリに 1GiB ヒュージページ（flags=0x83 = PRESENT | WRITABLE | HUGE_PAGE）が存在していた。**

UEFI は物理メモリを 1GiB 単位のヒュージページで identity mapping（物理アドレス = 仮想アドレス）する。`create_process_page_table()` はカーネルの L4 エントリを全コピーするため、この identity mapping がプロセスのページテーブルにもそのまま引き継がれる。

`MMAP_VADDR_BASE` は `0x4000_0000`（1GiB）に設定されていたが、これは L4[0] → L3[1] の範囲で、まさに UEFI のヒュージページと完全に重なっていた。結果、`find_free_mmap_region()` は全アドレスを「マッピング済み」と判定し、MMAP_VADDR_LIMIT まで延々とスキャンする無限ループに入っていた。

### なぜシェルでは動くのか

シェルプロセスの mmap selftest は `find_free_mmap_region()` を使わず、直接 `map_anonymous_pages_in_process()` にアドレスを渡していた（0x4000_0000 に 2 ページをマッピング）。つまり「空き領域の検索」をスキップしていたので、ヒュージページとの衝突が問題にならなかった。

### 修正

`MMAP_VADDR_BASE` を `0x100_0000_0000`（1TiB = L4[2] の範囲）に変更した。L4[0] は UEFI の identity mapping が入っている可能性があるが、L4[2] は空なので衝突しない。

```rust
// 修正前
const MMAP_VADDR_BASE: u64 = 0x4000_0000;     // 1 GiB (L4[0])
const MMAP_VADDR_LIMIT: u64 = 0x80_0000_0000;  // 512 GiB

// 修正後
const MMAP_VADDR_BASE: u64 = 0x100_0000_0000;  // 1 TiB (L4[2])
const MMAP_VADDR_LIMIT: u64 = 0x200_0000_0000;  // 2 TiB
```

## println! / String / Vec が動いた！

mmap 修正後、`user-std/src/main.rs` を以下のように書き換えた:

```rust
#![feature(restricted_std)]

fn main() {
    println!("Hello from SABOS std!");
    println!("2 + 3 = {}", 2 + 3);

    let s = String::from("Hello from std String!");
    println!("{}", s);

    let v: Vec<i32> = (1..=5).collect();
    let sum: i32 = v.iter().sum();
    println!("sum of 1..=5 = {}", sum);
}
```

**`raw_write()` ワークアラウンドを完全に削除し、`println!` のみで出力できるようになった。**

`println!` が動くまでの経路:
1. `println!` → std の `stdout()` → `OnceLock::get_or_init()` でスタティック Stdout を初期化
2. Stdout の初期化にはヒープ確保（`SYS_MMAP`）が必要
3. mmap 修正により SYS_MMAP が正常に動作 → Stdout 初期化成功
4. `Stdout::write()` → PAL の `sys_stdio_sabos.rs` → `SYS_WRITE` → シリアルコンソールに出力

`String::from` や `Vec::collect` もヒープアロケーション（SYS_MMAP）経由で正常動作。

## テスト結果

```
HELLOSTD.ELF test PASSED
  Arithmetic output OK
  Vec/alloc output OK
=== SELFTEST END: 40/40 PASSED ===
All tests PASSED!
```

## 学んだこと

- **x86_64 のページテーブル階層（L4 → L3 → L2 → L1）とヒュージページの仕組み**。L3 エントリに HUGE_PAGE フラグが立っていると、その下に L2/L1 テーブルは存在せず、1GiB の領域がまるごと 1 つのエントリでマッピングされる。
- **UEFI の identity mapping は想像以上に広い**。物理 RAM が 128MB でも、UEFI は 1GiB 単位でマッピングするため `0x40000000`〜`0x7FFFFFFF` まで identity mapping が存在する。
- **デバッグの手順は段階的に**。最初は大まかなログで問題の箇所を絞り、次に詳細ログで原因を特定する。最初から全部にログを入れるとノイズが多すぎる。

---

## std::fs の実装 — PAL fs モジュール

### PAL (Platform Abstraction Layer) とは

PAL は Rust の標準ライブラリ (`std`) がプラットフォームごとの違いを吸収するための仕組み。「Platform Abstraction Layer」の略で、日本語では「プラットフォーム抽象化層」。

Rust の `std` はマルチプラットフォーム対応だが、ファイルシステムの操作やプロセス管理、標準入出力などは OS ごとに全然違う。例えば Linux では `open()` + `read()` システムコール、Windows では `CreateFileW()` + `ReadFile()` API を使う。この違いを吸収するのが PAL の役割。

`std` のソースコード内では `sys/` ディレクトリ以下に各プラットフォームの実装が並んでいる:

```
std/src/sys/
├── pal/           # プラットフォーム固有の基本機能
│   ├── unix/      # Linux/macOS/FreeBSD 等
│   ├── windows/   # Windows
│   ├── hermit/    # Hermit OS（組み込みOS）
│   ├── sabos/     # ← SABOS の実装をここに追加！
│   └── ...
├── fs/            # ファイルシステム
│   ├── unix.rs    # Unix 系の実装
│   ├── windows.rs # Windows の実装
│   ├── sabos.rs   # ← 今日追加！
│   └── unsupported.rs  # 未対応プラットフォーム用
├── stdio/         # 標準入出力
├── alloc/         # メモリアロケータ
└── random/        # 乱数生成
```

`cfg_select!` マクロが `target_os` をチェックして、対応するモジュールを選択する。SABOS は `target_os = "sabos"` で分岐する。対応するモジュールがなければ `unsupported.rs`（全操作が「未対応」エラーを返す）にフォールバックする。

Phase 7 までは stdio と alloc の PAL だけを実装していたので、`std::fs` は unsupported のままだった。今日は fs の PAL を追加して `std::fs::read_to_string()` 等を使えるようにした。

### 実装内容

#### 新規ファイル: `sys_fs_sabos.rs`

SABOS のハンドルベース syscall を使って std::fs のインターフェースを実装。具体的にはこのファイルが提供するもの:

| std API | SABOS syscall |
|---------|---------------|
| `File::open()` | SYS_OPEN(70) でハンドルを取得 |
| `File::read()` | SYS_HANDLE_READ(71) |
| `File::write()` | SYS_HANDLE_WRITE(72) |
| `Drop for File` | SYS_HANDLE_CLOSE(73) でハンドルをクローズ |
| `File::seek()` | SYS_HANDLE_SEEK(78) |
| `File::file_attr()` | SYS_HANDLE_STAT(77) |
| `fs::metadata()` | open → stat → close |
| `fs::read_dir()` | SYS_DIR_LIST(13) → パース |
| `fs::remove_file()` | SYS_FILE_DELETE(12) |
| `DirBuilder::mkdir()` | SYS_DIR_CREATE(15) |
| `fs::remove_dir()` | SYS_DIR_REMOVE(16) |

`rename` / `symlink` / `set_permissions` 等の SABOS 未対応操作は `unsupported()` を返す。

#### 新規ファイル: `os_sabos_mod.rs` / `os_sabos_ffi.rs`

`std::os::sabos` モジュール。`OsStr` / `OsString` のバイト列変換トレイト (`OsStrExt::as_bytes()`, `OsStringExt::from_vec()`) を提供する。Unix 系と同じく `OsStr` の内部表現はバイト列そのままなので、Unix の実装を `#[path = "../unix/ffi/os_str.rs"]` で再利用している。

### ぶつかった壁: General Protection Fault (#GP)

fs の実装自体はスムーズに進んだが、実行時に **GPF（一般保護例外）** が発生して苦戦した。

#### 症状

`println!` までは正常に動作するが、`std::fs::read_to_string()` を呼んだ瞬間に GPF。

```
CPU EXCEPTION: GENERAL PROTECTION FAULT (#GP)
instruction_pointer: 0x403260
  movaps %xmm0,(%rcx)   ← GPF の原因
```

#### 原因

`movaps` は SSE の **アラインドメモリ移動命令** で、メモリオペランドが 16 バイト境界にアラインされていないと GPF になる。

問題は `_start` 関数のスタックアラインメントにあった:

- SABOS カーネルは `iretq` でユーザープロセスを起動する
- `iretq` で設定される RSP は 16 バイトアライン（16n）
- しかし System V ABI では、`call` 命令がリターンアドレスを push するため、関数エントリでは RSP = 16n - 8 が前提
- `_start` は `call` ではなく `iretq` で直接ジャンプされるので、この 8 バイトのずれが生じる
- コンパイラは RSP = 16n - 8 を前提にスタックフレームを構築するため、`movaps` のメモリオペランドが 16 バイト境界からずれて GPF になる

```
期待: call → push RIP → RSP = 16n - 8 → push rbx → RSP = 16n - 16 = 16の倍数
実際: iretq → RSP = 16n → push rbx → RSP = 16n - 8 ← ずれている！
```

#### 修正

`_start` をアセンブリで書いて、RSP を明示的に 16 バイトアラインしてから Rust 関数を `call` する方式に変更。`call` 命令自体がリターンアドレスを push するので、Rust 関数のエントリでは正しく RSP = 16n - 8 になる。

```rust
// 修正前: extern "C" fn _start() で直接 main を呼ぶ
// → iretq からの RSP = 16n が前提と合わず GPF

// 修正後: アセンブリで RSP を調整してから call する
core::arch::global_asm!(
    ".global _start",
    "_start:",
    "and rsp, -16",        // RSP を 16 バイト境界に揃える
    "call _start_rust",    // call が RIP を push → RSP = 16n - 8
    "ud2",
);
```

以前の `println!` が動いていたのは、println! の内部でたまたま `movaps` が使われなかっただけ。fs の操作（`SabosHandle` のゼロ初期化で `movaps` が使われた）で初めて問題が顕在化した。

### テスト結果

```
HELLOSTD.ELF test PASSED
  Arithmetic output OK
  Vec/alloc output OK
  fs::read_to_string OK     ← NEW!
  fs::write OK               ← NEW!
  fs::read_back OK           ← NEW!
  fs::metadata OK            ← NEW!
=== SELFTEST END: 40/40 PASSED ===
All tests PASSED!
```

### 学んだこと

- **PAL の構造を理解できた**。`std` がどのようにプラットフォームの差異を吸収しているか。`cfg_select!` でモジュールを切り替える仕組み。
- **System V ABI のスタックアラインメントの重要性**。`call` ではなく `iretq` で関数にジャンプする場合、RSP の初期値が通常の関数呼び出しと異なる。これが SSE 命令の `movaps` で GPF を引き起こす。
- **`movaps` と `movups` の違い**。コンパイラが最適化で `movaps`（アラインド）を使うことがあり、スタックアラインメントが崩れているとクラッシュする。`movups`（アンアラインド）なら問題にならないが、コンパイラの判断次第。
- **iretq で起動されるユーザープログラムの _start は特殊**。普通の関数呼び出しのように `call` で呼ばれるわけではないので、スタックの初期状態が異なる。Linux でも ELF の `_start` は同じ問題があり、glibc の `_start` はアセンブリで書かれている。

### 用語解説: 今日出てきた概念

#### System V ABI とは

**ABI** は「Application Binary Interface」の略で、コンパイルされたバイナリ同士がどうやりとりするかのルール。System V ABI は Unix 系 OS（Linux, macOS, FreeBSD など）で使われている x86_64 向けの ABI 規約。「System V」は歴史的な UNIX のバージョン名 (System V = AT&T が作った商用 UNIX) に由来するが、今は事実上の x86_64 の標準規約になっている。

System V ABI が決めている主なルール:
- **関数呼び出し時の引数の渡し方**: 最初の 6 個の整数/ポインタ引数は `rdi, rsi, rdx, rcx, r8, r9` レジスタで渡す。7 個目以降はスタック経由。
- **戻り値**: `rax` レジスタで返す。
- **callee-saved レジスタ**: `rbx, rbp, r12-r15` は呼ばれた側が保存する義務がある。
- **スタックアラインメント**: `call` 命令を実行する直前に RSP は 16 の倍数でなければならない。`call` がリターンアドレスを push するので、関数エントリでは RSP = 16n - 8 になる。

一方、Windows は **Microsoft x64 ABI** という別の規約を使う。引数は `rcx, rdx, r8, r9` で渡す（順番が違う）。SABOS カーネルは UEFI ターゲットでビルドされるため Microsoft x64 ABI だが、ユーザープログラムは `x86_64-sabos` ターゲット（System V ABI）でビルドされる。この二つの ABI が混在しているため、syscall ハンドラのアセンブリではレジスタの入れ替えが必要になる。

#### SSE と movaps / movups

**SSE** (Streaming SIMD Extensions) は x86 CPU の拡張命令セット。128 ビット幅の XMM レジスタ（`xmm0`〜`xmm15`）を使って、浮動小数点演算やデータの並列処理を行う。

- `movaps` = **Move Aligned Packed Single**: 128 ビット（16 バイト）のデータをメモリとレジスタ間でコピーする。**メモリアドレスは 16 バイト境界にアラインされている必要がある。** アラインされていないと GPF。
- `movups` = **Move Unaligned Packed Single**: 同じ操作だが、アラインメント制約がない。その分わずかに遅い（現代の CPU ではほぼ同じ速度）。

コンパイラは「スタックは常に 16 バイトアラインされている」と仮定して最適化するため、構造体のゼロ初期化やメモリコピーで `movaps` を積極的に使う。今回はこの仮定が崩れたため GPF になった。

#### iretq

**iretq** (Interrupt Return Quad) は x86_64 の割り込みからの復帰命令。割り込みハンドラ（カーネル空間）からユーザープログラム（ユーザー空間）に戻るために使う。SABOS ではプロセスの初回起動時にも iretq を使って Ring 0 (カーネル) → Ring 3 (ユーザー) の遷移を行う。

iretq はスタックから以下の順序で 5 つの値を pop する:
1. RIP（復帰先アドレス）
2. CS（コードセグメントセレクタ）
3. RFLAGS（フラグレジスタ）
4. RSP（復帰先スタックポインタ）
5. SS（スタックセグメントセレクタ）

通常の関数呼び出しでは `call` → `ret` の対で RIP だけを保存/復帰するが、iretq は特権レベルの切り替え（Ring 0 → Ring 3）も伴うため、CS/SS/RSP/RFLAGS も含めて一括で復帰する。

## PAL time の実装

fs が動いたので、次は time。Phase 8 の TODO リストで次に来ていた `PAL time の実装` に取り掛かった。

### やったこと

`std::time::Instant::now()` と `elapsed()` を動くようにした。SABOS にはすでに `SYS_CLOCK_MONOTONIC`（syscall 26）が実装されていて、起動からの経過ミリ秒を返す。これを PAL の time モジュールに接続するだけなので、fs に比べると圧倒的に簡単だった。

作ったファイルは `rust-std-sabos/sys_time_sabos.rs` の1つだけ。中身は：

- **Instant**: `Duration` を内部に持つ。`now()` で `SYS_CLOCK_MONOTONIC` を呼んで `Duration::from_millis(ms)` で変換。`checked_sub_instant` / `checked_add_duration` / `checked_sub_duration` は Duration の演算に委譲するだけ。
- **SystemTime**: SABOS にはリアルタイムクロック (RTC) がまだないので `now()` は panic する。将来 RTC を実装したら対応できる。

パッチスクリプトの更新パターンも fs と全く同じで、`apply-sysroot-patches.py` に `patch_time_mod()` を追加して `sys/time/mod.rs` の `_ => {` の前に sabos ブランチを挿入、`patch-rust-sysroot.sh` にファイルコピーを1行追加。

### テスト

`user-std/src/main.rs` に2つのテストを追加：

1. **`time::Instant OK`**: `Instant::now()` → ループで計算 → `elapsed()` が動作することを確認
2. **`time::monotonic OK`**: 2回 `now()` を呼んで t2 >= t1 であること（単調増加性）を確認

`make test` で selftest 40/40 + HELLOSTD.ELF の全テスト（fs 含む）が PASS。特にハマりどころなく、30分もかからず完了。fs の実装で PAL モジュール追加のパターンが完全に身についたので、同じ手順をなぞるだけだった。

### PIT タイマーの精度について

SABOS の `SYS_CLOCK_MONOTONIC` は PIT (Programmable Interval Timer) のティックカウントをミリ秒に変換している。PIT のデフォルト周波数は 1193182 Hz / 65536 ≈ 18.2 Hz で、1ティック ≈ 55ms。変換式は `ms = ticks * 10000 / 182`。つまり精度は約55ms単位で、ミリ秒精度というよりは「数十ミリ秒精度」。高精度なベンチマークには使えないが、タイムアウト管理やおおまかな経過時間の計測には十分。将来 HPET や TSC を使えばナノ秒精度も実現できる。

## PAL os の充実 + env の実装

time の次は os と env。これも既存の syscall に繋ぐだけだったので比較的スムーズ。

### os モジュールの改善

`sys_pal_sabos_os.rs` を更新して、これまで `unsupported()` や `panic!()` だった関数を実用的な値を返すようにした:

- **`getcwd()`**: SABOS はフラット FAT32 でディレクトリ階層の概念が薄いので、常に `/` を返す。Hermit OS も同じアプローチ（ルートを返す）なので妥当。
- **`temp_dir()`**: これも `/` を返す。panic するよりはるかに良い。
- **`home_dir()`**: `None` → `Some("/")` に変更。

`chdir()` と `current_exe()` はカーネル側に対応する syscall がないので unsupported のまま。

### env モジュールの新規作成

SABOS カーネルにはすでに `SYS_GETENV(37)` と `SYS_SETENV(38)` が実装されていて、タスクごとに `Vec<(String, String)>` で環境変数を管理している。spawn 時に親プロセスの環境変数がコピーされる仕組みも入っている。あとは PAL に繋ぐだけだった。

`rust-std-sabos/sys_env_sabos.rs` を新規作成:

- **`getenv(key)`**: SYS_GETENV(37) を呼ぶ。まず 256 バイトバッファで試し、BufferOverflow なら 4096 バイトで再試行。
- **`setenv(key, value)`**: SYS_SETENV(38) を呼ぶ。
- **`unsetenv(key)`**: SYS_SETENV で空の値を設定する擬似的な削除。完全な削除は将来 SYS_UNSETENV を追加して対応する。
- **`env()`**: 全環境変数の一覧を返すイテレータだが、SABOS に一覧取得 syscall がないので空の Vec を返す。`common::Env` 構造体を使う。

パッチスクリプトでは `sys/env/mod.rs` に sabos ブランチを追加するだけでなく、`common` モジュールの `#[cfg(any(...))]` 条件にも `target_os = "sabos"` を追加する必要があった。`common::Env` は条件付きコンパイルされる共有モジュールで、`Vec<(OsString, OsString)>` ベースのイテレータを提供する。

### テスト

`user-std/src/main.rs` に2つのテストを追加:

1. **`env::current_dir OK: /`** — `std::env::current_dir()` が "/" を返す
2. **`env::var OK: SABOS_TEST=hello_env`** — `set_var` → `var` の往復テスト

ビルドでは `std::env::set_var()` が最近の Rust (1.66+) で `unsafe` になっていて最初のビルドが失敗した。マルチスレッド環境でのデータ競合を防ぐための変更で、SABOS はシングルスレッドなので `unsafe` ブロックで囲んで解決。

もう1つハマったのが sysroot キャッシュの問題。`make build-user-std` はインクリメンタルビルドで、sysroot のソースを変更しても std の .rlib が再コンパイルされないことがある。`cargo clean` してからビルドし直す必要があった。これは fs のときにも起きた問題で、PAL モジュールを追加するたびに `cargo clean` が必要。

## sysroot キャッシュ問題の根本対策

fs → time → os/env と PAL モジュールを追加するたびに「sysroot パッチを当てたのにビルドに反映されない → cargo clean が必要」という問題に何度もハマった。毎回手動で clean するのは面倒だし、忘れてテストが通らないと原因究明に時間がかかる。

そこで `make build-user-std` に自動検出の仕組みを入れた。`rust-std-sabos/` ディレクトリ内の全ファイルの sha256 ハッシュを `user-std/.sysroot-hash` に記録しておき、次回ビルド時にハッシュを再計算して比較する。変更があれば自動で `cargo clean` を実行してからビルドする。変更がなければインクリメンタルビルド（0.03秒）のまま。

```makefile
@NEW_HASH=$$(find rust-std-sabos/ -type f | sort | xargs sha256sum | sha256sum | cut -d' ' -f1); \
OLD_HASH=$$(cat $(SYSROOT_HASH_FILE) 2>/dev/null || echo ""); \
if [ "$$NEW_HASH" != "$$OLD_HASH" ]; then \
    echo "sysroot パッチが変更されたため cargo clean を実行..."; \
    cd user-std && cargo clean; \
    ...
fi
```

これで PAL モジュールを追加・修正するたびに手動 `cargo clean` する必要がなくなった。開発体験がだいぶ改善された。

## PAL net の実装: std::net を SABOS で動かす

### やりたいこと

Phase 8 の最後の項目。SABOS にはすでに netd デーモンがユーザー空間で動いており、IPC 経由で DNS / TCP の操作ができる。これを Rust std の PAL net インターフェースに接続して、`std::net::TcpStream::connect()` や `("example.com", 80).to_socket_addrs()` が動くようにする。

### 設計方針

PAL net の実装は他の PAL（fs, time, env）と少し違う。ファイルシステムや時刻は直接 syscall を呼べばいいが、ネットワークは **netd というユーザー空間デーモンとの IPC 通信**が必要になる。つまり PAL の中で IPC プロトコルを組み立てて、`SYS_IPC_SEND(90)` / `SYS_IPC_RECV(91)` syscall を呼ぶという、ちょっと変わった構成になる。

```
std::net::TcpStream::connect("example.com:80")
    ↓
PAL net (sys/net/connection/sabos.rs)
    ↓  IPC メッセージ
    ↓  SYS_IPC_SEND → netd
    ↓  SYS_IPC_RECV ← netd
    ↓
netd (ユーザー空間デーモン)
    ↓
virtio-net → QEMU SLIRP → インターネット
```

netd のタスク ID は `SYS_GET_TASK_LIST(21)` でタスク一覧 JSON を取得して "NETD.ELF" を検索する。見つけた ID は `AtomicU64` でキャッシュし、IPC 送信失敗時にはキャッシュをクリアして再検索 + 1 回リトライする仕組みにした。

### 実装内容

新規ファイル `rust-std-sabos/sys_net_connection_sabos.rs` に以下を実装した:

- **TcpStream**: netd の conn_id をフィールドに持つ。connect で TCP_CONNECT(opcode 2) を送信、read で TCP_RECV(opcode 4)、write で TCP_SEND(opcode 3)、Drop で TCP_CLOSE(opcode 5)。read_timeout / write_timeout はローカルフィールドで管理（`&self` しかないので unsafe で interior mutability）。
- **TcpListener**: bind で TCP_LISTEN(opcode 6)、accept で TCP_ACCEPT(opcode 7)。
- **LookupHost**: DNS_LOOKUP(opcode 1) で解決。IP アドレスリテラルの場合は直接パース。
- **UdpSocket**: unsupported（never type `!` 型のまま）。netd が UDP 未対応のため。
- **netd_request**: IPC リクエスト/レスポンスの共通関数。リクエストヘッダ（opcode + payload_len）とレスポンスヘッダ（opcode + status + data_len）のパースを行う。

### つまずいたところ

#### syscall 番号の間違い

最初 `SYS_GET_TASK_LIST` の番号を `36` と書いてしまっていた。正しくは `21`。カーネルの syscall.rs と user の syscall.rs で番号が定義されているが、PAL は独立した実装なので番号を自前で定義する必要がある。番号を間違えると「failed to get task list」というエラーになって netd が見つからない → 全 net 操作が失敗する。

ここで「syscall 番号を定数として一元管理する仕組みがあればよかった」と反省。PAL は sysroot パッチとして Rust std のソースツリーに配置されるので、user クレートの定数を直接参照することはできない。将来的には共有定数ファイルを用意するか、ビルドスクリプトで生成するのがいいかもしれない。

#### TCP 接続のタイムアウト

HELLOSTD.ELF から `std::net::TcpStream::connect("93.184.216.34:80")` を試したところ、netd からのレスポンスが IPC タイムアウト（5秒 → 10秒に延長しても）内に返ってこなかった。一方、既存の no_std バイナリ（shell の selftest_net）では同じ操作が成功する。

原因は QEMU SLIRP NAT 越しの外部 TCP 接続が時間がかかることと、HELLOSTD.ELF が前のテスト（fs, time, env）を実行した後で netd と通信するためタイミングが変わること。CI テストとして安定しないため、HELLOSTD.ELF の TCP テストは SocketAddr のパーステストに変更し、実際の TCP 通信テストは既存の selftest_net に委ねることにした。

DNS lookup は成功している（`net::lookup OK: 104.18.26.120:80`）ので、PAL → IPC → netd → DNS の経路は正常に動作している。

### テスト結果

```
HELLOSTD.ELF test PASSED
  net::lookup OK       ← DNS 解決が std::net::ToSocketAddrs 経由で動作
  net::tcp_parse OK    ← SocketAddr パース + std::net の型が使える

selftest: 40/40 PASSED
net selftest: 4/4 PASSED  ← 既存の TCP テストも影響なし
```

### 変更ファイル

| ファイル | 操作 |
|---------|------|
| `rust-std-sabos/sys_net_connection_sabos.rs` | 新規作成（PAL net 実装） |
| `scripts/apply-sysroot-patches.py` | `patch_net_connection_mod()` 追加 |
| `scripts/patch-rust-sysroot.sh` | sabos.rs コピー追加 |
| `user-std/src/main.rs` | net テスト追加（DNS + SocketAddr パース） |
| `scripts/run-selftest.sh` | net テストパターン追加 |
| `TODO_std.md` | PAL net を完了にマーク |

これで Phase 8（PAL の充実）の全項目が完了！`std::fs`, `std::time`, `std::env`, `std::net` が一通り使えるようになった。

## syscall 番号の一元管理: 定義ずれ（drift）の構造的防止

### 問題

PAL net 実装時に `SYS_GET_TASK_LIST` の番号を `36` と書いてしまった話（上記参照）から、これは構造的に対策すべき問題だと気づいた。現状、syscall 番号は 3 箇所に独立定義されている:

| 場所 | 形式 |
|------|------|
| `kernel/src/syscall.rs` | `pub const SYS_*: u64` |
| `user/src/syscall.rs` | `pub const SYS_*: u64` |
| `user/src/syscall_netd.rs` | `pub const SYS_*: u64`（サブセット） |
| `rust-std-sabos/*.rs` | `const SYS_*` やインライン asm リテラル |

手動コピーで同期するのは人間がやる限りいつか間違える。今後 PAL を追加するたびに同じリスクがある。

### 対策: 2 本柱

**柱 1: 共有クレート `libs/sabos-syscall/`**

kernel と user の定数を一元化する `#![no_std]` クレートを作成した。`libs/blockdev/` や `libs/fat-core/` と同じパターン。

```
libs/sabos-syscall/
├── Cargo.toml
└── src/
    └── lib.rs    ← 44 個の SYS_* 定数の唯一の定義場所
```

kernel と user の Cargo.toml に `sabos-syscall = { path = "../libs/sabos-syscall" }` を追加し、元の定数定義を `pub use sabos_syscall::*;` に置換。これで kernel と user の番号がコンパイル時に一致することが保証される。

```rust
// kernel/src/syscall.rs（変更後）
/// sabos-syscall クレートで一元管理している。
pub use sabos_syscall::*;
```

`user/src/syscall_netd.rs` も同様に、必要な 6 個の定数を `pub use sabos_syscall::{...};` で参照するように変更。

**柱 2: PAL 検証スクリプト `scripts/check-syscall-numbers.py`**

PAL ファイル（rust-std-sabos/*.rs）は sysroot パッチとして Rust std のソースツリーに配置されるため、外部 crate に依存できない。番号を直接ハードコードするしかない。そこで Python スクリプトで正定義との整合性を検証する。

スクリプトは 2 つのパターンを検出する:

```python
# パターン1: ローカル定数定義
RE_PAL_CONST = re.compile(r"const (SYS_\w+):\s*u64\s*=\s*(\d+)\s*;")

# パターン2: インライン asm リテラル
RE_PAL_ASM = re.compile(r'in\("rax"\)\s+(\d+)u64\s*,\s*//\s*(SYS_\w+)')
```

### CI 組み込み

- `Makefile` に `check-syscall` ターゲットを追加し、`test` の依存に組み込んだ。`make test` すると QEMU 起動前にまず syscall 番号の検証が走る。
- `.github/workflows/build.yml` の build ジョブにも `python3 scripts/check-syscall-numbers.py` ステップを追加。

### 検証結果

```
$ python3 scripts/check-syscall-numbers.py
Loaded 44 syscall definitions from canonical source
PASSED: all PAL syscall numbers match canonical source

$ make test
...
=== SELFTEST END: 22/22 PASSED ===
All tests PASSED!
```

### 開発サイクルの振り返り

今日の作業で改善すべきだったポイント:

1. **syscall 番号の重複定義** → 共有クレートで解消（今回の対策）
2. **PAL ファイルの番号検証** → CI スクリプトで解消（今回の対策）

今回のように「バグの原因が構造的な問題にある」と気づいたら、場当たり的な修正ではなく仕組みで防ぐのが大事。手動コピーは必ずズレる。コンパイル時保証とCIチェックの二重防護で、今後の syscall 追加時に番号ずれが起きない仕組みができた。

---

## Phase 9: コマンドライン引数 + 外部クレート対応

### 目標

1. `std::env::args()` を動かす（多くのクレートの前提条件）
2. 代表的な外部クレート（`serde_json`）がビルド・動作するか確認

### 調査: カーネル側は既に引数対応済み

実装前の調査で重要な発見があった。**引数渡しのインフラはカーネル側で完成していた**。

- `setup_user_stack_args()` が argc/argv/envp をスタックに配置済み
- `jump_to_usermode` が rdi=argc, rsi=argv, rdx=envp をレジスタセット済み
- `spawn_user()` が args スライスを受け取れる
- no_std の `exit0.rs` は `_start(argc, argv, envp)` で受け取って検証済み（exec_args テスト PASS）

つまり、足りなかったのは:
1. PAL の `_start_rust()` が `main(0, null)` をハードコードしていた（レジスタの argc/argv を無視）
2. std の `sys/args` モジュールが SABOS に存在しなかった（unsupported フォールバック）
3. SYS_EXEC / SYS_SPAWN が 2 引数のみ（ユーザー空間から引数を渡す手段がなかった）

### タスク 1: PAL の _start_rust を修正

`_start` アセンブリでは `and rsp, -16` → `call _start_rust` を実行する。iretq 後の rdi/rsi は System V ABI（x86-64 の C 関数呼び出し規約）に従ってそのまま第1・第2引数として渡る。`and rsp, -16` は rsp しか変更しないので rdi/rsi は保持される。つまり修正は **_start_rust の引数を変えるだけ**。

```rust
// 変更前
pub unsafe extern "C" fn _start_rust() -> ! {
    let result = unsafe { main(0, core::ptr::null()) };

// 変更後
pub unsafe extern "C" fn _start_rust(argc: isize, argv: *const *const u8) -> ! {
    let result = unsafe { main(argc, argv) };
```

さらに `sys/args/sabos.rs` を新規作成。Unix 実装と同じく Atomic 変数で argc/argv を保存し、`std::env::args()` から遅延取得する仕組み。パッチスクリプト（`apply-sysroot-patches.py`）に args/mod.rs への sabos ブランチ追加も実装。

### タスク 2: SYS_EXEC / SYS_SPAWN を 4 引数に拡張

既存の SYS_EXEC(30) / SYS_SPAWN(31) に arg3=args_ptr, arg4=args_len を追加。arg3=0 なら従来通り。

引数バッファのフォーマットは SABOS の設計哲学に従い、null 終端ではなく**長さプレフィックス形式**:

```
[u16 len][bytes][u16 len][bytes]...
```

各引数は「2バイトのリトルエンディアン長さ」+「その長さ分のバイト列」で連続配置。カーネル側で `parse_args_buffer()` がこれをパースして `Vec<String>` に変換し、`spawn_user()` に渡す。

ユーザー側は `build_args_buffer()` で固定サイズスタックバッファ（1KB）に引数を書き込む。`alloc` に依存しない実装にしたのは、`exit0.rs` のような no_std + no alloc のバイナリでもコンパイルが通るようにするため。

### タスク 3: シェルの引数対応

`run /HELLO.ELF arg1 arg2` のようにスペース区切りで引数を渡せるようにした。

### タスク 4: serde_json の外部クレート対応

ここで大きな壁にぶつかった。**`restricted_std` 問題**だ。

SABOS はカスタムターゲット（`x86_64-sabos.json`）なので、Rust の std は `restricted_std` として unstable 扱いされる。自分の `main.rs` は `#![feature(restricted_std)]` で回避できるが、外部クレート（serde, memchr 等）が `use std::...` するとコンパイルエラーになる。

原因は std の `build.rs` にハードコードされた OS ホワイトリスト。sabos がリストにないので `cfg(restricted_std)` が立ち、std 全体が unstable feature として扱われる。

いくつか試した:
- `RUSTFLAGS='--cfg feature="restricted_std"'` → feature gate は cfg では効かない
- `RUSTC_BOOTSTRAP=1` → 効かない
- `-Z force-unstable-if-unmarked` → 効かない

最終的に、std の build.rs に存在する**公式のエスケープハッチ**を発見:

```rust
// std/build.rs より
|| env::var("RUSTC_BOOTSTRAP_SYNTHETIC_TARGET").is_ok()
```

`RUSTC_BOOTSTRAP_SYNTHETIC_TARGET=1` を設定すると、カスタムターゲットでも unrestricted に std を使えるようになる。これで serde_json を含む全依存がビルドを通った。

### テスト結果

```
args::count OK
args::argv0 OK
serde::to_string OK: {"x":1,"y":2}
serde::from_str OK: Point { x: 1, y: 2 }
```

全 40 テスト PASS。serde_json の JSON シリアライズ/デシリアライズも正常動作。

### 学び

1. **System V ABI の理解**: `call` 命令は rdi/rsi をそのまま引数として渡す。`and rsp, -16` は rsp しか変更しない。この ABI の理解があれば `_start_rust` の修正は自明だった。

2. **restricted_std の仕組み**: std の build.rs に OS ホワイトリストがあり、カスタムターゲットは自動的に `restricted_std` になる。`RUSTC_BOOTSTRAP_SYNTHETIC_TARGET` がエスケープハッチ。

3. **alloc 依存の罠**: user/src/syscall.rs は全バイナリで共有されるので、`Vec` を使うと no_std + no alloc のバイナリ（exit0.rs）でコンパイルエラーになる。固定サイズスタックバッファで解決。

### 開発サイクルの振り返り

1. **カーネル側の既存実装の調査が重要** — 「足りないもの」を実装する前に「既にあるもの」を調べたことで、最小限の変更で目標を達成できた。

2. **段階的なコミット** — タスクごとにコミットすることで、問題が起きたときの切り分けが容易になった。

---

## env::vars() の実装（SYS_LISTENV）

Phase 9 が終わったので、TODO_std.md の残タスクから一番簡単そうな `env::vars()` に着手した。
難易度 ★☆☆☆☆ の通り、カーネル側の `env_vars: Vec<(String, String)>` を読んで返すだけなので素直に実装できた。

### やったこと

1. **SYS_LISTENV(39) syscall の追加** — `libs/sabos-syscall/src/lib.rs` に定数追加
2. **カーネル側ハンドラ** — `scheduler::list_env_vars()` で "KEY=VALUE\n" 形式の文字列を構築し、`sys_listenv()` でユーザーバッファに書き込む
3. **user/src/syscall.rs** — `listenv()` ラッパーを追加
4. **PAL 更新** — `rust-std-sabos/sys_env_sabos.rs` の `env()` 関数を空の Vec を返すのではなく、SYS_LISTENV を呼んでパースするように変更
5. **E2E テスト** — `user-std/src/main.rs` で `std::env::vars()` を呼び、直前に `set_var` した `SABOS_TEST` が含まれることを確認

### バッファ形式

"KEY=VALUE\n" の繰り返しを採用。シンプルで実装も容易。
PAL 側では `split(|&b| b == b'\n')` で行分割し、各行の最初の `=` で key/value を分割する。

### テスト結果

```
env::vars OK: count=1
env::vars_contains OK: SABOS_TEST found
```

全 40 テスト + std テスト全項目 PASS。

### 学び

- カーネル→ユーザー空間のデータ受け渡しパターンが確立されてきた。バッファサイズ不足時は `-4 (BufferOverflow)` を返し、PAL 側で大きなバッファにリトライする。GETENV、DIR_LIST、LISTENV すべて同じパターン。
- PAL ファイルは `check-syscall-numbers.py` でインラインアセンブリの番号が自動検証されるので、番号のドリフトを心配せずに済む。

---

## PAL process の実装（std::process::Command）

env::vars() の次は PAL process（★★☆☆☆）。`std::process::Command` を動かす。

### やったこと

1. **`rust-std-sabos/sys_process_sabos.rs` を新規作成** — Rust std の `sys/process/` に SABOS 実装を追加
   - `Command` 構造体: プログラム名と引数を保持し、`spawn()` で SYS_SPAWN を呼ぶ
   - `Process` 構造体: タスク ID を保持し、`wait()` で SYS_WAIT、`kill()` で SYS_KILL を呼ぶ
   - `ExitStatus` 構造体: 終了コードを保持し、`success()` / `code()` を提供
   - パイプ（stdin/stdout/stderr リダイレクト）は SABOS にないので全て None

2. **パッチスクリプト更新** — `apply-sysroot-patches.py` に `patch_process_mod()` 追加、`patch-rust-sysroot.sh` にファイルコピー追加

3. **E2E テスト** — `user-std/src/main.rs` で EXIT0.ELF を使って:
   - `Command::new("/EXIT0.ELF").status()` → exit_code=0 確認
   - `Command::new("/EXIT0.ELF").spawn()` + `child.wait()` → id 取得 + success 確認

### 設計判断

unsupported.rs をベースにしつつ、`spawn()` だけを実装する方針にした。
Unix 実装は fork/exec ベースで複雑だが、SABOS はシンプルに SYS_SPAWN 一発で済む。

引数バッファの構築は既存の SABOS 長さプレフィックス形式（`[u16 LE len][bytes]...`）を使い、
user/src/syscall.rs の `build_args_buffer` と同じロジックを PAL 側にも書いた。

### テスト結果

```
process::status OK: exit_code=0
process::spawn OK: id=XX
process::wait OK: success=true
```

全テスト PASS。

---

## SystemTime の実装（CMOS RTC + SYS_CLOCK_REALTIME）

TODO に ★★★☆☆ と書いていたが、思ったより素直にいけた。

### CMOS RTC とは

CMOS RTC（Complementary Metal-Oxide-Semiconductor Real-Time Clock）は、x86 PC に搭載されている電池駆動のリアルタイムクロックで、PC の電源が切れていても時刻を保持する。I/O ポート 0x70（インデックス）と 0x71（データ）でアクセスする。

レジスタ構成はシンプルで、秒(0x00)、分(0x02)、時(0x04)、日(0x07)、月(0x08)、年(0x09)、世紀(0x32) が BCD（二進化十進数）形式で格納されている。

### やったこと

1. **`kernel/src/rtc.rs` を新規作成** — CMOS RTC ドライバ
   - `cmos_read()`: ポート 0x70/0x71 でレジスタ読み取り
   - `bcd_to_binary()`: BCD → バイナリ変換（`0x59` → `59`）
   - `wait_for_uip_clear()`: UIP（Update In Progress）フラグの確認。RTC がレジスタ更新中のときは読むと不整合なデータになるので、更新が完了するまで待つ
   - `read_rtc_raw()`: 2 回連続で同じ値が読めるまでリトライ（整合性保証）
   - `datetime_to_unix_epoch()`: Gregorian 暦から UNIX エポック秒への変換。閏年の計算を含む
   - `read_unix_epoch_seconds()`: 上記を組み合わせたエントリポイント

2. **SYS_CLOCK_REALTIME(130)** — 新しいシステムコール番号帯「時刻(130-139)」を追加

3. **PAL time 更新** — `SystemTime::now()` の panic を `SYS_CLOCK_REALTIME` 呼び出しに置き換え

4. **テスト** — user-std で `SystemTime::now().duration_since(UNIX_EPOCH)` を呼び、2020 年以降であることを確認

### BCD って何？

BCD (Binary-Coded Decimal) は、10進数の各桁を 4 ビットで表現する形式。たとえば `59` は `0101 1001` (= 0x59) になる。人間には読みやすいが、プログラムでの計算には不便なので、`((bcd >> 4) * 10) + (bcd & 0x0F)` でバイナリに変換する。CMOS RTC がこの古い形式を使っているのは、1980 年代の MC146818 チップの設計がそのまま残っているため。

### UIP フラグと整合性

CMOS RTC は 1 秒に 1 回レジスタを更新する。更新中（約 244 マイクロ秒間）にレジスタを読むと、秒が 59 なのに分が既にインクリメントされている、といった不整合が起きる。これを防ぐために:

1. ステータスレジスタ A のビット 7（UIP フラグ）が 0 になるまで待つ
2. 全レジスタを読み取る
3. もう一度全レジスタを読んで一致するか確認（一致しなければリトライ）

### テスト結果

```
time::SystemTime OK: epoch_secs=17XXXXXXXX
```

QEMU は `-rtc base=utc` がデフォルトで、ホストの UTC 時刻を RTC に設定する。2026 年の UNIX エポック秒が正しく返ることを確認。selftest にも `clock_realtime` テストを追加（2020〜2100 年の範囲チェック）。

## PAL thread の実装: std::thread::spawn() が動いた！

### 目標

`std::thread::spawn()` と `join()` を動かす。カーネル側には既に SYS_THREAD_CREATE(110) / SYS_THREAD_EXIT(111) / SYS_THREAD_JOIN(112) が実装済みで、no_std のユーザープログラムからはテスト済み。PAL（Platform Abstraction Layer）で std のスレッド API に接続する。

### 実装のポイント

#### ThreadInit::init() のバイパス

std::thread の内部では、新しいスレッドが開始すると `ThreadInit::init()` を呼んで `set_current()` でスレッドハンドルを登録する。しかし SABOS は thread_local で `no_threads` モード（Cell ベース）を使っており、thread-local 変数が実際にはグローバルな Cell になる。メインスレッドで既に CURRENT が設定済みのため、子スレッドで `set_current()` を呼ぶと `rtabort!` でクラッシュする。

対策として、`init.init()` をスキップし、ThreadInit を destructure して `rust_start` クロージャを直接取り出す方式にした。Hermit OS の実装を参考にしつつ、`no_threads` モード固有の制約を回避した。

```rust
let ThreadInit { handle: _handle, rust_start } = *init;
rust_start();
syscall_thread_exit(0);
```

これにより `std::thread::current()` はスポーンしたスレッドからメインスレッドのハンドルを返す制約があるが、基本的な spawn/join は問題なく動作する。将来 thread_local を `thread_local_key` モードに切り替えれば制約は解消できる。

#### スタックアラインメントの罠

最初の実装ではスレッドが起動直後にクラッシュした。原因は **スタックアラインメント** だった。

iretq でスレッドのエントリポイントにジャンプすると RSP = 16n（16 の倍数）になる。しかし System V ABI では、関数エントリで RSP = 16n - 8 が期待される（call 命令がリターンアドレスを push するため）。`_start` では `and rsp, -16; call _start_rust` というアセンブリトランポリンで対処していたが、最初のスレッド実装ではこれを忘れていた。

修正として、`_start` と同じパターンのアセンブリトランポリンを作成:

```asm
_thread_entry_trampoline:
    and rsp, -16
    call _thread_entry_rust   // rdi = arg がそのまま渡る
    ud2
```

この 3 行のアセンブリで、iretq 後の RSP = 16n から call による 8 バイト push で RSP = 16n - 8 となり、ABI 準拠になる。このバグは SSE 命令（movaps 等）の 16 バイトアラインメント要求で GPF として顕在化する。

#### スタック確保と解放

スレッドのユーザー空間スタックは SYS_MMAP(28) で匿名ページとして確保し、join() 後に SYS_MUNMAP(29) で解放する。x86_64 ではスタックは下向きに伸びるため、スタックトップ = 確保した領域の末尾。

### 変更ファイル

| ファイル | 内容 |
|---------|------|
| `rust-std-sabos/sys_thread_sabos.rs` | 新規作成: Thread/spawn/join/yield_now/sleep |
| `scripts/apply-sysroot-patches.py` | patch_thread_mod() 追加 |
| `scripts/patch-rust-sysroot.sh` | sys/thread/sabos.rs コピー追加 |
| `user-std/src/main.rs` | thread E2E テスト追加 |
| `scripts/run-selftest.sh` | thread テストアサーション追加 |
| `TODO_std.md` | PAL thread を完了済みに更新 |

### テスト結果

```
thread::spawn_join OK
thread::return_value OK: 42
thread::yield_now OK
```

AtomicBool で共有変数を使ったスレッド間通信、スレッドからの戻り値受け渡し、yield_now の非クラッシュ確認の 3 テストが全て PASS。

---

## UDP ソケット実装: std::net::UdpSocket が動いた！

### 目標

SABOS の std::net は TCP + DNS が動作していたが、UdpSocket は `UdpSocket(!)` （never type）で全メソッド unsupported だった。netd（ユーザー空間ネットワークデーモン）に UDP プロトコル処理を追加し、`std::net::UdpSocket` を動作させる。

### 現状の分析

実装前に現状を整理した:

- **netstack.rs の `handle_udp()`**: DNS レスポンス（src_port==53）のみを処理して `udp_response` グローバル変数に保存。DNS 以外の UDP パケットは完全に無視されている。
- **PAL の `UdpSocket(!)`**: never type で全メソッドが unreachable。bind すらできない。
- **netd の IPC オペコード 1-7**: DNS_LOOKUP(1) と TCP 系(2-7) のみ。UDP 操作のオペコードがない。

つまり、UDP の「送信」インフラ（`send_udp_packet()`）は既にあるが、「ソケットとして管理する仕組み」がなかった。これを層ごとに積み上げていく。

### 新しい IPC オペコード設計

TCP の 2-7 に続く番号 8-11 を割り当てた:

| Opcode | 名前 | ペイロード | レスポンスデータ |
|--------|------|-----------|----------------|
| 8 | UDP_BIND | `[port: u16 LE]` | `[socket_id: u32 LE][local_port: u16 LE]` |
| 9 | UDP_SEND_TO | `[socket_id: u32][dst_ip: 4B][dst_port: u16 LE][data...]` | なし |
| 10 | UDP_RECV_FROM | `[socket_id: u32][max_len: u32 LE][timeout_ms: u64 LE]` | `[src_ip: 4B][src_port: u16 LE][data...]` |
| 11 | UDP_CLOSE | `[socket_id: u32 LE]` | なし |

設計で気をつけたポイント:
- **port=0 でエフェメラルポート自動割り当て**: bind(0) で 49152〜65535 の範囲から自動で割り当て。std::net の慣例と一致。
- **ID 空間は TCP と共有**: `alloc_conn_id()` は TCP コネクションにも使われるカウンタで、UDP ソケットもここから ID を取得する。TCP と UDP で ID が衝突しない。
- **DNS の既存動作は壊れない**: handle_udp() で「まずバインドされたソケットを探し、なければ従来の DNS レスポンス処理にフォールバック」という優先順序にした。

### 実装の 6 ステップ

各ステップでコミットする方針で進めた:

#### Step 1: netstack.rs に UDP ソケット管理を追加

`UdpSocketEntry` 構造体を追加:
```rust
pub struct UdpSocketEntry {
    pub id: u32,
    pub local_port: u16,
    pub recv_queue: VecDeque<([u8; 4], u16, Vec<u8>)>,
}
```

`recv_queue` は `(送信元IP, 送信元ポート, データ)` のタプルを VecDeque で保持する。TCP の recv_buffer が `Vec<u8>` のストリームなのに対し、UDP はメッセージ境界を保持する必要があるため、各データグラムを個別に保存する。

`handle_udp()` を修正して、宛先ポートにバインドされたソケットがあれば recv_queue に追加するようにした。

#### Step 2: netd.rs にオペコードディスパッチを追加

TCP のディスパッチと全く同じパターンで、ペイロードをパースして `netstack::udp_*` API に委譲する。特に工夫は不要で、既存パターンの複製。

#### Step 3: user/src/net.rs に UdpSocket API を追加

`net::UdpSocket` 構造体は TcpStream と同じ RAII パターンで、Drop でソケットを自動クローズする。`send_to` / `recv_from` / `set_recv_timeout` を実装。

#### Step 4: selftest に UDP テスト追加

QEMU SLIRP 環境で確実に動くテストとして、DNS サーバー (10.0.2.3:53) への手動 UDP クエリを選んだ。DNS プロトコルのバイト列を手組みして send_to → recv_from でレスポンスの ID 一致と送信元アドレスを検証する。

#### Step 5: PAL UdpSocket 実装

ここが一番大きい変更。`UdpSocket(!)` （never type で全メソッドが `self.0` = unreachable）を実際の構造体に置き換えた。

never type は Rust の `!` 型で、「この値は存在し得ない」ことを表す。`pub struct UdpSocket(!);` と定義すると UdpSocket のインスタンスは絶対に作れないため、メソッドの中身は `self.0` と書くだけで「到達不能」が証明される。unsupported なプラットフォーム機能を表現するのに便利なパターンだが、実装するときは構造体を丸ごと書き直す必要がある。

新しい UdpSocket は TcpStream と同じ設計思想で:
- `socket_id: u32` — netd が管理するソケット ID
- `connected_addr: Option<SocketAddr>` — `connect()` で設定されたデフォルト送信先
- `read_timeout / write_timeout` — タイムアウト設定（interior mutability で `&self` から変更）

`send()` / `recv()` は `connected_addr` を使って `send_to()` / `recv_from()` に委譲する。これは std::net::UdpSocket の仕様通り。

#### Step 6: E2E テスト + ドキュメント更新

user-std/src/main.rs に `std::net::UdpSocket` のテストを追加。`UdpSocket::bind("0.0.0.0:0")` でバインドし、DNS クエリを `send_to("10.0.2.3:53")` → `recv_from()` で受信。

### テスト結果

```
=== NET SELFTEST START ===
[PASS] net_init_netd
[PASS] net_addr_types
[PASS] net_dns_lookup
[PASS] net_tcp_http_get
[PASS] net_udp_dns_query      ← NEW!
=== NET SELFTEST END: 5/5 PASSED ===

net::udp_bind OK               ← NEW!
net::udp_send OK: 29 bytes     ← NEW!
net::udp_recv OK: XX bytes from 10.0.2.3:53  ← NEW!
```

no_std selftest (5/5) と std E2E テスト両方で UDP テストが PASS。全テスト成功。

### 学んだこと

- **UDP と TCP の構造の違い**: TCP はストリーム（バイト列）なので recv_buffer は `Vec<u8>` でよいが、UDP はデータグラム（メッセージ境界あり）なので `VecDeque<(src_ip, src_port, data)>` で個別に保持する必要がある。これはネットワークプロトコルの基本だが、実装してみると違いが明確になる。

- **never type `!` のパターン**: `pub struct Foo(!);` で unsupported な機能を表現するのは Rust std の PAL でよく使われるパターン。メソッドの中身を `self.0` と書くだけで unreachable が証明されるのは面白い。ただし実装するときは構造体定義からメソッド全部を書き直す必要がある。

- **既存インフラの再利用が鍵**: `send_udp_packet()` は DNS 用に既に実装されていたので、新しいのはソケット管理と recv_queue のディスパッチだけ。TCP 実装のときに作った IPC パターン（netd_request + opcode + payload parse）もそのまま複製できた。

### 変更ファイル一覧

| ファイル | 操作 |
|---------|------|
| `user/src/netstack.rs` | UdpSocketEntry + udp_bind/send_to/recv_from/close + handle_udp 修正 |
| `user/src/bin/netd.rs` | IPC オペコード 8-11 ディスパッチ |
| `user/src/net.rs` | ユーザー向け UdpSocket API |
| `user/src/bin/shell.rs` | selftest_net に UDP テスト追加 |
| `rust-std-sabos/sys_net_connection_sabos.rs` | PAL UdpSocket 実装（never type → 実構造体） |
| `user-std/src/main.rs` | std::net::UdpSocket E2E テスト |
| `scripts/run-selftest.sh` | UDP テストアサーション追加 |
| `TODO_std.md` | UdpSocket を完了にマーク |

---

## IPv6 基盤実装（Phase 1: ICMPv6 + NDP）

### 目標

SABOS のネットワークスタックは IPv4 のみ対応だった。IPv6 パケットの送受信基盤を作り、`ping6 fec0::2`（QEMU ゲートウェイ）が動作することをゴールとする。

### QEMU SLIRP の IPv6 設定

QEMU の `-netdev user` に `ipv6=on` を追加すると、SLIRP が IPv6 を有効にする:
- ゲスト IPv6: `fec0::15`（IPv4 の 10.0.2.15 に対応）
- ゲートウェイ: `fec0::2`
- DNS: `fec0::3`
- SLIRP が Router Advertisement を送信し、Neighbor Solicitation でゲストの MAC を問い合わせてくる

### ICMPv6 チェックサムの IPv4 との違い

IPv4 の ICMP チェックサムは ICMP データのみから計算するが、ICMPv6 は **IPv6 疑似ヘッダー**を含めて計算する（RFC 4443）。疑似ヘッダーの構造は:

```
Source Address (16B) + Destination Address (16B) + Payload Length (4B, BE) + Zero (3B) + Next Header (1B = 58)
```

これは TCP チェックサムの計算方法と同じパターン（IPv4 でも TCP は疑似ヘッダーを含む）なので、既存の `calculate_checksum()` 関数をそのまま再利用できた。

### NDP (Neighbor Discovery Protocol)

NDP は IPv6 版の ARP に相当する。IPv4 では ARP が MAC アドレス解決を担当するが、IPv6 では ICMPv6 の中に Neighbor Solicitation (NS) と Neighbor Advertisement (NA) というメッセージタイプを定義して同じ機能を実現する。

QEMU SLIRP が送ってくる NS に応答しないと、ゲートウェイがゲストの MAC アドレスを知れず、IPv6 パケットを送れない。NS のターゲットが自分の IPv6 アドレス (fec0::15) に一致する場合、NA を返す実装を入れた。

NA パケットの構造:
```
ICMPv6 Header (4B): type=136, code=0, checksum
Flags (4B): S=1 (Solicited), O=1 (Override) → 0x60000000
Target Address (16B): 自分の IPv6 アドレス
Option: Target Link-Layer Address (8B): type=2, length=1, MAC(6B)
```

### ソリシテッドノードマルチキャスト

NDP の NS パケットは、宛先として**ソリシテッドノードマルチキャスト**アドレス（`ff02::1:ffXX:XXXX`）を使う。これはターゲットアドレスの下位 24 ビットを `ff02::1:ff00:0/104` に付加した形式で、Ethernet のブロードキャストを避けるための仕組み。例えば `fec0::15` のソリシテッドノードは `ff02::1:ff00:15` になる。`handle_ipv6()` ではユニキャスト (MY_IPV6) だけでなく、このマルチキャスト宛のパケットも受信できるようにした。

### IPv6 アドレスパーサー

シェルの `ping6` コマンドのために、IPv6 アドレス文字列を 16 バイトにパースする関数を実装した。`::` の省略記法をサポートする。左右を `::` で分割し、左側のグループと右側のグループの間をゼロで埋める方式。

### 実装構成

| ファイル | 変更内容 |
|---------|---------|
| `Makefile` | QEMU に `ipv6=on` 追加 |
| `user/src/netstack.rs` | IPv6Header + ICMPv6Header + NDP + send_ipv6_packet + ping6 送受信 |
| `user/src/bin/netd.rs` | OPCODE_PING6(12) ディスパッチ |
| `user/src/net.rs` | Ipv6Addr + ping6() API + NetError バリアント追加 |
| `user/src/bin/shell.rs` | ping6 コマンド + IPv6 アドレスパーサー + selftest IPv6 テスト |
| `scripts/run-selftest.sh` | QEMU `ipv6=on` 追加 |
| `TODO_std.md` | IPv6 Phase 1 完了マーク |

### Phase 2 以降の展望（今回はやらない）

- `IpAddr` enum（V4/V6）導入 + netstack 全体のアドレス抽象化
- IPC プロトコルのアドレスフィールドを 16 バイトに拡張
- TCP/UDP over IPv6
- DNS AAAA レコード対応
- PAL の IPv6 対応

---

## スラブアロケータの実装（メモリアロケータ Phase 3）

### 目標

`linked_list_allocator` crate を自作のスラブアロケータに置き換え、alloc/dealloc を O(1) に高速化する。TODO.md のメモリアロケータロードマップ Phase 3 にあたる。

### スラブアロケータとは

スラブアロケータ（Slab Allocator）は、あらかじめ決められた固定サイズのスロットを大量に用意しておき、メモリ要求があったら適切なサイズのスロットを渡すアロケータ。Jeff Bonwick が Solaris のカーネル用に考案した手法で、Linux カーネルの SLUB や FreeBSD でも使われている。

従来の `linked_list_allocator` はフリーリストを線形探索するため O(n) だが、スラブアロケータではサイズクラスが決まればフリーリストの先頭を pop するだけで O(1)。

### 設計

7 つの固定サイズクラス（32B, 64B, 128B, 256B, 512B, 1024B, 2048B）と、それ以上の大オブジェクト用アロケータで構成する。

ヒープ領域（16 MiB）を 32 等分し、各サイズクラスに 1 ユニットずつ（計 7 ユニット = 3.5 MiB）、残り 25 ユニット（12.5 MiB）を大オブジェクトに配分した。大オブジェクトへの配分を多くしたのは、ELF ファイル読み込みなど MiB 単位のバッファが頻繁に必要になるため。

各スラブはフリーリスト + バンプポインタのハイブリッド方式:
- **alloc**: フリーリストが非空なら先頭を pop（O(1)）。空ならバンプポインタから切り出し（O(1)）。
- **dealloc**: 解放スロットの先頭 8 バイトに次ノードのポインタを埋め込む intrusive linked list で push（O(1)）。

### 大オブジェクトアロケータの工夫

2048B を超えるアロケーションは first-fit + バンプのハイブリッドで管理する。ここで苦戦したのが **Vec の倍々成長による断片化** の問題。

Vec は容量が足りなくなると 2 倍のバッファを新たに確保し、旧バッファをコピーして解放する。例えば 100KB の Vec が成長すると: 100KB → 200KB(new) → 100KB(free) → 400KB(new) → 200KB(free) ... という具合に、一時的に旧バッファ + 新バッファの両方が Large 領域を消費する。

FAT32 の `read_file_data()` が `Vec::new()` で始めて extend_from_slice で育てていたため、ELF ファイル（約 1 MiB）を読むだけで倍々成長が数 MiB の一時メモリを食い尽くしていた。

3 つの対策を入れた:

1. **隣接ブロック結合（coalescing）**: dealloc 時にフリーリストをアドレス順にソートして挿入し、隣接するフリーブロックを結合する。これにより小さなフリーブロックが大きなブロックになり、再利用しやすくなる。

2. **バンプポインタ巻き戻し**: フリーリスト末尾のブロックがバンプポインタに隣接していたら、ブロックを削除してバンプポインタを巻き戻す。これにより、最後に解放されたブロックの分だけ未使用領域が回復する。

3. **GlobalAlloc::realloc のオーバーライド**: バンプポインタ直前のブロック（= 最後に確保されたブロック）を in-place で拡張できる場合、コピーなしで realloc を完了する。Vec の倍々成長パターンでは最後のバッファが末尾にあることが多いため、効果絶大。

さらに副次的な改善として、FAT32 の `read_file_data()` に `Vec::with_capacity(entry.size)` を追加して、事前にファイルサイズ分の容量を確保するようにした。これにより倍々成長自体がなくなり、一時メモリの無駄遣いを根本的に解消した。

### 実装したファイル

| ファイル | 操作 |
|---------|------|
| `kernel/src/slab_allocator.rs` | 新規 — スラブアロケータ本体 |
| `kernel/src/allocator.rs` | 修正 — LockedSlabAllocator に切り替え |
| `kernel/src/main.rs` | 修正 — `mod slab_allocator;` 追加 |
| `kernel/Cargo.toml` | 修正 — `linked_list_allocator` 依存削除 |
| `kernel/src/shell.rs` | 修正 — selftest にスラブテスト追加 |
| `kernel/src/fat32.rs` | 修正 — read_file_data に with_capacity 追加 |
| `CLAUDE.md` | 修正 — 外部 crate 推奨の方針を追記 |
| `TODO.md` | 修正 — Phase 3 完了マーク |

### テスト

selftest に `slab_allocator` テストを追加した。テスト内容:
- 小さいアロケーション（Box<u64> × 100）の確保・解放・再確保
- アライメント要件（align=16, 64, 128, 256）の検証
- 大オブジェクト（4KiB, 8KiB, 16KiB）の書き込み・読み返し整合性
- 全サイズクラスの混合ストレステスト（断片化耐性）

全テスト PASS。

### 学んだこと

- **intrusive linked list** の仕組み。解放済みメモリ自体にリンク情報を書き込むことで、ノード用の別メモリが不要になる。アロケータの bootstrap 問題（ノード用メモリの確保にアロケータが必要 → 鶏と卵問題）を回避できる。

- **Vec の倍々成長がアロケータに与える影響**。O(1) のスラブでも、Large アロケータの断片化で実質的な容量不足になり得る。realloc の in-place 拡張と隣接ブロック結合の両方が必要だった。

- **Rust 2024 edition の unsafe fn 制約**。`unsafe fn` の中でも unsafe 操作には `unsafe {}` ブロックが必要になった（`unsafe_op_in_unsafe_fn` 警告）。unsafe の範囲をより明確にする良い変更。

- **アロケータの配分チューニング**。最初は TODO.md の計画通り 32B×2 + 64B×2 + 128B×2 + ... で Large 6 MiB にしたが、ELF 読み込みの消費量を考慮して 32 分割・各 1 ユニット・Large 25 ユニットに変更。ワークロードに合った配分が重要。

---

## TODO_std.md 残タスクの消化

### debug ビルドの OOM 問題 → 解決済み

TODO_std.md に残っていた「debug ビルドの OOM 問題の改善」は、実はスラブアロケータ実装 + FAT32 の `with_capacity` 追加で既に解決していた。

元々の問題は、debug ビルドの ELF が大きく（6.4MB → スラブ実装後は 9.3 MB）、カーネルヒープ上の Vec で読み込むときに倍々成長の一時メモリ消費で OOM になるというもの。スラブアロケータの Large 領域を 12.5 MiB に拡大し、realloc の in-place 拡張と隣接ブロック結合を実装したことで、大きなバッファの確保が効率化された。さらに `read_file_data()` に `Vec::with_capacity(entry.size)` を追加して倍々成長自体を回避した。

TODO を完了マークに更新した。

### nightly 更新時の sysroot パッチ追従 → rust-toolchain.toml で日付固定

もう 1 つの残タスク「nightly 更新時の sysroot パッチ追従」は、`rust-toolchain.toml` の `channel` を日付付きに固定することで対応した。

```toml
# 変更前
channel = "nightly"

# 変更後
channel = "nightly-2026-02-02"
```

`channel = "nightly"` のままだと、`rustup update` や新しいマシンで環境構築したときに最新の nightly がインストールされ、std のソースコード構造が変わって sysroot パッチ（`scripts/patch-rust-sysroot.sh`）が壊れるリスクがあった。日付を固定することで、意図的に更新しない限り同じ nightly バージョン（rustc 1.95.0-nightly, commit 57d2fb136, 2026-02-01）が使われる。

nightly を更新するときは:
1. `rust-toolchain.toml` の日付を新しい nightly に変更
2. `make patch-sysroot` でパッチを再適用
3. `make build && make test` で動作確認
4. 問題なければコミット

という手順を踏む。これで「知らないうちにパッチが壊れていた」という事態を防げる。

TODO を完了マークに更新した。これで TODO_std.md の全タスクが完了！
