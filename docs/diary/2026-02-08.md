# 2026-02-08: Day 8 — SYS_MMAP バグ修正 → std::fs 実装で read_to_string が動いた！

## 今日の意気込み

昨日の std 対応 Phase 7 で `user-std` クレートが動くようになったものの、`println!` が使えず `raw_write()` というワークアラウンドで凌いでいた。今日はその根本原因を突き止めて、ちゃんと `println!` / `String` / `Vec` が動くようにしたい。

## SYS_MMAP ハングの原因究明

### 症状のおさらい

- `SYS_MMAP` を exec/spawn で起動したプロセスから呼ぶと制御が返らない
- シェルプロセス自身からの mmap selftest は PASS する
- `SYS_WRITE`（raw_write）は exec プロセスからも正常に動作する

### デバッグ手法

`sys_mmap` → `find_free_mmap_region` → `is_page_mapped` の各関数にデバッグ出力（`kprintln!`）を仕込んで、どの行でハングするか特定した。

最初は大まかなログ（関数の入り口/出口）から始めて、次にページテーブルの各レベル（L4 → L3 → L2 → L1）のエントリ内容を詳細に出力するように段階的にログを増やした。

### 判明した根本原因

```
[is_page_mapped] 0x40000000: L3[1] flags=0x83 addr=0x40000000 unused=false
[is_page_mapped] 0x40000000: L3 HUGE_PAGE -> mapped
```

**L3 エントリに 1GiB ヒュージページ（flags=0x83 = PRESENT | WRITABLE | HUGE_PAGE）が存在していた。**

UEFI は物理メモリを 1GiB 単位のヒュージページで identity mapping（物理アドレス = 仮想アドレス）する。`create_process_page_table()` はカーネルの L4 エントリを全コピーするため、この identity mapping がプロセスのページテーブルにもそのまま引き継がれる。

`MMAP_VADDR_BASE` は `0x4000_0000`（1GiB）に設定されていたが、これは L4[0] → L3[1] の範囲で、まさに UEFI のヒュージページと完全に重なっていた。結果、`find_free_mmap_region()` は全アドレスを「マッピング済み」と判定し、MMAP_VADDR_LIMIT まで延々とスキャンする無限ループに入っていた。

### なぜシェルでは動くのか

シェルプロセスの mmap selftest は `find_free_mmap_region()` を使わず、直接 `map_anonymous_pages_in_process()` にアドレスを渡していた（0x4000_0000 に 2 ページをマッピング）。つまり「空き領域の検索」をスキップしていたので、ヒュージページとの衝突が問題にならなかった。

### 修正

`MMAP_VADDR_BASE` を `0x100_0000_0000`（1TiB = L4[2] の範囲）に変更した。L4[0] は UEFI の identity mapping が入っている可能性があるが、L4[2] は空なので衝突しない。

```rust
// 修正前
const MMAP_VADDR_BASE: u64 = 0x4000_0000;     // 1 GiB (L4[0])
const MMAP_VADDR_LIMIT: u64 = 0x80_0000_0000;  // 512 GiB

// 修正後
const MMAP_VADDR_BASE: u64 = 0x100_0000_0000;  // 1 TiB (L4[2])
const MMAP_VADDR_LIMIT: u64 = 0x200_0000_0000;  // 2 TiB
```

## println! / String / Vec が動いた！

mmap 修正後、`user-std/src/main.rs` を以下のように書き換えた:

```rust
#![feature(restricted_std)]

fn main() {
    println!("Hello from SABOS std!");
    println!("2 + 3 = {}", 2 + 3);

    let s = String::from("Hello from std String!");
    println!("{}", s);

    let v: Vec<i32> = (1..=5).collect();
    let sum: i32 = v.iter().sum();
    println!("sum of 1..=5 = {}", sum);
}
```

**`raw_write()` ワークアラウンドを完全に削除し、`println!` のみで出力できるようになった。**

`println!` が動くまでの経路:
1. `println!` → std の `stdout()` → `OnceLock::get_or_init()` でスタティック Stdout を初期化
2. Stdout の初期化にはヒープ確保（`SYS_MMAP`）が必要
3. mmap 修正により SYS_MMAP が正常に動作 → Stdout 初期化成功
4. `Stdout::write()` → PAL の `sys_stdio_sabos.rs` → `SYS_WRITE` → シリアルコンソールに出力

`String::from` や `Vec::collect` もヒープアロケーション（SYS_MMAP）経由で正常動作。

## テスト結果

```
HELLOSTD.ELF test PASSED
  Arithmetic output OK
  Vec/alloc output OK
=== SELFTEST END: 40/40 PASSED ===
All tests PASSED!
```

## 学んだこと

- **x86_64 のページテーブル階層（L4 → L3 → L2 → L1）とヒュージページの仕組み**。L3 エントリに HUGE_PAGE フラグが立っていると、その下に L2/L1 テーブルは存在せず、1GiB の領域がまるごと 1 つのエントリでマッピングされる。
- **UEFI の identity mapping は想像以上に広い**。物理 RAM が 128MB でも、UEFI は 1GiB 単位でマッピングするため `0x40000000`〜`0x7FFFFFFF` まで identity mapping が存在する。
- **デバッグの手順は段階的に**。最初は大まかなログで問題の箇所を絞り、次に詳細ログで原因を特定する。最初から全部にログを入れるとノイズが多すぎる。

---

## std::fs の実装 — PAL fs モジュール

### PAL (Platform Abstraction Layer) とは

PAL は Rust の標準ライブラリ (`std`) がプラットフォームごとの違いを吸収するための仕組み。「Platform Abstraction Layer」の略で、日本語では「プラットフォーム抽象化層」。

Rust の `std` はマルチプラットフォーム対応だが、ファイルシステムの操作やプロセス管理、標準入出力などは OS ごとに全然違う。例えば Linux では `open()` + `read()` システムコール、Windows では `CreateFileW()` + `ReadFile()` API を使う。この違いを吸収するのが PAL の役割。

`std` のソースコード内では `sys/` ディレクトリ以下に各プラットフォームの実装が並んでいる:

```
std/src/sys/
├── pal/           # プラットフォーム固有の基本機能
│   ├── unix/      # Linux/macOS/FreeBSD 等
│   ├── windows/   # Windows
│   ├── hermit/    # Hermit OS（組み込みOS）
│   ├── sabos/     # ← SABOS の実装をここに追加！
│   └── ...
├── fs/            # ファイルシステム
│   ├── unix.rs    # Unix 系の実装
│   ├── windows.rs # Windows の実装
│   ├── sabos.rs   # ← 今日追加！
│   └── unsupported.rs  # 未対応プラットフォーム用
├── stdio/         # 標準入出力
├── alloc/         # メモリアロケータ
└── random/        # 乱数生成
```

`cfg_select!` マクロが `target_os` をチェックして、対応するモジュールを選択する。SABOS は `target_os = "sabos"` で分岐する。対応するモジュールがなければ `unsupported.rs`（全操作が「未対応」エラーを返す）にフォールバックする。

Phase 7 までは stdio と alloc の PAL だけを実装していたので、`std::fs` は unsupported のままだった。今日は fs の PAL を追加して `std::fs::read_to_string()` 等を使えるようにした。

### 実装内容

#### 新規ファイル: `sys_fs_sabos.rs`

SABOS のハンドルベース syscall を使って std::fs のインターフェースを実装。具体的にはこのファイルが提供するもの:

| std API | SABOS syscall |
|---------|---------------|
| `File::open()` | SYS_OPEN(70) でハンドルを取得 |
| `File::read()` | SYS_HANDLE_READ(71) |
| `File::write()` | SYS_HANDLE_WRITE(72) |
| `Drop for File` | SYS_HANDLE_CLOSE(73) でハンドルをクローズ |
| `File::seek()` | SYS_HANDLE_SEEK(78) |
| `File::file_attr()` | SYS_HANDLE_STAT(77) |
| `fs::metadata()` | open → stat → close |
| `fs::read_dir()` | SYS_DIR_LIST(13) → パース |
| `fs::remove_file()` | SYS_FILE_DELETE(12) |
| `DirBuilder::mkdir()` | SYS_DIR_CREATE(15) |
| `fs::remove_dir()` | SYS_DIR_REMOVE(16) |

`rename` / `symlink` / `set_permissions` 等の SABOS 未対応操作は `unsupported()` を返す。

#### 新規ファイル: `os_sabos_mod.rs` / `os_sabos_ffi.rs`

`std::os::sabos` モジュール。`OsStr` / `OsString` のバイト列変換トレイト (`OsStrExt::as_bytes()`, `OsStringExt::from_vec()`) を提供する。Unix 系と同じく `OsStr` の内部表現はバイト列そのままなので、Unix の実装を `#[path = "../unix/ffi/os_str.rs"]` で再利用している。

### ぶつかった壁: General Protection Fault (#GP)

fs の実装自体はスムーズに進んだが、実行時に **GPF（一般保護例外）** が発生して苦戦した。

#### 症状

`println!` までは正常に動作するが、`std::fs::read_to_string()` を呼んだ瞬間に GPF。

```
CPU EXCEPTION: GENERAL PROTECTION FAULT (#GP)
instruction_pointer: 0x403260
  movaps %xmm0,(%rcx)   ← GPF の原因
```

#### 原因

`movaps` は SSE の **アラインドメモリ移動命令** で、メモリオペランドが 16 バイト境界にアラインされていないと GPF になる。

問題は `_start` 関数のスタックアラインメントにあった:

- SABOS カーネルは `iretq` でユーザープロセスを起動する
- `iretq` で設定される RSP は 16 バイトアライン（16n）
- しかし System V ABI では、`call` 命令がリターンアドレスを push するため、関数エントリでは RSP = 16n - 8 が前提
- `_start` は `call` ではなく `iretq` で直接ジャンプされるので、この 8 バイトのずれが生じる
- コンパイラは RSP = 16n - 8 を前提にスタックフレームを構築するため、`movaps` のメモリオペランドが 16 バイト境界からずれて GPF になる

```
期待: call → push RIP → RSP = 16n - 8 → push rbx → RSP = 16n - 16 = 16の倍数
実際: iretq → RSP = 16n → push rbx → RSP = 16n - 8 ← ずれている！
```

#### 修正

`_start` をアセンブリで書いて、RSP を明示的に 16 バイトアラインしてから Rust 関数を `call` する方式に変更。`call` 命令自体がリターンアドレスを push するので、Rust 関数のエントリでは正しく RSP = 16n - 8 になる。

```rust
// 修正前: extern "C" fn _start() で直接 main を呼ぶ
// → iretq からの RSP = 16n が前提と合わず GPF

// 修正後: アセンブリで RSP を調整してから call する
core::arch::global_asm!(
    ".global _start",
    "_start:",
    "and rsp, -16",        // RSP を 16 バイト境界に揃える
    "call _start_rust",    // call が RIP を push → RSP = 16n - 8
    "ud2",
);
```

以前の `println!` が動いていたのは、println! の内部でたまたま `movaps` が使われなかっただけ。fs の操作（`SabosHandle` のゼロ初期化で `movaps` が使われた）で初めて問題が顕在化した。

### テスト結果

```
HELLOSTD.ELF test PASSED
  Arithmetic output OK
  Vec/alloc output OK
  fs::read_to_string OK     ← NEW!
  fs::write OK               ← NEW!
  fs::read_back OK           ← NEW!
  fs::metadata OK            ← NEW!
=== SELFTEST END: 40/40 PASSED ===
All tests PASSED!
```

### 学んだこと

- **PAL の構造を理解できた**。`std` がどのようにプラットフォームの差異を吸収しているか。`cfg_select!` でモジュールを切り替える仕組み。
- **System V ABI のスタックアラインメントの重要性**。`call` ではなく `iretq` で関数にジャンプする場合、RSP の初期値が通常の関数呼び出しと異なる。これが SSE 命令の `movaps` で GPF を引き起こす。
- **`movaps` と `movups` の違い**。コンパイラが最適化で `movaps`（アラインド）を使うことがあり、スタックアラインメントが崩れているとクラッシュする。`movups`（アンアラインド）なら問題にならないが、コンパイラの判断次第。
- **iretq で起動されるユーザープログラムの _start は特殊**。普通の関数呼び出しのように `call` で呼ばれるわけではないので、スタックの初期状態が異なる。Linux でも ELF の `_start` は同じ問題があり、glibc の `_start` はアセンブリで書かれている。
