# 2026-02-19: ネットワーク実機対応 Phase 2 一気に完了！

## 今日の目標

実機対応ロードマップの Phase 2（ネットワーク実機対応）を進めたい。Phase 2-1「ARP キャッシュ」、Phase 2-2「DHCP クライアント」、Phase 2-3「e1000e NIC ドライバ」の 3 つを一気に片付けて、ネットワーク周りの実機対応を完了させたい。

---

## Phase 2-1: ARP キャッシュ — ブロードキャスト MAC からの脱却

実機対応ロードマップの Phase 2-1「ARP キャッシュと正しい MAC 解決」を実装した。これまで SABOS のネットワークスタックは、すべての送信パケットで宛先 MAC アドレスをブロードキャスト（FF:FF:FF:FF:FF:FF）にハードコードしていた。QEMU の SLIRP ネットワークではこれでも動くが、実ネットワークではスイッチが正しく転送してくれないため、ARP（Address Resolution Protocol）による正しい MAC 解決が必要になる。これまで SABOS のネットワークスタックは、すべての送信パケットで宛先 MAC アドレスをブロードキャスト（FF:FF:FF:FF:FF:FF）にハードコードしていた。QEMU の SLIRP ネットワークではこれでも動くが、実ネットワークではスイッチが正しく転送してくれないため、ARP（Address Resolution Protocol）による正しい MAC 解決が必要になる。

## やったこと

### ARP とは

ARP は IP アドレスから MAC アドレスを解決するためのプロトコル。Ethernet 上で通信するには宛先の MAC アドレスが必要だが、アプリケーションが知っているのは IP アドレスだけ。ARP Request をブロードキャストで送信し、該当 IP のホストが ARP Reply でユニキャスト応答することで、IP → MAC のマッピングを得る。このマッピングをキャッシュしておくのが ARP キャッシュ（ARP テーブル）。

### ARP キャッシュの設計

`NetState` に `arp_cache: Vec<ArpEntry>` を追加した。`ArpEntry` は `{ ip: [u8; 4], mac: [u8; 6] }` のシンプルな構造体。最大 64 エントリで、溢れたら先頭（最も古いもの）を削除する LRU 風の管理にした。TTL による有効期限管理は今回は見送り。

```rust
struct ArpEntry {
    ip: [u8; 4],
    mac: [u8; 6],
}
```

操作関数は 2 つ:
- `arp_lookup(ip)` — キャッシュから MAC を検索
- `arp_update(ip, mac)` — キャッシュに追加/更新

### ARP 学習の拡張

既存の `handle_arp()` は ARP Request を受信して Reply を返すだけだった。これを拡張して:

1. **すべての ARP パケット**（Request/Reply 両方）から送信元 IP/MAC をキャッシュに学習するようにした。Gratuitous ARP（自分の IP を問い合わせる特殊な ARP）にも対応。
2. **IPv4 パケット受信時**にも、Ethernet ヘッダの送信元 MAC と IPv4 ヘッダの送信元 IP をキャッシュに学習するようにした。これにより、ICMP Echo Reply 等の応答を返す際に ARP Request なしで即座に MAC を解決できる。

### ARP Request 送信

`send_arp_request(target_ip)` を新規実装した。宛先 MAC をブロードキャスト、ターゲット MAC をゼロ（不明）にして ARP Request を送信する。

### resolve_mac — MAC アドレス解決の全フロー

`resolve_mac(dst_ip)` が今回の実装の中核。以下のフローで MAC を解決する:

1. ブロードキャスト IP (255.255.255.255) → そのままブロードキャスト MAC を返す
2. サブネット判定（10.0.2.0/24）。サブネット外ならゲートウェイ IP (10.0.2.2) の MAC を解決対象にする
3. ARP キャッシュを検索 → ヒットすれば返す
4. ミスなら ARP Request を送信し、`wait_net_condition` で応答を待つ
5. 最大 3 回リトライ、各回 1000ms タイムアウト

### net_poller デッドロック問題

ここで一つ面白い問題に遭遇した。`resolve_mac()` は `wait_net_condition()` を使って ARP Reply を待つが、この関数は net_poller タスクがパケットを処理して waiter を起床させることを前提としている。ところが `send_icmp_echo_reply()` や `send_tcp_packet_internal()` は net_poller タスク自身から呼ばれる（handle_packet → handle_ipv4 → handle_icmp → send_icmp_echo_reply の流れ）。もし net_poller 内で `resolve_mac()` を呼んで `wait_net_condition()` でスリープしたら、パケットを処理するタスクがいなくなってデッドロックする。

解決策として、関数の呼び出しコンテキストに応じて使い分けた:

| 関数 | コンテキスト | MAC 解決方法 |
|------|-------------|-------------|
| `send_udp_packet` | ユーザータスク | `resolve_mac()` でブロッキング解決 |
| `tcp_connect` | ユーザータスク | SYN 送信前に `resolve_mac()` でキャッシュを温める |
| `send_tcp_packet_internal` | net_poller / ユーザータスク | `arp_lookup()` で非ブロッキング（ミス時は BROADCAST_MAC フォールバック） |
| `send_icmp_echo_reply` | net_poller | `arp_lookup()` で非ブロッキング（handle_ipv4 で学習済み） |

`send_tcp_packet_internal` が net_poller から呼ばれるケースは、受信したパケットへの応答（SYN-ACK, ACK 等）なので、直前の `handle_ipv4` で送信元 MAC を学習済み。つまり `arp_lookup` はほぼ確実にヒットする。

### selftest

`arp_resolve` テストを追加した。`resolve_mac(&GATEWAY_IP)` でゲートウェイ（10.0.2.2）の MAC が解決できることを確認する。QEMU SLIRP は ARP に正しく応答するので、ARP Request → Reply → キャッシュ → MAC 返却の全フローがテストされる。

## テスト結果

```
=== SELFTEST END: 57/57 PASSED ===
```

新規追加の `arp_resolve` を含む全 57 テストが PASS。

## 気づき

- ARP キャッシュの実装自体はシンプルだが、**どのコンテキストから呼ばれるか**を意識する必要があった。カーネル内のネットワークスタックでは、パケット処理タスク（net_poller）と通常タスクが同じ関数を呼ぶケースがあり、ブロッキング/非ブロッキングの使い分けが重要。
- IPv4 パケット受信時にも送信元 MAC を学習するようにしたのは良い判断だった。ARP パケットだけでなく、通常のデータパケットからも MAC を学べるので、ARP キャッシュの温まりが速くなる。
- TTL 管理は今回見送ったが、実環境では ARP エントリの有効期限管理が必要になる。TODO に残してある。

---

## Phase 2-2: DHCP クライアント — IP アドレスの動的取得

### DHCP とは

DHCP (Dynamic Host Configuration Protocol) は、ネットワーク上のデバイスに IP アドレスやサブネットマスク、ゲートウェイ、DNS サーバーなどの設定を自動的に割り当てるプロトコル。これまで SABOS は IP アドレスを 10.0.2.15 にハードコードしていた。QEMU の SLIRP ではたまたまこの IP がデフォルトだったが、実ネットワークでは DHCP で動的に取得する必要がある。

### 実装の準備：ネットワーク設定の動的化

DHCP を実装する前提として、IP アドレスやゲートウェイ等のネットワーク設定を `const` から `static Mutex` に変更する必要があった。`kernel/src/net_config.rs` を新設して、実行時に設定を書き換えられるようにした。初期値は QEMU SLIRP のデフォルト値（10.0.2.15 等）を維持し、DHCP が成功したら上書きする。失敗してもデフォルトが残るので安全。

### DHCP 4 ステップ

DHCP のプロトコルは 4 つのメッセージで構成される:

1. **Discover** — クライアントがブロードキャストで「IP ください」と要求
2. **Offer** — サーバーが「この IP どうですか」と提案
3. **Request** — クライアントが「その IP をお願いします」と確認
4. **Ack** — サーバーが「OK、割り当てました」と承認

実装では `dhcp_discover()` 関数として、Discover → Offer 待ち → Request → Ack 待ちの全フローを 1 つの関数にまとめた。UDP ポート 68（クライアント）で受信し、ポート 67（サーバー）に送信する。

### DHCP オプションのパース

DHCP メッセージには「オプション」フィールドがあり、TLV (Type-Length-Value) 形式で様々な情報が格納される。重要なのは:

- Option 1: サブネットマスク
- Option 3: デフォルトゲートウェイ
- Option 6: DNS サーバー
- Option 51: リース時間
- Option 53: メッセージタイプ（Offer / Ack の判定に使う）
- Option 54: DHCP サーバーの IP

マジックナンバー `0x63825363` で始まるオプション領域をパースして、上記の値を取得する。

### テスト結果

```
=== SELFTEST END: 58/58 PASSED ===
```

新規追加の `dhcp_config` テストを含む全 58 テストが PASS。`dhcp_config` は DHCP で取得した IP がデフォルト値（0.0.0.0）でないことを確認する。

---

## Phase 2-3: e1000e NIC ドライバ — 初の実 NIC ドライバ

### なぜ e1000e か

SABOS のネットワークは virtio-net だけに対応していた。virtio-net は QEMU 向けの仮想デバイスで、実機には存在しない。実機でネットワークを使うには実際の NIC (Network Interface Card) のドライバが必要。Intel e1000e (82574L) を選んだ理由は:

1. **広く普及** — 多くのサーバーやビジネス PC に搭載
2. **仕様公開** — Intel がデータシートを公開している
3. **QEMU でテスト可能** — `-device e1000e` でエミュレート可能
4. **シンプル** — 最新の NIC に比べてプログラミングモデルが素直

### e1000e の仕組み

e1000e は MMIO（Memory-Mapped I/O）でレジスタにアクセスする。PCI の BAR0 にマップされた物理アドレスに対して、read_volatile / write_volatile で読み書きする。NVMe ドライバと同じパターン。

パケットの送受信は **ディスクリプタリング**（リングバッファ）で管理する。RX リングと TX リングの 2 つがあり、それぞれ 32 個のディスクリプタ（16 バイト）を持つ。

- **RX（受信）**: ハードウェアがパケットを受信すると、ディスクリプタの `addr` が指すバッファにデータを書き込み、`status` の DD ビットをセットする。ソフトウェアは DD ビットをポーリングして受信を検出する。
- **TX（送信）**: ソフトウェアがディスクリプタに送信データのアドレスと長さをセットし、TDT（Tail）レジスタを更新する。ハードウェアが送信完了すると DD ビットをセットする。

### 初期化シーケンス

1. PCI Bus Master + Memory Space を有効化
2. BAR0（MMIO ベースアドレス）を取得
3. CTRL レジスタの RST ビットでデバイスリセット
4. IMC レジスタで全割り込みを無効化（ポーリングモード）
5. CTRL の SLU ビットでリンクアップを強制
6. RAL/RAH レジスタから MAC アドレスを読み取り
7. RX/TX ディスクリプタリングとバッファを `alloc_zeroed` で確保
8. RDBAL/RDBAH/RDLEN/RDH/RDT レジスタを設定
9. TDBAL/TDBAH/TDLEN/TDH/TDT レジスタを設定
10. RCTL で受信有効化、TCTL で送信有効化

### BAR の 32-bit / 64-bit 判定

最初にハマったのが BAR0 の読み取り。計画では `read_bar64()` で 64-bit として読み取る想定だったが、QEMU の e1000e は **32-bit MMIO BAR** を使っていた。BAR0 を 64-bit として読むと、BAR1 の値が上位 32 ビットに混入してアドレスがおかしくなり、MMIO アクセスで General Protection Fault が発生した。

解決策として、BAR の type bits ([2:1]) を確認してから 32-bit / 64-bit を判定するようにした:

```rust
let bar0_low = pci::read_bar(dev.bus, dev.device, dev.function, 0);
let bar_type = (bar0_low >> 1) & 0x03;
let bar0 = if bar_type == 0x02 {
    // 64-bit MMIO
    pci::read_bar64(dev.bus, dev.device, dev.function, 0) & !0xF
} else {
    // 32-bit MMIO
    (bar0_low & !0xF) as u64
};
```

NVMe は 64-bit BAR を使うので `read_bar64()` で問題なかったが、e1000e は 32-bit だった。デバイスごとに BAR のサイズが異なることを学んだ。

### netstack の NIC 抽象化

ドライバだけ作っても、netstack が virtio-net を直接参照しているままでは e1000e が使われない。`send_frame()` / `recv_frame_nonblocking()` / `kick_virtio_net()` を抽象化して、virtio-net → e1000e のフォールバック順で動作するように変更した:

```rust
fn send_frame(data: &[u8]) -> Result<(), &'static str> {
    // virtio-net を優先（QEMU デフォルト）
    if let Some(ref mut d) = *crate::virtio_net::VIRTIO_NET.lock() {
        return d.send_packet(data);
    }
    // e1000e にフォールバック
    if let Some(ref mut d) = *crate::e1000e::E1000E.lock() {
        return d.send_packet(data);
    }
    Err("no network device available")
}
```

`kick_virtio_net()` も `kick_net_device()` にリネームし、e1000e の場合は ICR レジスタを read-to-clear する処理に変更した。

`netstack::init()` も同様に、virtio-net → e1000e の順で MAC アドレスを取得するように変更。QEMU では両方のデバイスが存在するが、virtio-net が優先されるので既存の動作に影響しない。

### QEMU 設定

`run-qemu.sh` と `run-selftest.sh` の両方に `-netdev user,id=net1 -device e1000e,netdev=net1` を追加した。最初は `run-qemu.sh` だけ変更してテストが通らず、テスト用の QEMU は `run-selftest.sh` で独自に起動されることに気づいた。

### テスト結果

```
=== SELFTEST END: 59/59 PASSED ===
```

新規追加の `e1000e_detect` を含む全 59 テストが PASS。

---

## 今日のまとめ

Phase 2（ネットワーク実機対応）を 3 つ全て完了した:

| Phase | 内容 | テスト数 |
|-------|------|---------|
| 2-1 | ARP キャッシュ | 57/57 |
| 2-2 | DHCP クライアント | 58/58 |
| 2-3 | e1000e NIC ドライバ | 59/59 |

これで SABOS は実機のネットワーク環境（Intel e1000e NIC + DHCP サーバー）で動作する準備が整った。virtio-net しかない QEMU 環境でも、フォールバック機構のおかげで従来通り動作する。

---

## Phase 3-2: 電源管理（ACPI shutdown / reboot）

### やったこと

これまで SABOS の電源操作は「HLT ループで CPU 停止」か「QEMU 専用 ISA debug exit」しかなかった。実機で使うには ACPI を使って正しくシャットダウン・リブートする必要がある。

#### FADT 解析

ACPI の FADT (Fixed ACPI Description Table) テーブルから電源管理に必要な情報を取得する処理を `acpi.rs` に追加した:

- **PM1a Control Block**: シャットダウン時に SLP_TYP と SLP_EN を書き込む I/O ポート
- **Reset Register**: リブート時に reset_value を書き込むアドレス（I/O ポートまたは MMIO）
- **S5 スリープタイプ**: DSDT の `_S5_` パッケージからバイトスキャンで取得

S5 スリープタイプの取得は面白い実装になった。通常は AML (ACPI Machine Language) インタープリタを使って DSDT を解釈するが、それはかなり大掛かり。代わりに DSDT のバイト列を直接スキャンして `_S5_` という名前のオブジェクトを探し、パッケージの最初の要素（SLP_TYPa）を読み取るという軽量な方法を採用した。

#### shutdown コマンド

PM1a_CNT レジスタに `(SLP_TYPa << 10) | (1 << 13)` を書き込んで S5 ステート（Soft Off）に遷移させる。QEMU ではこの操作で仮想マシンが正常終了する。

#### reboot コマンド

3 段階のフォールバック戦略:
1. **FADT reset register**: ACPI 2.0 標準のリセット方法
2. **8042 キーボードコントローラ**: レガシーなリセット方法（I/O ポート 0x64 に 0xFE を送信）
3. **トリプルフォルト**: IDT を無効化して例外を発生させる最終手段

#### selftest

`acpi_fadt` テストを追加（59 → 60 項目）。FADT から PM1a Control Block と S5 スリープタイプが正しく取得されていることを確認する。

### つまずいたポイント

- **packed struct のアライメント問題**: `acpi` crate の `Fadt` 構造体は `#[repr(C, packed)]` なので、フィールドを直接参照すると Rust のアライメントチェックに引っかかる。`let flags = { fadt.flags };` のようにローカル変数にコピーしてからメソッドを呼ぶ必要があった。

## 気づき・学び

- **BAR は 32-bit と 64-bit がある**。NVMe だけ触っていると 64-bit が当たり前と思い込んでしまうが、e1000e のようなレガシーなデバイスは 32-bit BAR を使う。type bits を見て判定するのが正しいお作法。
- **テストスクリプトが複数箇所に QEMU コマンドを持っている**ことに気づいた。`run-qemu.sh` と `run-selftest.sh` で同じデバイス構成を別々に管理しているのは技術的負債。いずれ共通化したい。
- **NIC 抽象化はフォールバック方式がシンプルで良い**。virtio-net → e1000e の順に試すだけで、設定なしに適切なデバイスが選ばれる。将来 Realtek RTL8168 等を追加しても同じパターンで拡張できる。
- DHCP → ARP → MAC 解決 → IP 通信という一連のフローが、実機と同じ形で動くようになった。QEMU SLIRP の「おまかせ設定」に甘えていた部分を一つずつ自前で実装していく過程は、ネットワークプロトコルの理解が深まって楽しい。

---

## Phase 3-3: エラーリカバリ（ストレージ I/O リトライ + ネットワークリンク検出）

### やったこと

実機で安定動作するために必要なエラーリカバリ機能を実装した。現在の SABOS は全ストレージドライバが単発実行で、一時的なエラーが起きるとすぐに Err を返してしまう。また NIC ドライバはリンク状態を監視していないため、ケーブルが抜けた状態でもパケットを送ろうとしてしまう。

#### ストレージ I/O リトライ

3 つのストレージドライバ（AHCI、NVMe、virtio-blk）すべてに、`read_sector` / `write_sector` のリトライロジックを追加した。実装パターンは共通:

1. バリデーションエラー（sector out of range、buffer too small 等）はリトライせず即 Err を返す
2. 実際の I/O 操作を `read_sector_once` / `write_sector_once` に分離
3. 公開関数でリトライラッパーを実装（最大 3 回、間にスピンウェイト）

```rust
const IO_RETRY_COUNT: u32 = 3;
const IO_RETRY_SPIN_WAIT: u32 = 100_000;

for attempt in 0..IO_RETRY_COUNT {
    match self.read_sector_once(sector, buf) {
        Ok(()) => return Ok(()),
        Err(e) => { /* ログ出力してリトライ、最後は Err を返す */ }
    }
}
```

QEMU 環境では一時的エラーは発生しないので、正常系で 1 回目で成功するだけ。しかし実機では SATA リンクの一時的な不安定や NVMe コントローラのリセットなどでエラーが起きうるため、リトライがあると安心。

#### ネットワークリンク状態検出

e1000e ドライバに `is_link_up()` メソッドを追加した。STATUS レジスタの LU（Link Up）ビットを読み取るだけのシンプルな実装。`send_packet()` の冒頭でリンクダウン時に早期エラーを返すようにした。

virtio-net にも同じインターフェースの `is_link_up()` を追加したが、QEMU 仮想デバイスなので常に `true` を返す。将来 VIRTIO_NET_F_STATUS feature bit をネゴシエートすればデバイスステータスから読み取れるが、今は必要ない。

netstack に `is_network_link_up()` を追加し、virtio-net → e1000e のフォールバック順でリンク状態を確認する。`linkstatus` シェルコマンドでリンク状態を表示できるようにした。

### テスト結果

```
=== SELFTEST END: 62/62 PASSED ===
```

新規追加の `storage_retry`（AHCI でセクタ 0 を読み取り、リトライ付き正常パスの確認）と `network_link`（QEMU 環境でリンクが UP であることの確認）を含む全 62 テストが PASS。

### 気づき

- リトライロジックのパターンが 3 ドライバで全く同じになった。マクロやトレイトで共通化する手もあるが、各ドライバのコンテキストが微妙に異なる（エラーメッセージのプレフィックスなど）ので、現時点ではコピペで十分。
- USB ホットプラグはロードマップに含まれるが、xHCI ドライバ自体が未実装なのでスコープ外とした。xHCI を実装するときに一緒に対応する。

---

## Phase 3-1: TCP 堅牢化（ISN ランダム化 + DNS ランダム化 + TIME_WAIT タイマー）

ロードマップの Phase 3-1「TCP の堅牢化」から、小さめの 3 項目を実装した。再送タイマー・アウトオブオーダー・輻輳制御は今回のスコープ外。

### TCP ISN ランダム化

これまで TCP の ISN（Initial Sequence Number）が固定値 `1000` だった。これだと TCP シーケンス番号予測攻撃に脆弱。ISN というのは TCP 接続の開始時に使うシーケンス番号の初期値で、これが予測可能だと攻撃者がなりすましパケットを注入できてしまう。

RDRAND 命令を使ってランダムな 32 ビット値を生成するようにした。RDRAND は Intel/AMD の CPU に搭載されているハードウェア乱数生成命令で、暗号学的に安全な乱数を高速に取得できる。`kernel_rdrand64()` というヘルパー関数を netstack 内に定義し、10 回リトライしてもダメなら 0 を返すフォールバック付き。

### DNS クエリ ID / ソースポートランダム化

DNS クエリ ID が `0x1234` 固定、ソースポートが `12345` 固定だった。これだと DNS キャッシュポイズニングに脆弱。DNS キャッシュポイズニングとは、攻撃者が偽の DNS レスポンスを送りつけて、正規のドメイン名を不正な IP アドレスに解決させる攻撃。クエリ ID とソースポートが予測可能だと、攻撃者は正規のレスポンスより先に偽レスポンスを送り込める。

クエリ ID は RDRAND で毎回ランダム生成、ソースポートはエフェメラルポート範囲（49152-65535）からランダム選択するようにした。

### TIME_WAIT タイマー

TCP 接続を閉じるとき、FinWait1/FinWait2 から TimeWait に遷移した後、即座に接続を削除していた。RFC 793 では TIME_WAIT は 2MSL（Maximum Segment Lifetime × 2、通常 120 秒）待ってから接続を削除すべき。TIME_WAIT の目的は、ネットワーク上に残っている遅延パケットが新しい接続のデータとして誤認されるのを防ぐこと。

`TcpConnection` に `time_wait_deadline` フィールド（`Option<u64>`）を追加し、TimeWait 遷移時に PIT tick ベースのデッドラインを設定する。学習用 OS なので 10 秒（182 ticks）に設定。`net_poller` のメインループで期限切れをチェックし、期限が来たら `retain()` で接続を削除する。`tcp_close()` では TimeWait の場合は即削除せず残すよう変更した。

### selftest 結果（63/63 PASS）

```
=== SELFTEST END: 63/63 PASSED ===
```

新規追加の `tcp_isn_random`（2 つの TCP 接続の ISN が異なることを確認）を含む全 63 テストが PASS。

---

## Phase 3-1 追加: TCP 再送タイマー

### 背景

これまでの SABOS TCP は、SYN/データ/FIN パケットを送信した後、応答がなくても再送しなかった。ネットワーク上でパケットがロストした場合、`wait_net_condition` のタイムアウトまで待って失敗するだけ。TCP の信頼性を高めるため、未 ACK パケットの再送タイマーを実装した。

### 再送タイマーとは

TCP では、送信したパケットに対して相手から ACK（確認応答）が返ってこなかった場合、パケットがロストしたと見なして再送する。再送までの待ち時間を RTO（Retransmission Timeout）と呼び、再送するたびに RTO を倍にしていく（指数バックオフ）。これにより、ネットワークの混雑時に再送が殺到するのを防ぐ。

### 実装内容

`UnackedPacket` 構造体を新設し、`TcpConnection` に `unacked_packet: Option<UnackedPacket>` フィールドを追加。SYN/SYN-ACK/データ/FIN を送信するたびに再送情報を記録し、ACK を受信したらクリアする。

`net_poller_task()` のメインループで、TIME_WAIT クリーンアップの後に再送チェックを追加。デッドラインを超えた未 ACK パケットを指数バックオフ（1s→2s→4s→8s→16s、最大 5 回）で再送する。最大再送回数を超えた接続は自動的に Closed に遷移する。

Mutex デッドロック防止のため、再送パケット情報を `with_net_state` 内で収集し、Mutex 外で `send_tcp_packet_internal` を呼ぶパターンを採用した（既存の `handle_tcp` と同じアプローチ）。

### selftest 結果（64/64 PASS）

```
=== SELFTEST END: 64/64 PASSED ===
```

新規追加の `tcp_retransmit`（`UnackedPacket` の記録・クリアが正しく動くことを確認）を含む全 64 テストが PASS。
