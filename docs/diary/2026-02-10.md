# 2026-02-10 開発日記

## 今日の目標

TODO.md の「IPC 基盤の改善」3 項目を一気に片付けたい。具体的には:

1. IPC recv のポーリングを Sleep/Wake 方式に改修して CPU 浪費を止める
2. recv のキャンセル機構を追加する
3. IPC 経由で Capability（ハンドル）を委譲できるようにする

マイクロカーネルへの道のりとして、IPC はまさに心臓部。ここを良くしておけば、将来ファイルシステムやネットワークをユーザー空間に移したときに、ちゃんと動くはず。

## IPC recv の Sleep/Wake 化

### 問題

これまでの `recv()` は `loop { try_recv(); yield_now(); }` というポーリング方式だった。タスクは常に Ready 状態のままで、PIT の ~55ms 間隔でしか受信チェックされない。つまり:

- CPU サイクルを無駄に消費する（Ready なので毎回スケジューリングされる）
- レイテンシが悪い（最悪 55ms 待たされる）

### 解決策

futex.rs で使っている `set_current_sleeping` + `wake_task` パターンをそのまま IPC にも適用した。

```
send() 側:
  1. メッセージをキューに追加
  2. dest が IPC_WAITERS にいれば wake_task(dest) で起床

recv() 側:
  1. try_recv() で即チェック → あればすぐ返す
  2. IPC_WAITERS に自分を登録
  3. set_current_sleeping(wake_at) で Sleeping に遷移
  4. ダブルチェック: もう一度 try_recv → 来ていたら自分を起こして返す
  5. yield_now() でスケジューラに制御を渡す
  6. 起床後: WAITERS 除去、キャンセルチェック、try_recv
```

ステップ 4 の「ダブルチェック」が重要。`set_current_sleeping` と `yield_now` の間に `send()` が来た場合、wake_task は Sleeping→Ready にするだけなので、まだ yield していなければ効果がない。だからもう一度 try_recv して、メッセージが来ていたら自分で自分を起こす。

これは futex_wait() でも「値チェック → TABLE 登録 → sleeping → yield」の順序で同じ考慮をしている。並行プログラミングの定番パターン（ダブルチェックロッキングの変形）。

### IPC_WAITERS と IPC_CANCELLED

新たに 2 つのグローバルデータ構造を追加:

- `IPC_WAITERS: Mutex<BTreeSet<u64>>` — recv 待ちタスクの集合
- `IPC_CANCELLED: Mutex<BTreeSet<u64>>` — キャンセルされたタスクの集合

BTreeSet を使ったのは、contains / insert / remove が O(log n) で、タスク数が少ない SABOS では十分速いから。

## キャンセル機構: SYS_IPC_CANCEL (92)

`cancel_recv(target_task_id)` を呼ぶと:

1. `IPC_CANCELLED` にターゲットを追加
2. `wake_task(target_task_id)` で起床させる

起床した recv 側は、try_recv でメッセージがなく、CANCELLED チェックでフラグが立っていたら `Cancelled` エラーを返す。

新しいエラーコード `-50` (Cancelled) を `SyscallError` に追加した。エラーコード体系:
- 1-9: ポインタ・メモリ関連
- 10-19: 引数・データ形式関連
- 20-29: ファイル・ハンドル関連
- 30-39: 権限・セキュリティ関連
- 40-49: システム関連
- **50-59: IPC 関連** ← NEW

## Capability 委譲: SYS_IPC_SEND_HANDLE (93) / SYS_IPC_RECV_HANDLE (94)

マイクロカーネルの核心部分。IPC メッセージにハンドル（Capability）を付けて送受信できるようにした。

### duplicate_handle

handle.rs に `duplicate_handle()` を追加した。元のハンドルと同じ rights/kind/path/data を持つ新しいハンドルを作成する。token は新規生成、pos は 0 にリセット。

なぜ duplicate するかというと、送信元のハンドルと受信先のハンドルは独立した存在であるべきだから。送信後に元ハンドルを close しても、受信側のハンドルは生きている。逆に、受信側がハンドルを close しても送信元には影響しない。

### IPC_HANDLE_QUEUES

通常の `IPC_QUEUES` とは別に、ハンドル付きメッセージ専用のキュー `IPC_HANDLE_QUEUES` を持つ。分けた理由は、通常メッセージとハンドル付きメッセージを混在させると型が複雑になるし、タスク終了時のクリーンアップでハンドルの close が必要なため。

`cleanup_task()` では、未読のハンドル付きメッセージのハンドルも close するようにした。これがないと、受信者が先に死んだ場合にハンドルがリークする。

### recv_with_handle

タイムアウトなしで、`cancel_recv()` でキャンセルされるまで待つ設計にした。これはマイクロカーネルの IPC パターンに合わせている — サーバーはクライアントからの要求を無期限に待ち、必要に応じてキャンセルする。

## IPC ベンチマーク: ipc_bench コマンド

rdtsc 命令（TSC: Time Stamp Counter）を使って、send+recv のラウンドトリップを N 回計測する。min/avg/max のサイクル数を表示する。

TSC（Time Stamp Counter）は x86 CPU のクロックサイクルカウンタで、rdtsc 命令で読み取れる。1 サイクル単位の精度があるので、マイクロベンチマークに最適。ただし、省電力機能で CPU クロックが変動する環境では注意が必要（QEMU ではほぼ一定）。

10 回のウォームアップを入れてから本計測する。

## selftest 追加

### ipc_cancel テスト

selftest は単一タスクで動くので、「別タスクから cancel する」テストはできない。代わりに、自分自身を cancel 対象にしてから recv する方式にした:

1. `cancel_recv(自分)` → CANCELLED フラグが立つ + wake_task（自分は Running なので無効）
2. `recv(自分, 1000)` → try_recv(なし) → sleep → yield → 起床（cancel の wake で） → CANCELLED チェック → Cancelled エラー

### ipc_handle テスト

1. テスト用ファイルハンドル（`"IPC handle test data"`）を作成
2. 自分自身にハンドル付きメッセージを送信
3. `try_recv_with_handle()` で即座に受信（自分宛なのでキューにある）
4. 受信したハンドルで read して、元のデータと一致するか確認
5. 両方のハンドルを close

## テスト結果

```
make test → 全テスト PASS（ipc_cancel, ipc_handle 含む）
```

既存の ipc, ipc_typed テストも引き続き PASS。Sleep/Wake 化しても後方互換性は維持できた。

## 振り返り

### 良かった点

- futex.rs のパターンをそのまま流用できた。一度学んだパターンが再利用できるのは気持ちいい
- ダブルチェックパターンを忘れずに入れられた。並行処理の罠を踏まずに済んだ
- TODO.md の 3 項目を一気に片付けられた

### 学んだこと

- **ダブルチェックパターン**: Sleep に遷移した直後に、もう一度条件をチェックすることで TOCTOU（Time Of Check To Time Of Use）レースを防ぐ。これは futex でも IPC でも共通の重要パターン
- **Capability 委譲の設計**: 送信元と受信先のハンドルは独立すべき。duplicate + 別 token で実現。close の責任が明確になる
- **rdtsc**: x86 の TSC カウンタはマイクロベンチマークに便利。インラインアセンブリで `out("eax")` と `out("edx")` を使って 64 ビット値を組み立てる

### 次にやりたいこと

- ipc_bench の結果を実際に見てみたい（手動で `make run-gui` して `ipc_bench` コマンドを叩く）
- 型安全 IPC (recv_typed) も Sleep/Wake 化したので、netd との通信が速くなっているはず
- HELLOSTD.ELF の IPC タイムアウト問題が改善されたか確認したい（Sleep/Wake 化でレイテンシが改善しているかも）

## IPC recv ループ修正

### 問題

IPC の `recv()` が1回の sleep-wake サイクルで終了していた。具体的には:

1. `try_recv()` → メッセージなし
2. `set_current_sleeping(deadline)` で寝る
3. タイマーで起床（メッセージが来たからではなく、PIT 割り込みで）
4. `try_recv()` → まだメッセージなし → **即 Timeout を返す**

これだと、タイムアウト時間が残っていてもメッセージを受信できない。netd の DNS ルックアップは数秒かかるので、HELLOSTD.ELF のネットワークテストが「タイムアウト設定は長いのにすぐ諦める」という症状になっていた。

### 解決策

`recv()`, `recv_typed()`, `recv_with_handle()` の 3 関数をすべてループ化した。

```
loop {
    try_recv → あれば返す
    is_deadline_reached → タイムアウトなら Timeout
    WAITERS 登録 → sleep → ダブルチェック → yield
    起床後: キャンセルチェック → ループ先頭に戻る
}
```

`is_deadline_reached()` ヘルパーを追加。deadline が `u64::MAX`（無期限待ち）なら常に false、それ以外は現在のタイマーティックと比較する。

### フレーキーテスト対策

テスト中に `[FAIL] ipc` と `[FAIL] ipc_cancel` が不安定に発生する問題を発見した。調査の結果:

- ループ修正前（前のコミット）でも同じ失敗が再現 → ループ修正が原因ではない
- 別の実行では 47/47 全パス → タイミング依存のフレーキーテスト

原因の仮説: ネットワークテスト等から遅延 IPC メッセージがカーネルシェルのキューに残ることがある。`test_ipc` の `try_recv` でそれを先に拾ってしまい、期待する "ping" と一致しない。

対策: `test_ipc` と `test_ipc_cancel` の先頭で `while try_recv().is_some() {}` でキューをドレインするようにした。

### テスト結果

```
make test → 47/47 PASSED（ipc, ipc_cancel, ipc_handle 含む）
```

## VFS 統一: マウントテーブル導入

### 問題

SABOS のファイル操作は、syscall.rs・handle.rs・shell.rs の各所で `Fat32::new()` を直接呼び出しており、`/proc` パスの判定が 10 箇所以上に分散していた。VFS trait（FileSystem, VfsNode）は定義済みで FAT32・ProcFs とも実装していたが、マウント管理がなく実際には使われていなかった。

```
// 旧: あちこちで Fat32 を直接作って使う
let mut fs = fat32::Fat32::new()?;
if path.starts_with("/proc") {
    // procfs 分岐...
} else {
    fs.read_file(path)?;
}
```

### 解決策: VFS マネージャ

vfs.rs に VFS マネージャ（マウントテーブル）を導入した。BTreeMap でマウントポイント → ファクトリ関数を管理し、パスの最長一致でファイルシステムを選択する。

```
VfsManager {
    mounts: BTreeMap<String, MountEntry>
    // "/" -> Fat32
    // "/proc" -> ProcFs
}

// 新: VFS 経由で自動ルーティング
vfs::read_file("/proc/meminfo")  // → ProcFs に委譲
vfs::read_file("/HELLO.TXT")     // → Fat32 に委譲
```

#### MountEntry のファクトリ関数パターン

最初は `Arc<dyn FileSystem>` を保持する設計を考えたが、Fat32 は内部に mutable state を持つため、Mutex で包む必要が出てくる。しかし VFS 自体も Mutex で保護しているので、二重ロック → デッドロックのリスクがある。

解決策として、MountEntry にはファクトリ関数（`Box<dyn Fn() -> Box<dyn FileSystem>>`）を持たせ、`resolve()` 時に毎回インスタンスを生成する方式にした。Fat32 はステートレス（virtio-blk に直接アクセスする）なので、これで問題ない。VFS の Mutex は resolve 後すぐ解放されるので、デッドロックの心配もない。

#### パス解決の流れ

1. `normalize_path()` でパスを正規化（先頭 `/` 確保、末尾 `/` 除去、`..` 対策）
2. マウントテーブルを走査して最長一致するマウントポイントを見つける
3. マウントポイントのプレフィックスを除去して、FileSystem に相対パスを渡す

例: `/proc/meminfo` → マウントポイント `/proc` にマッチ → ProcFs に `"meminfo"` を渡す

#### ルートディレクトリの仮想エントリ

`list_dir("/")` では、Fat32 の実エントリに加えて、マウントポイント（`proc/`）を仮想的に追加する。ユーザーから見ると `/proc` が自然にディレクトリとして見える。

### 変更箇所

全 7 ステップで実施:

| ファイル | 変更内容 |
|---------|---------|
| `vfs.rs` | VfsManager、MountEntry、グローバルシングルトン、public API 追加 |
| `fat32.rs` | `new_fs()` コンストラクタ、`FileSystem::read_file()` オーバーライド追加 |
| `procfs.rs` | パス処理簡素化（VFS がプレフィックス除去済み前提に） |
| `main.rs` | `vfs::init()` 追加、INIT.ELF 読み込みを VFS 経由に |
| `handle.rs` | close() のフラッシュを VFS 経由に |
| `syscall.rs` | 11 箇所の Fat32 直接呼び出し + /proc 分岐を除去 |
| `shell.rs` | 6 コマンド（ls/cat/write/rm/run/spawn）を VFS 経由に |

### トラブルシューティング

#### OOM（メモリ不足）

最初のテストで INIT.ELF（1MB 超）の読み込みで OOM が発生した。原因は二重メモリ確保:

1. `Fat32FileSystem::open()` がファイル全体をメモリに読み込む（Fat32File に保持）
2. `vfs::read_file()` がさらに別のバッファを確保してコピー

ピーク時にファイルサイズの 2 倍のメモリを消費していた。

対策: `FileSystem` trait に `read_file()` メソッドを追加し、Fat32 の実装では `Fat32Fs::read_file()` を直接呼ぶようにした。open → read → copy のパスを通らず、一回のメモリ確保で済む。

#### メソッド名の衝突

`FileSystem::read_file()` を trait に追加したところ、Fat32 の `open()` 内の `fs.read_file(path)` が inherent メソッド（`&mut self`）ではなく trait メソッド（`&self`）に解決されるようになった。

Rust の名前解決では、trait メソッドが優先される場合がある。修正は完全修飾構文（Fully Qualified Syntax）で明示的に inherent メソッドを呼ぶようにした:

```rust
// NG: trait の read_file に解決される
let data = fs.read_file(path)?;

// OK: inherent の read_file を明示的に呼ぶ
let data = Fat32Fs::<KernelBlockDevice>::read_file(&mut fs, path)?;
```

#### handle_create_file テスト失敗

`open_path_to_handle()` で書き込み可能かどうかを `node.write(0, &[])` で判定していたが、`Fat32File::write()` は常に `NotSupported` を返す（ハンドルシステムはメモリ上のバッファに書き込み、close 時に FAT32 にフラッシュする方式）。そのため全 Fat32 ファイルが書き込み拒否されていた。

修正: `/proc` パスかどうかでリードオンリー判定するシンプルな方式に変更。

### テスト結果

```
make test → 46/47 PASSED（network_dns のみ FAIL、既知のフレーキーテスト）
```

selftest の procfs テストも VFS 経由に書き換え済み。`vfs::list_dir("/proc")` と `vfs::read_file("/proc/meminfo")` が正しく動作することを確認。

## 振り返り

### 良かった点

- 計画をしっかり立ててから実装に入ったので、7 ステップを順調に進められた
- OOM やメソッド名衝突など想定外の問題にも対処できた
- Fat32 直接呼び出し + /proc 分岐が消えて、コードがすっきりした

### 学んだこと

- **ファクトリ関数パターン**: Mutex 内に mutable な FileSystem を持つ代わりに、ファクトリ関数で毎回インスタンス生成。デッドロック回避の実用的なパターン
- **Rust の完全修飾構文**: inherent メソッドと trait メソッドが同名の場合、`Type::method(&self)` で明示的に呼び分けられる
- **VFS のマウント設計**: 最長一致でマウントポイントを解決するのは Linux と同じアプローチ。シンプルだが拡張性がある

### 開発サイクルの改善点

- 大きな変更は計画を書いてからやると効率が良い。「ファイルを1つずつ変えてビルド確認」のサイクルが安定していた
- OOM のデバッグに時間がかかった。メモリプロファイリングの仕組み（ピーク使用量の表示等）があると便利かもしれない

## SYS_WAITPID 実装

### 動機

パイプラインで複数の子プロセスを同時起動した場合、従来の `SYS_WAIT(34)` では exit_code しか返されず、**どの子プロセスが終了したか分からない**という問題があった。init の supervisor_loop でも同じ問題を TODO コメントで残していた。

また、WNOHANG（ノンブロッキングモード）がないため、子プロセスの終了をポーリングする手段がなかった。

### 設計

```
SYS_WAITPID(target_task_id, exit_code_ptr, flags) -> child_task_id
```

SYS_WAIT との違い:
- **戻り値が child_task_id**: どの子が終了したか分かる
- **exit_code はポインタ経由**: ユーザー空間のメモリに直接書き込む
- **WNOHANG フラグ**: 終了済みの子がいなければ即座に 0 を返す

引数は 3 つなので `syscall3` で収まる（構造体不要）。syscall 番号はコンソール I/O 範囲（0-9）の空き番号 7 を使った。

### 実装

scheduler.rs の `waitpid()` は `wait_for_child()` とほぼ同じロジックだが、タプル `(child_task_id, exit_code)` を返す。WNOHANG の場合はループせずに `Ok((0, 0))` を返す。

テスト用に `exec_spawn_for_test()` を追加した。これは exec_by_path と同じロジックだが wait_for_child を呼ばず、spawn だけして task_id を返す。selftest では EXIT0.ELF を spawn → waitpid で回収し、child_id と exit_code が正しいことを確認する。

### テスト結果

```
selftest: 49/49 テスト中 48 PASS（waitpid PASS、network_dns のみ既知のフレーキー FAIL）
```

## シェルパイプの改善

ここからが今日の本題。パイプライン周りに 3 つの改善を行う。

### 改善 1: waitpid 統合

パイプラインの子プロセス待ちを `wait` → `waitpid` に切り替え、init の supervisor_loop も waitpid で「どのサービスが終了したか」を正確に判定できるようにする。

### 改善 2: パイプ対応コマンド拡充

現状、パイプラインで使えるビルトインは echo/cat/sed/grep だけ。ls/ps/mem/dns などの出力系コマンドもパイプに対応させる。

### 改善 3: 暗黙 run 対応

現状 `run /FOO.ELF | grep pattern` と書かないといけないが、`/FOO.ELF | grep pattern` でも外部コマンドとして扱えるようにする。

### 実装

3 つの改善をまとめて実装した。

**waitpid 統合:**
- `user/src/bin/shell.rs` のパイプライン子プロセス待ちを `wait(task_id, 0)` → `waitpid(task_id, 0)` に変更
- `user/src/bin/init.rs` の supervisor_loop を全面書き換え。`wait(0, 0)` → `waitpid(0, 0)` にして、戻り値の child_task_id から終了したサービスを O(1) で特定。以前は exit_code しか分からなかったので全サービスを総当たりで生存確認していた

**パイプ対応コマンド拡充:**
- `pipeline_ls()`, `pipeline_ps()`, `pipeline_mem()`, `pipeline_ip()`, `pipeline_df()` の 5 つの関数を追加
- ヘルパー関数 `push_number()`, `push_padded()` も追加（`no_std` 環境では `format!` が使えないので、String に手動で数値やパディング付き文字列を追加する）
- パイプラインのビルトイン解決（builtin-only モードと external モード両方）に新コマンドを追加

**暗黙 run 対応:**
- `execute_pipeline_external()` の外部コマンド判定を `match cmd { "run" => ... }` から `if is_external` に変更
- `is_external` は `cmd == "run" || cmd.starts_with('/') || cmd.ends_with(".ELF") || cmd.ends_with(".elf")` で判定
- これにより `/FOO.ELF | grep pattern` のような自然な書き方ができるようになった

### テスト結果

```
selftest: 48/49 PASS（network_dns のみ既知のフレーキー FAIL）
```

初回実行時にタイムアウトすることがあった（handle_write テストの前でハング）が、再実行で正常に通った。タイミング依存の問題と思われる。

### 振り返り

#### 良かった点

- SYS_WAITPID をすぐにユーザー空間で活用できた。作ったシステムコールがすぐ使われるのは嬉しい
- `push_number()` / `push_padded()` ヘルパーを共通化したので、今後パイプ対応コマンドを追加するのが楽になる

#### つまずいた点

- `match` → `if/else` への変換でインデント崩れが起きてビルドエラーに。Rust は `{` の対応が厳密なので注意
- テストのタイミング依存ハングが 1 回発生。原因不明だが再実行で通った。CI で何度も失敗するようなら調査が必要

#### 次にやりたいこと

- `network_dns` のフレーキーテストを修正して、CI を安定させたい

## network_dns フレーキーテスト修正

### 問題

`network_dns` テストが不安定に失敗していた。QEMU SLIRP 経由で DNS クエリを送り、応答を待つテストだが、約半分の確率で「DNS query timeout」になる。一方、ユーザー空間 netd 経由の `network_netd_dns` は安定して成功する。

### 原因分析

`dns_lookup()` のポーリングに 2 つの問題があった:

1. **タイミングが不正確**: `for _ in 0..100000 { spin_loop(); }` で約 100ms 待つ想定だが、ホスト CPU の速度や QEMU のオーバーヘッドで実際の待ち時間が大きくずれる。結果として、30 回のループ（名目 3 秒）が実際には数百ミリ秒で終わったり、逆に数十秒かかったりする
2. **パケット処理が 1 個ずつ**: `poll_and_handle()` は 1 回の呼び出しで 1 パケットしか処理しない。ARP 応答や他のネットワークトラフィックが先にキューにあると、DNS 応答を処理する前にループが終わる

### 修正

```rust
// 修正前: spin_loop ベースの不安定なタイミング
for _ in 0..30 {
    poll_and_handle();             // 1パケットだけ
    // check...
    for _ in 0..100000 { spin_loop(); }  // 不安定な100ms
}

// 修正後: PIT タイマー基準の正確なタイムアウト
let start_tick = TIMER_TICK_COUNT.load(Relaxed);
let timeout_ticks = 5 * 182 / 10;  // 5秒
loop {
    for _ in 0..64 { poll_and_handle(); }  // 最大64パケット一括処理
    // check...
    if now - start_tick >= timeout_ticks { break; }
    for _ in 0..10000 { spin_loop(); }  // 短い待機
}
```

- PIT タイマーティック（約 55ms/tick）を使うことで、ホスト CPU 速度に依存しない正確なタイムアウトを実現
- 受信キューを最大 64 パケット一括処理することで、他のパケットに埋もれる問題を解消

しかし、これだけでは完全に安定しなかった。5 回に 1 回程度まだ失敗する。

### 根本原因の発見: netd とのパケット取得レース

さらに調査を進めると、本当の原因が判明した。netd（ユーザー空間のネットワークデーモン）が `SYS_NET_RECV_FRAME` syscall で virtio-net の受信キューからパケットを取得する。カーネルの `poll_and_handle()` も同じ受信キューからパケットを取る。つまり、**カーネルの dns_lookup と netd がパケットの取り合い**をしている。

```
DNS 応答パケット → virtio-net 受信キュー
                          ↓
    ┌─────────────────┐   ↓   ┌──────────────────────┐
    │ netd             │ ← → │ kernel dns_lookup     │
    │ sys_net_recv_frame│       │ poll_and_handle()     │
    └─────────────────┘       └──────────────────────┘

    どちらが先に取るかはスケジューリング次第（レース条件）
```

netd が先に DNS 応答を取ると、カーネル側の `poll_and_handle()` は永遠にレスポンスを受け取れない。

### 最終修正: DNS クエリのリトライ

根本的にはパケット受信のアーキテクチャ問題（カーネルとユーザー空間が同じ受信キューを共有）だが、これを解決するにはネットワークスタックの大幅な再設計が必要。現実的な解として、**DNS クエリを最大 3 回リトライする**方式にした。

```rust
for attempt in 0..3 {
    // レスポンスバッファクリア
    // クエリ送信
    // 各試行 2 秒タイムアウトで応答待ち
    // 受信できたら即返す
}
```

- 1 回のレースに負けても次の試行で成功する
- 3 回全部失敗する確率は極めて低い（netd が 3 回連続で DNS 応答を横取りすることはほぼない）
- 合計最大 6 秒のタイムアウト（実際はほとんど 1 回目で成功）

### テスト結果

```
5 回連続で 49/49 PASS（network_dns 含む）
```

以前は約 50% の確率で失敗していた `network_dns` が完全に安定した。
