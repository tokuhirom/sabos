# 2026-02-18: net_poller カーネルタスク導入 — TCP accept 競合の根本解決

## 今日の目標

昨日発見した httpd と telnetd の TCP accept 競合問題を根本的に解決したい。両方のサービスが `tcp_accept` を呼ぶと、それぞれが `poll_and_handle_timeout()` でパケットを取り合い、telnetd が接続を accept できなくなるという問題だった。今日は専用カーネルタスク `net_poller` を導入して、パケット処理を1箇所に集約する。

## やったこと

### 問題の根本原因

各 syscall（`tcp_accept`, `tcp_recv`, `dns_lookup` 等）が個別にパケット受信・処理を行う設計が問題だった。httpd と telnetd が同時にポーリングすると、SYN パケットを httpd が食べてしまい telnetd に届かない、といった競合が発生する。

```
【Before: 各 syscall が個別にポーリング】
httpd:    tcp_accept → poll_and_handle_timeout → recv_frame → handle_packet
telnetd:  tcp_accept → poll_and_handle_timeout → recv_frame → handle_packet
  → パケットの取り合いが発生！

【After: net_poller が一括処理、syscall は結果を待つだけ】
net_poller:  recv_frame → handle_packet → wake_all_net_waiters (ループ)
httpd:       tcp_accept → wait_net_condition(check: pending_accept?)
telnetd:     tcp_accept → wait_net_condition(check: pending_accept?)
  → パケット処理は net_poller だけ。syscall はイベント通知を待つ
```

### net_poller カーネルタスクの実装

`kernel/src/netstack.rs` に以下を追加した。

#### NetState に waiter リストを追加

```rust
net_waiters: Vec<u64>,  // ネットワークイベントを待つタスク ID のリスト
```

ネットワークイベントを待っているタスクを管理するためのリスト。`register_net_waiter()` で登録、`unregister_net_waiter()` で解除する。

#### wait_net_condition — 汎用ネットワークイベント待機関数

```rust
fn wait_net_condition<T, F>(timeout_ms: u64, check_fn: F) -> Option<T>
where F: Fn() -> Option<T>
```

これが今回の実装の中核。ジェネリックな条件チェック関数 `check_fn` を受け取り、条件が満たされるかタイムアウトするまで sleep/wake ループで待機する。

動作フロー:
1. 即座チェック → 成立なら即 return
2. `timeout_ms == 0` なら None を返す（非ブロッキング）
3. waiter 登録 → sleep → wake → チェック → のループ
4. タイムアウトで None、条件成立で Some(T)

#### net_poller_task — パケット処理の中央集権化

```rust
pub fn net_poller_task() {
    loop {
        while let Some(frame) = recv_frame_nonblocking() {
            handle_packet(&frame);
            received = true;
        }
        if received { wake_all_net_waiters(); }
        kick_virtio_net();
        enable_and_hlt();  // QEMU SLIRP に処理時間を与える
    }
}
```

`enable_and_hlt()` は CPU を停止して次の割り込み（タイマー、ネットワーク等）を待つ命令。QEMU の SLIRP ネットワークバックエンドにパケット処理の時間を与えるために重要。

#### 7つの syscall を置き換え

| 関数 | timeout | 確認内容 |
|------|---------|---------|
| `dns_lookup` | 5000ms | UDP レスポンスにマッチする query_id |
| `tcp_connect` | 5000ms | connection が Established or Closed |
| `tcp_accept` | 引数 | `tcp_pending_accept` にポートが一致するエントリ |
| `tcp_recv` | 引数 | `recv_buffer` が非空 or CloseWait/Closed |
| `tcp_close` | 5000ms | connection が TimeWait or Closed |
| `udp_recv_from` | 引数 | socket の `recv_queue.pop_front()` |
| `wait_icmpv6_echo_reply` | 引数 | `icmpv6_echo_reply.take()` |

全て `wait_net_condition` に統一され、`poll_and_handle_timeout` と `drain_frames` は削除した。

### timeout=0 の非ブロッキング動作が重要だった

最初の実装では `timeout=0` に対してフォールバックタイムアウト（tcp_accept→100ms、tcp_recv→5000ms）を設定していた。これが大きなバグだった。

telnetd は以下のようなポーリングイベントループを持っている:

```rust
loop {
    // 新規接続チェック（非ブロッキング、timeout=0）
    if let Ok(conn_id) = tcp_accept(port, 0) { ... }
    // 既存接続のデータチェック（非ブロッキング、timeout=0）
    if let Ok(data) = tcp_recv(conn_id, 0) { ... }
    sleep(10);  // 少し待って再ループ
}
```

`timeout=0` が 100ms ブロッキングになると、このイベントループが詰まってしまい、新規接続も既存接続のデータ読み取りもできなくなる。修正後は `timeout_ms == 0` のとき即座にチェック＆復帰するようにした。

### httpd の復帰

パケット競合が解消されたので、`user/src/bin/init.rs` の SERVICES に httpd を追加してデフォルト起動に復帰させた。また `kernel/src/shell.rs` の selftest に `httpd_service` テストを有効化した。

### テスト結果

- カーネル selftest: **47/47 PASSED**（httpd_service, telnetd_service 含む）
- telnet テスト: **5/5 PASSED**（mkdir, ls, selftest_net, hellostd, cat）

httpd と telnetd が同時に動作し、両方のサービステストが通るようになった！

## 学び

### イベント駆動 vs ポーリングの設計

今回の問題は「各 syscall が個別にポーリングする」というアーキテクチャの限界だった。サービスが1つしかない間は動くが、複数サービスが同じリソース（NIC）を共有すると破綻する。

解決策は「受信処理を1箇所に集約し、関心のあるタスクにイベント通知する」というイベント駆動パターン。Linux の NAPI（New API）も同じ発想で、ネットワークドライバがパケットを受信して上位レイヤーに配送する構造になっている。

### 非ブロッキング API の重要性

`timeout=0` は「今すぐチェックして結果がなければ即座に返る」という意味。これをブロッキングにしてしまうと、イベントループベースのサーバー（telnetd, httpd）が正常動作しなくなる。API の契約（timeout=0 → 非ブロッキング）を厳密に守ることが大事。

### wake/sleep パターン

`wait_net_condition` は Linux の `wait_event_timeout` に似た仕組み。条件関数を渡して、条件成立かタイムアウトまで寝て待つ。waiter リストへの登録・解除を確実に行わないとリークする（unregister を忘れると永遠に起床通知が飛ぶ）。

### IPC recv の timeout=0 バグ（潜在バグの発見）

net_poller 導入後、telnetd が接続を accept できなくなった。調査の結果、原因は net_poller ではなく IPC の `recv()` にあった。

`ipc_recv(0)` は telnetd のポーリングループで「メッセージがあれば受け取る、なければすぐ返る」という非ブロッキング呼び出しのつもりだったが、実装上は `timeout_ms=0 → calc_wake_at(0) → u64::MAX → 無期限待ち` になっていた。

以前は `tcp_accept` がパケット処理をインラインで行っていたため、accept のタイミングで接続が取れていた。ipc_recv(0) でブロックしても、次の接続が来たタイミングで tsh からの IPC メッセージが起床させていた。net_poller でパケット処理が非同期になったことで、このレースコンディション的な動作が崩れ、問題が顕在化した。

修正: `recv()`, `recv_from()`, `recv_typed()` で `timeout_ms == 0` の場合は `try_recv()` で即座チェックし、メッセージがなければ即 `Err(Timeout)` を返す。

### テスト結果（最終）

- ユーザーランドテスト（telnet経由）: **5/5 PASSED**
- カーネル selftest: **50/50 PASSED**（httpd_service, telnetd_service 含む）

### 9P ファイルシステム経由の spawn が失敗するバグの修正

`run /9p/user/target/.../selftest_net` のように 9P マウント経由で ELF バイナリを spawn すると失敗するバグを発見・修正した。

#### 原因: walk() の off-by-one バグ

`kernel/src/virtio_9p.rs` の `walk()` 関数で、ループ条件が `while start <= components.len()` になっていた。これは off-by-one エラーで、パスの全コンポーネントを処理した後に余分な 1 イテレーションが走っていた。

9P プロトコルの walk（Twalk）は、パスのコンポーネントを辿って新しい fid（ファイル識別子）を割り当てる操作。1 回の Twalk で最大 16 コンポーネントを辿れる。nwname=0（コンポーネント数ゼロ）で送ると fid のクローン操作になる。

具体例: `walk("user/target/x86_64-unknown-none/debug/selftest_net")` の場合

```
【バグ動作】
イテレーション 1: start=0, components=5個 → 全5コンポーネントを walk → new_fid に割り当て → start=5
イテレーション 2: start=5, 5 <= 5 → true（バグ！）
  → 空チャンク（nwname=0）で同じ new_fid への再 walk を送信
  → サーバーが「この fid は既に使用中」とエラーを返す
  → walk 失敗 → read_file 失敗 → spawn 失敗
```

#### なぜ既存テストで検出されなかったか

カーネル selftest の `9p_read` テストは `list_dir("/9p")` しか行っておらず、VFS 解決後の相対パスは空文字列（""）だった。空パスの場合は `components.is_empty()` チェックで `break` するため、バグが発動しなかった。

つまり、9P のディレクトリ一覧は動くがファイル読み込みは壊れているという状態がずっと隠れていた。

#### 修正内容

1. 空パスのケース（fid クローン）をループ前に分離して早期 return
2. ループ条件を `while start < components.len()` に修正
3. ループ末尾の `components.is_empty()` チェック（空パス用の旧ワークアラウンド）を除去

```rust
// 修正前
while start <= components.len() {  // ← off-by-one!
    ...
    start = end;
    if components.is_empty() { break; }
}

// 修正後
if components.is_empty() {
    // 空パス = root_fid をクローン（専用処理）
    ...
    return Ok((new_fid, last_qid));
}
while start < components.len() {  // ← 正しい条件
    ...
    start = end;
}
```

#### テスト結果

- カーネル selftest: **50/50 PASSED**（9p_read 含む）
- 9P 経由 spawn: **動作するようになった**（selftest_net が起動し `net_addr_types` PASS を確認）

## 変更ファイル

- `kernel/src/netstack.rs` — net_poller, wait_net_condition, waiter 管理、7 syscall の置き換え
- `kernel/src/main.rs` — net_poller タスクの spawn
- `kernel/src/shell.rs` — httpd_service テストの有効化
- `user/src/bin/init.rs` — httpd をデフォルトサービスに追加
- `kernel/src/ipc.rs` — timeout=0 を非ブロッキングに修正（recv, recv_from, recv_typed）
- `kernel/src/syscall.rs` — IPC recv のコメント修正
- `kernel/src/virtio_9p.rs` — walk() の off-by-one バグ修正（9P 経由 spawn が失敗する原因）

---

## Phase 0: 実機対応の基盤整備（ACPI + APIC + PCI改善 + 例外ハンドラ）

午後から実機対応の基盤整備に着手した。SABOS は QEMU 上で安定動作するが、実機では割り込みルーティング（Legacy PIC のみ）やストレージ（virtio-blk のみ）の制約で起動すら困難。Phase 0 として QEMU 上でテスト可能な範囲の改善を行った。

### Step 1: CPU 例外ハンドラの追加

IDT に不足していた 11 個の例外ハンドラを追加した。

| ベクタ | 名称 | 用途 |
|--------|------|------|
| 2 | #NMI | HW ウォッチドッグ、メモリエラー |
| 4 | #OF (Overflow) | INTO 命令 |
| 5 | #BR (Bound Range) | BOUND 命令 |
| 7 | #NM (Device Not Available) | x87 FPU |
| 10 | #TS (Invalid TSS) | タスク切り替え時 TSS 不正 |
| 11 | #NP (Segment Not Present) | セグメントの P ビット 0 |
| 12 | #SS (Stack Segment Fault) | スタックセグメントエラー |
| 16 | #MF (x87 FP Exception) | x87 演算エラー |
| 17 | #AC (Alignment Check) | アライメント違反 |
| 18 | #MC (Machine Check) | 致命的 HW エラー（diverging） |
| 19 | #XM (SIMD FP Exception) | SSE/AVX 演算エラー |

これまでは未登録の例外が発生するとダブルフォルト→トリプルフォルト→無言の再起動になっていた。実機では #NMI（メモリエラー）や #MC（ハードウェア障害）が起きうるので、原因を表示して停止できるようにした。

### Step 2: RSDP アドレスの取得

ACPI テーブルのルート（RSDP: Root System Description Pointer）は UEFI の Configuration Table から取得する。`exit_boot_services()` の後は UEFI サービスが使えなくなるため、その直前に取得して `static AtomicU64` に保存する設計にした。

ACPI 2.0 (ACPI2_GUID) を優先し、なければ ACPI 1.0 (ACPI_GUID) にフォールバックする。QEMU + OVMF では ACPI 2.0 の RSDP が `0xfb7e014` に見つかった。

`uefi::system::with_config_table()` のクロージャが `Fn`（不変参照キャプチャ）なので、ローカル変数への代入ができない。`AtomicU64` に直接 store することで回避した。

### Step 3: ACPI テーブルパース

`acpi` crate (v5.0) を使って RSDP → RSDT/XSDT → MADT（Multiple APIC Description Table）をパースする。SABOS はアイデンティティマッピング（物理アドレス = 仮想アドレス）なので、`AcpiHandler` の `map_physical_region()` / `unmap_physical_region()` は no-op で実装できた。

v6 系は `aml` feature のコンパイルエラーがあったので v5 系を使用。`acpi` crate は `no_std + alloc` に対応しており、ヒープさえあれば使える。

パース結果:
- Local APIC: `0xFEE00000`（x86 の標準アドレス）
- I/O APIC #0: `0xFEC00000`（GSI base 0）
- Legacy PIC: あり

### Step 4: Local APIC + I/O APIC 初期化

ここが今回の核心。PIC (8259) から APIC に割り込みコントローラを移行した。

**APIC とは何か**: APIC (Advanced Programmable Interrupt Controller) は PIC の後継。Local APIC は各 CPU コアに内蔵されたタイマー＋割り込み配送器で、I/O APIC は外部デバイスからの IRQ を Local APIC にルーティングする中継器。PIC は IRQ が 16 本しかないが、APIC は 24+ 本に拡張され、マルチプロセッサや MSI/MSI-X にも対応する。

**初期化手順**:
1. PIC を全マスク（0xFF, 0xFF）して無効化
2. `x2apic` crate の `LocalApicBuilder` で Local APIC を初期化（タイマー vec=32, エラー vec=0xFE, スプリアス vec=0xFF, Periodic モード）
3. I/O APIC を初期化（offset=32、IRQ1=キーボード、IRQ12=マウスを有効化）
4. `IS_APIC_ACTIVE` フラグを true に設定

**EOI の切り替え**: 割り込みハンドラの EOI（End Of Interrupt）送信を `eoi()` 関数に統一した。APIC active なら Local APIC の EOI レジスタ（base+0xB0）に直接書き込み、そうでなければ PIC に送る。割り込みハンドラ内で Mutex を取るとデッドロックの危険があるため、APIC EOI は Mutex なしの直接 MMIO 書き込みにした。

**APIC タイマーの調整で苦労した**: 最初 Div128 / initial=10,000,000（割り込み間隔 1.28秒）にしたら、スケジューリングが遅すぎて telnet テストがタイムアウトした。次に Div16 / initial=625,000（10ms）にしたら、今度は高頻度すぎてオーバーヘッドでユーザーシェルが30秒以内に起動できなくなった。最終的に Div64 / initial=1,000,000（64ms ≈ 15.6Hz）で PIT の 18.2Hz と同等の頻度に落ち着いた。

QEMU のバスクロックが 1GHz という前提で計算しているが、実機ではバスクロックが異なるため PIT キャリブレーションが必要になる（Phase 1 以降の課題）。

### Step 5: PCI マルチバス列挙

既存の `enumerate_bus()` はバス 0 のみスキャンしていた。実機では NVMe コントローラが PCIe ブリッジの配下（バス 1 以降）にあることがある。

`scan_bus()` を再帰実装に変更した。各デバイスのクラスコードを見て PCI-to-PCI ブリッジ（class=0x06, subclass=0x04）を検出すると、ブリッジの secondary bus 番号を読み取って再帰スキャンする。深さ制限は 8 階層（PCI 仕様上バスは 256 本まで）。

その他の追加:
- `read_bar64()`: 64bit BAR 対応（4GB 以上のアドレスを使うデバイス向け）
- `enumerate_capabilities()`: PCI ケイパビリティリスト走査（MSI/MSI-X の検出用、Phase 0 では検出のみ）

### Step 6: selftest 追加と結果

3 テストを追加して **52/52 全 PASS**:

| テスト名 | 検証内容 |
|----------|---------|
| `acpi_detect` | ACPI テーブルから APIC 情報が取得できること |
| `apic_active` | PIC から APIC への移行が完了していること |
| `pci_multibus` | マルチバス列挙がバス 0 のデバイスを包含すること |

ユーザーランドテスト（telnet 経由）も 5/5 PASSED。APIC 移行後もタイマー、キーボード、マウス、ネットワーク全て正常動作。

### 使用した外部クレート

- `acpi` v5.0 — ACPI テーブルパース（no_std + alloc 対応）。`AcpiTables::from_rsdp()` → `PlatformInfo::new()` で MADT の APIC 情報を取得
- `x2apic` v0.5 — Local APIC / I/O APIC 制御（no_std 対応）。`LocalApicBuilder` パターンで初期化、`IoApic::new()` で I/O APIC 設定

## 変更ファイル（Phase 0）

- `kernel/src/interrupts.rs` — 11 個の CPU 例外ハンドラ追加、`eoi()` 関数で PIC/APIC 切り替え
- `kernel/src/main.rs` — RSDP 取得、ACPI 初期化、APIC 初期化の呼び出し
- `kernel/src/acpi.rs` — 新規。ACPI テーブルパース、APIC 情報取得
- `kernel/src/apic.rs` — 新規。Local APIC + I/O APIC 初期化
- `kernel/src/pci.rs` — マルチバス列挙、64bit BAR、ケイパビリティリスト走査
- `kernel/src/shell.rs` — selftest 3 件追加（acpi_detect, apic_active, pci_multibus）
- `kernel/Cargo.toml` — acpi v5.0, x2apic v0.5 追加

---

## Phase 1-1: AHCI (SATA) ドライバ実装

Phase 0 の基盤整備（ACPI + APIC + PCI改善 + 例外ハンドラ、52/52 PASS）が完了したので、次は実機ブートの要となる AHCI ドライバに取りかかった。AHCI は SATA デバイスを制御するための標準インターフェースで、Intel ICH/PCH シリーズのオンボード SATA コントローラがこれに該当する。実機では virtio-blk が存在しないため、AHCI (SATA) か NVMe でストレージにアクセスするしかない。

### AHCI の仕組み

AHCI HBA (Host Bus Adapter) は PCI デバイスとして存在し、BAR5 (ABAR) に MMIO レジスタがマップされる。HBA には最大 32 ポートがあり、各ポートに 1 台の SATA デバイスが接続される。コマンド発行の流れは以下の通り:

1. **Command List** にコマンドヘッダーを設定（物理アドレス、FIS 長、PRDT 数など）
2. **Command Table** に ATA コマンド FIS と PRDT (Physical Region Descriptor Table) を設定
3. ポートの **CI (Command Issue)** レジスタにビットをセットしてコマンドを発行
4. CI ビットがクリアされるまでポーリングして完了を待つ

FIS (Frame Information Structure) は SATA のフレーム形式で、Register H2D FIS（ホスト→デバイス）を使って ATA コマンドを送信する。PRDT は DMA 転送先の物理メモリ領域を記述するテーブル。AHCI の肝は「全てが物理アドレスベースの DMA」であること。CPU がデータをコピーするのではなく、HBA が自律的にメインメモリとの間でデータ転送を行う。

### 実装した内容

**`kernel/src/ahci.rs`** を新規作成（約 900 行）。主な構造体とコマンド:

- **HbaMemory**: HBA 全体のグローバルレジスタ（CAP, GHC, PI, VS など）
- **HbaPort**: ポートごとのレジスタ（CLB, FB, CMD, TFD, SIG, SSTS, CI など）
- **HbaCmdHeader**: コマンドリストの各スロット（32 バイト、CFL/W/PRDTL/CTBA など）
- **HbaCmdTable**: コマンド FIS + PRDT エントリ
- **FisRegH2D**: Register H2D FIS（ATA コマンド送信用、20 バイト）
- **IDENTIFY DEVICE** (0xEC): デバイスの容量（48-bit LBA）とモデル名を取得
- **READ DMA EXT** (0x25) / **WRITE DMA EXT** (0x35): セクタの読み書き

AHCI は virtio-blk と比べてかなり複雑だった。virtio-blk はディスクリプタ 3 つ繋げて notify するだけだが、AHCI は Command List → Command Table → FIS → PRDT という多段構造を全て物理アドレスで設定する必要がある。しかも各構造体にアライメント制約がある（Command List は 1KB、Command Table は 128B、FIS は 256B）。

### BlockBackend enum の導入

`kernel/src/fat32.rs` の `KernelBlockDevice` に `BlockBackend` enum を追加して、virtio-blk と AHCI を切り替えられるようにした。

```rust
pub enum BlockBackend {
    VirtioBlk(usize),  // virtio-blk デバイス index
    Ahci(usize),       // AHCI デバイス index
}
```

VFS の `/` は従来通り virtio-blk[0]（disk.img）、AHCI デバイスが見つかれば `/ahci` として追加マウントする構成。既存の virtio-blk テストを壊さずに AHCI 対応を追加できた。

### QEMU 設定

QEMU コマンドラインに AHCI デバイスを追加:

```bash
-device ahci,id=ahci0
-drive if=none,format=raw,file=ahci-test.img,id=ahci-disk0
-device ide-hd,drive=ahci-disk0,bus=ahci0.0
```

`ahci-test.img` は 64MB の FAT32 イメージ。Makefile で自動作成する。既存の virtio-blk デバイス 2 台はそのまま維持しているので、1 つの QEMU インスタンスに virtio-blk × 2 + AHCI × 1 = 3 ブロックデバイスが接続される。

### テスト結果

2 テストを追加して **54/54 全 PASS**:

| テスト名 | 検証内容 |
|----------|---------|
| `ahci_detect` | AHCI コントローラが PCI で検出できること |
| `ahci_read` | AHCI デバイスのセクタ 0 の FAT32 ブートシグネチャ (0x55AA) を確認 |

既存の virtio-blk/FAT32 関連テストも全て PASS。共存が正しく動いている。

### unsafe fn と Rust 2024 Edition の罠

`init_port()` と `stop_port()` を `unsafe fn` として定義したら、関数内の個々の `core::ptr::read_volatile()` / `core::ptr::write_volatile()` が「unsafe block が必要」という警告を大量に出した。

Rust 2024 Edition では `unsafe fn` の中でも個々の unsafe 操作に `unsafe {}` ブロックが必要になった（以前は `unsafe fn` 内は暗黙的に unsafe だった）。対処として `unsafe fn` をやめて通常の `fn` にし、関数内部全体を 1 つの `unsafe {}` ブロックで囲む方式にした。OS ドライバコードでは MMIO 操作が大量にあるため、この方が見通しが良い。

### 変更ファイル

- `kernel/src/ahci.rs` — 新規。AHCI ドライバ本体（約 900 行）
- `kernel/src/pci.rs` — `find_ahci_controllers()` 追加
- `kernel/src/fat32.rs` — `BlockBackend` enum 導入、全メソッドを backend 対応に変更
- `kernel/src/vfs.rs` — AHCI デバイスを `/ahci` にマウント
- `kernel/src/main.rs` — `mod ahci;` + `ahci::init()` 追加
- `kernel/src/shell.rs` — `ahci_detect`, `ahci_read` テスト追加
- `scripts/run-qemu.sh` — AHCI デバイスの QEMU オプション追加
- `Makefile` — `ahci-test.img` 作成ルール追加
- `.gitignore` — `ahci-test.img` 追加
- `docs/spec/real-hardware-roadmap.md` — Phase 1-1 完了マーク

---

## 開発サイクルの振り返り

APIC タイマーの初期カウント値でハマった。「QEMU のバスクロックがいくつか」を知らないまま勘で値を決めたのが原因。今後は PIT を使ったキャリブレーション（PIT のカウンタを使って APIC タイマーの 1tick がどのくらいの時間に相当するか計測する）を実装すれば、QEMU でも実機でも正確な周波数設定ができる。

また、acpi crate の v6 系がコンパイルできない問題に遭遇した。no_std 環境で使うクレートは version pinning が重要。v5 系は安定して動作した。

---

## Phase 1-3: NVMe ドライバ実装

AHCI ドライバの次は NVMe。最近の PC は NVMe SSD が主流なので、実機対応には避けて通れない。AHCI が完成した直後の勢いでそのまま着手した。

### NVMe と AHCI の違い

AHCI は SATA デバイスを制御するためのインターフェースで、HBA のポートにコマンドヘッダーと FIS を書き込んで CI レジスタを叩く、という流れだった。NVMe は PCIe ネイティブのストレージインターフェースで、全く異なるアーキテクチャになっている。

NVMe のキーとなる概念は **キューベース通信**:

1. **Submission Queue (SQ)**: 64 バイトのコマンドエントリ（SQE）を書き込むリングバッファ
2. **Completion Queue (CQ)**: 16 バイトの完了エントリ（CQE）を読み取るリングバッファ
3. **Doorbell レジスタ**: SQ の Tail や CQ の Head をコントローラに通知する MMIO レジスタ

```
ホスト側                    コントローラ側
  SQ に SQE を書き込み
  → SQ Tail Doorbell を更新  → コントローラがコマンドを処理
                              → CQ に CQE を書き込み
  ← CQ をポーリング（Phase Tag でチェック）
  CQ Head Doorbell を更新    → コントローラが消費を認識
```

AHCI との大きな違いは **Phase Tag** という仕組み。CQ のリングバッファが一巡するたびに Phase ビットが反転する（0→1→0→...）。CQE の status bit [0] が現在の期待 Phase と一致すれば「新しいエントリが来た」と判定する。これにより、CQ を効率的にポーリングできる。AHCI の CI ビットポーリングに相当する機能だが、NVMe の方が洗練された設計に感じる。

### キューの種類

NVMe には 2 種類のキューがある:

- **Admin Queue** (Queue ID=0): コントローラの管理用。Identify（デバイス情報取得）、I/O Queue の作成/削除など
- **I/O Queue** (Queue ID=1〜): 実際のデータ Read/Write 用

起動時はまず Admin Queue だけ作成し、Admin コマンドで I/O Queue を作成してから Read/Write ができるようになる。AHCI はポートが最初からあるが、NVMe は「キューを作る」ところからスタートする。

### 初期化シーケンス

実装した初期化手順:

1. PCI で NVMe コントローラを検出（class=0x01, subclass=0x08, prog_if=0x02）
2. BAR0 読み取り（NVMe は BAR0 に MMIO レジスタ。AHCI の BAR5 とは異なる）
3. CAP レジスタから MQES（最大キューエントリ数）と DSTRD（Doorbell Stride）を取得
4. CC.EN=0 でコントローラを無効化 → CSTS.RDY=0 を待つ
5. Admin SQ/CQ の物理メモリを確保（ページアライン）
6. AQA, ASQ, ACQ レジスタに設定
7. CC を設定して CC.EN=1 → CSTS.RDY=1 を待つ
8. Identify Controller（モデル名取得）
9. Identify Namespace 1（容量、ブロックサイズ取得）
10. Create I/O CQ → Create I/O SQ
11. Read/Write 可能な状態に

AHCI と比べてステップ数は多いが、各ステップは単純。「キューにコマンドを投入 → Doorbell を叩く → CQ をポーリング」のパターンを繰り返すだけ。

### AHCI のパターンを踏襲

既存の AHCI ドライバ（`kernel/src/ahci.rs`）と同じパターンで VFS に統合した:

- `BlockBackend::Nvme(usize)` を `fat32.rs` の enum に追加
- `read_sector` / `write_sector` の match arm に NVMe を追加
- VFS に `/nvme` マウントポイントを追加
- selftest に `nvme_detect` と `nvme_read` を追加

これは Phase 1-1 で AHCI を追加したときに `BlockBackend` enum を導入しておいたおかげ。新しいストレージバックエンドの追加が enum variant を足すだけで済む。

### selftest スクリプトの落とし穴

最初のテスト実行で nvme_detect / nvme_read が FAIL した。原因は `scripts/run-selftest.sh` が `run-qemu.sh` を使わず独自に QEMU を起動していて、NVMe デバイスのオプションが含まれていなかったため。

面白いのは AHCI テストは元から PASS していたこと。これは `-machine q35` が内蔵の ICH9 AHCI コントローラを自動で持っているから。NVMe は明示的に `-device nvme` を追加しないと存在しない。

`run-selftest.sh` に AHCI と NVMe のデバイスオプションを追加して解決した。QEMU の起動オプションが 2 箇所に分散しているのは今後のメンテナンスの問題になりそう。将来的には selftest でも `run-qemu.sh` のオプション構築関数を再利用する仕組みが欲しい。

### テスト結果

2 テストを追加して **56/56 全 PASS**:

| テスト名 | 検証内容 |
|----------|---------|
| `nvme_detect` | NVMe コントローラが PCI で検出できること |
| `nvme_read` | NVMe デバイスのセクタ 0 の FAT32 ブートシグネチャ (0x55AA) を確認 |

### 変更ファイル

- `kernel/src/nvme.rs` — 新規。NVMe ドライバ本体（約 560 行）
- `kernel/src/pci.rs` — `find_nvme_controllers()` 追加
- `kernel/src/fat32.rs` — `BlockBackend::Nvme` 追加
- `kernel/src/vfs.rs` — `/nvme` マウントポイント追加
- `kernel/src/main.rs` — `mod nvme;` + `nvme::init()` 追加
- `kernel/src/shell.rs` — `nvme_detect`, `nvme_read` テスト追加
- `scripts/run-qemu.sh` — NVMe デバイスの QEMU オプション追加
- `scripts/run-selftest.sh` — NVMe + AHCI デバイスオプション追加
- `Makefile` — `nvme-test.img` 作成ルール追加
- `.gitignore` — `nvme-test.img` 追加
- `docs/spec/real-hardware-roadmap.md` — Phase 1-3 完了マーク
