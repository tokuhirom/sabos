# 2026-02-18: net_poller カーネルタスク導入 — TCP accept 競合の根本解決

## 今日の目標

昨日発見した httpd と telnetd の TCP accept 競合問題を根本的に解決したい。両方のサービスが `tcp_accept` を呼ぶと、それぞれが `poll_and_handle_timeout()` でパケットを取り合い、telnetd が接続を accept できなくなるという問題だった。今日は専用カーネルタスク `net_poller` を導入して、パケット処理を1箇所に集約する。

## やったこと

### 問題の根本原因

各 syscall（`tcp_accept`, `tcp_recv`, `dns_lookup` 等）が個別にパケット受信・処理を行う設計が問題だった。httpd と telnetd が同時にポーリングすると、SYN パケットを httpd が食べてしまい telnetd に届かない、といった競合が発生する。

```
【Before: 各 syscall が個別にポーリング】
httpd:    tcp_accept → poll_and_handle_timeout → recv_frame → handle_packet
telnetd:  tcp_accept → poll_and_handle_timeout → recv_frame → handle_packet
  → パケットの取り合いが発生！

【After: net_poller が一括処理、syscall は結果を待つだけ】
net_poller:  recv_frame → handle_packet → wake_all_net_waiters (ループ)
httpd:       tcp_accept → wait_net_condition(check: pending_accept?)
telnetd:     tcp_accept → wait_net_condition(check: pending_accept?)
  → パケット処理は net_poller だけ。syscall はイベント通知を待つ
```

### net_poller カーネルタスクの実装

`kernel/src/netstack.rs` に以下を追加した。

#### NetState に waiter リストを追加

```rust
net_waiters: Vec<u64>,  // ネットワークイベントを待つタスク ID のリスト
```

ネットワークイベントを待っているタスクを管理するためのリスト。`register_net_waiter()` で登録、`unregister_net_waiter()` で解除する。

#### wait_net_condition — 汎用ネットワークイベント待機関数

```rust
fn wait_net_condition<T, F>(timeout_ms: u64, check_fn: F) -> Option<T>
where F: Fn() -> Option<T>
```

これが今回の実装の中核。ジェネリックな条件チェック関数 `check_fn` を受け取り、条件が満たされるかタイムアウトするまで sleep/wake ループで待機する。

動作フロー:
1. 即座チェック → 成立なら即 return
2. `timeout_ms == 0` なら None を返す（非ブロッキング）
3. waiter 登録 → sleep → wake → チェック → のループ
4. タイムアウトで None、条件成立で Some(T)

#### net_poller_task — パケット処理の中央集権化

```rust
pub fn net_poller_task() {
    loop {
        while let Some(frame) = recv_frame_nonblocking() {
            handle_packet(&frame);
            received = true;
        }
        if received { wake_all_net_waiters(); }
        kick_virtio_net();
        enable_and_hlt();  // QEMU SLIRP に処理時間を与える
    }
}
```

`enable_and_hlt()` は CPU を停止して次の割り込み（タイマー、ネットワーク等）を待つ命令。QEMU の SLIRP ネットワークバックエンドにパケット処理の時間を与えるために重要。

#### 7つの syscall を置き換え

| 関数 | timeout | 確認内容 |
|------|---------|---------|
| `dns_lookup` | 5000ms | UDP レスポンスにマッチする query_id |
| `tcp_connect` | 5000ms | connection が Established or Closed |
| `tcp_accept` | 引数 | `tcp_pending_accept` にポートが一致するエントリ |
| `tcp_recv` | 引数 | `recv_buffer` が非空 or CloseWait/Closed |
| `tcp_close` | 5000ms | connection が TimeWait or Closed |
| `udp_recv_from` | 引数 | socket の `recv_queue.pop_front()` |
| `wait_icmpv6_echo_reply` | 引数 | `icmpv6_echo_reply.take()` |

全て `wait_net_condition` に統一され、`poll_and_handle_timeout` と `drain_frames` は削除した。

### timeout=0 の非ブロッキング動作が重要だった

最初の実装では `timeout=0` に対してフォールバックタイムアウト（tcp_accept→100ms、tcp_recv→5000ms）を設定していた。これが大きなバグだった。

telnetd は以下のようなポーリングイベントループを持っている:

```rust
loop {
    // 新規接続チェック（非ブロッキング、timeout=0）
    if let Ok(conn_id) = tcp_accept(port, 0) { ... }
    // 既存接続のデータチェック（非ブロッキング、timeout=0）
    if let Ok(data) = tcp_recv(conn_id, 0) { ... }
    sleep(10);  // 少し待って再ループ
}
```

`timeout=0` が 100ms ブロッキングになると、このイベントループが詰まってしまい、新規接続も既存接続のデータ読み取りもできなくなる。修正後は `timeout_ms == 0` のとき即座にチェック＆復帰するようにした。

### httpd の復帰

パケット競合が解消されたので、`user/src/bin/init.rs` の SERVICES に httpd を追加してデフォルト起動に復帰させた。また `kernel/src/shell.rs` の selftest に `httpd_service` テストを有効化した。

### テスト結果

- カーネル selftest: **47/47 PASSED**（httpd_service, telnetd_service 含む）
- telnet テスト: **5/5 PASSED**（mkdir, ls, selftest_net, hellostd, cat）

httpd と telnetd が同時に動作し、両方のサービステストが通るようになった！

## 学び

### イベント駆動 vs ポーリングの設計

今回の問題は「各 syscall が個別にポーリングする」というアーキテクチャの限界だった。サービスが1つしかない間は動くが、複数サービスが同じリソース（NIC）を共有すると破綻する。

解決策は「受信処理を1箇所に集約し、関心のあるタスクにイベント通知する」というイベント駆動パターン。Linux の NAPI（New API）も同じ発想で、ネットワークドライバがパケットを受信して上位レイヤーに配送する構造になっている。

### 非ブロッキング API の重要性

`timeout=0` は「今すぐチェックして結果がなければ即座に返る」という意味。これをブロッキングにしてしまうと、イベントループベースのサーバー（telnetd, httpd）が正常動作しなくなる。API の契約（timeout=0 → 非ブロッキング）を厳密に守ることが大事。

### wake/sleep パターン

`wait_net_condition` は Linux の `wait_event_timeout` に似た仕組み。条件関数を渡して、条件成立かタイムアウトまで寝て待つ。waiter リストへの登録・解除を確実に行わないとリークする（unregister を忘れると永遠に起床通知が飛ぶ）。

### IPC recv の timeout=0 バグ（潜在バグの発見）

net_poller 導入後、telnetd が接続を accept できなくなった。調査の結果、原因は net_poller ではなく IPC の `recv()` にあった。

`ipc_recv(0)` は telnetd のポーリングループで「メッセージがあれば受け取る、なければすぐ返る」という非ブロッキング呼び出しのつもりだったが、実装上は `timeout_ms=0 → calc_wake_at(0) → u64::MAX → 無期限待ち` になっていた。

以前は `tcp_accept` がパケット処理をインラインで行っていたため、accept のタイミングで接続が取れていた。ipc_recv(0) でブロックしても、次の接続が来たタイミングで tsh からの IPC メッセージが起床させていた。net_poller でパケット処理が非同期になったことで、このレースコンディション的な動作が崩れ、問題が顕在化した。

修正: `recv()`, `recv_from()`, `recv_typed()` で `timeout_ms == 0` の場合は `try_recv()` で即座チェックし、メッセージがなければ即 `Err(Timeout)` を返す。

### テスト結果（最終）

- ユーザーランドテスト（telnet経由）: **5/5 PASSED**
- カーネル selftest: **50/50 PASSED**（httpd_service, telnetd_service 含む）

### 9P ファイルシステム経由の spawn が失敗するバグの修正

`run /9p/user/target/.../selftest_net` のように 9P マウント経由で ELF バイナリを spawn すると失敗するバグを発見・修正した。

#### 原因: walk() の off-by-one バグ

`kernel/src/virtio_9p.rs` の `walk()` 関数で、ループ条件が `while start <= components.len()` になっていた。これは off-by-one エラーで、パスの全コンポーネントを処理した後に余分な 1 イテレーションが走っていた。

9P プロトコルの walk（Twalk）は、パスのコンポーネントを辿って新しい fid（ファイル識別子）を割り当てる操作。1 回の Twalk で最大 16 コンポーネントを辿れる。nwname=0（コンポーネント数ゼロ）で送ると fid のクローン操作になる。

具体例: `walk("user/target/x86_64-unknown-none/debug/selftest_net")` の場合

```
【バグ動作】
イテレーション 1: start=0, components=5個 → 全5コンポーネントを walk → new_fid に割り当て → start=5
イテレーション 2: start=5, 5 <= 5 → true（バグ！）
  → 空チャンク（nwname=0）で同じ new_fid への再 walk を送信
  → サーバーが「この fid は既に使用中」とエラーを返す
  → walk 失敗 → read_file 失敗 → spawn 失敗
```

#### なぜ既存テストで検出されなかったか

カーネル selftest の `9p_read` テストは `list_dir("/9p")` しか行っておらず、VFS 解決後の相対パスは空文字列（""）だった。空パスの場合は `components.is_empty()` チェックで `break` するため、バグが発動しなかった。

つまり、9P のディレクトリ一覧は動くがファイル読み込みは壊れているという状態がずっと隠れていた。

#### 修正内容

1. 空パスのケース（fid クローン）をループ前に分離して早期 return
2. ループ条件を `while start < components.len()` に修正
3. ループ末尾の `components.is_empty()` チェック（空パス用の旧ワークアラウンド）を除去

```rust
// 修正前
while start <= components.len() {  // ← off-by-one!
    ...
    start = end;
    if components.is_empty() { break; }
}

// 修正後
if components.is_empty() {
    // 空パス = root_fid をクローン（専用処理）
    ...
    return Ok((new_fid, last_qid));
}
while start < components.len() {  // ← 正しい条件
    ...
    start = end;
}
```

#### テスト結果

- カーネル selftest: **50/50 PASSED**（9p_read 含む）
- 9P 経由 spawn: **動作するようになった**（selftest_net が起動し `net_addr_types` PASS を確認）

## 変更ファイル

- `kernel/src/netstack.rs` — net_poller, wait_net_condition, waiter 管理、7 syscall の置き換え
- `kernel/src/main.rs` — net_poller タスクの spawn
- `kernel/src/shell.rs` — httpd_service テストの有効化
- `user/src/bin/init.rs` — httpd をデフォルトサービスに追加
- `kernel/src/ipc.rs` — timeout=0 を非ブロッキングに修正（recv, recv_from, recv_typed）
- `kernel/src/syscall.rs` — IPC recv のコメント修正
- `kernel/src/virtio_9p.rs` — walk() の off-by-one バグ修正（9P 経由 spawn が失敗する原因）
