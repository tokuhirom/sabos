# 2026-02-18: net_poller カーネルタスク導入 — TCP accept 競合の根本解決

## 今日の目標

昨日発見した httpd と telnetd の TCP accept 競合問題を根本的に解決したい。両方のサービスが `tcp_accept` を呼ぶと、それぞれが `poll_and_handle_timeout()` でパケットを取り合い、telnetd が接続を accept できなくなるという問題だった。今日は専用カーネルタスク `net_poller` を導入して、パケット処理を1箇所に集約する。

## やったこと

### 問題の根本原因

各 syscall（`tcp_accept`, `tcp_recv`, `dns_lookup` 等）が個別にパケット受信・処理を行う設計が問題だった。httpd と telnetd が同時にポーリングすると、SYN パケットを httpd が食べてしまい telnetd に届かない、といった競合が発生する。

```
【Before: 各 syscall が個別にポーリング】
httpd:    tcp_accept → poll_and_handle_timeout → recv_frame → handle_packet
telnetd:  tcp_accept → poll_and_handle_timeout → recv_frame → handle_packet
  → パケットの取り合いが発生！

【After: net_poller が一括処理、syscall は結果を待つだけ】
net_poller:  recv_frame → handle_packet → wake_all_net_waiters (ループ)
httpd:       tcp_accept → wait_net_condition(check: pending_accept?)
telnetd:     tcp_accept → wait_net_condition(check: pending_accept?)
  → パケット処理は net_poller だけ。syscall はイベント通知を待つ
```

### net_poller カーネルタスクの実装

`kernel/src/netstack.rs` に以下を追加した。

#### NetState に waiter リストを追加

```rust
net_waiters: Vec<u64>,  // ネットワークイベントを待つタスク ID のリスト
```

ネットワークイベントを待っているタスクを管理するためのリスト。`register_net_waiter()` で登録、`unregister_net_waiter()` で解除する。

#### wait_net_condition — 汎用ネットワークイベント待機関数

```rust
fn wait_net_condition<T, F>(timeout_ms: u64, check_fn: F) -> Option<T>
where F: Fn() -> Option<T>
```

これが今回の実装の中核。ジェネリックな条件チェック関数 `check_fn` を受け取り、条件が満たされるかタイムアウトするまで sleep/wake ループで待機する。

動作フロー:
1. 即座チェック → 成立なら即 return
2. `timeout_ms == 0` なら None を返す（非ブロッキング）
3. waiter 登録 → sleep → wake → チェック → のループ
4. タイムアウトで None、条件成立で Some(T)

#### net_poller_task — パケット処理の中央集権化

```rust
pub fn net_poller_task() {
    loop {
        while let Some(frame) = recv_frame_nonblocking() {
            handle_packet(&frame);
            received = true;
        }
        if received { wake_all_net_waiters(); }
        kick_virtio_net();
        enable_and_hlt();  // QEMU SLIRP に処理時間を与える
    }
}
```

`enable_and_hlt()` は CPU を停止して次の割り込み（タイマー、ネットワーク等）を待つ命令。QEMU の SLIRP ネットワークバックエンドにパケット処理の時間を与えるために重要。

#### 7つの syscall を置き換え

| 関数 | timeout | 確認内容 |
|------|---------|---------|
| `dns_lookup` | 5000ms | UDP レスポンスにマッチする query_id |
| `tcp_connect` | 5000ms | connection が Established or Closed |
| `tcp_accept` | 引数 | `tcp_pending_accept` にポートが一致するエントリ |
| `tcp_recv` | 引数 | `recv_buffer` が非空 or CloseWait/Closed |
| `tcp_close` | 5000ms | connection が TimeWait or Closed |
| `udp_recv_from` | 引数 | socket の `recv_queue.pop_front()` |
| `wait_icmpv6_echo_reply` | 引数 | `icmpv6_echo_reply.take()` |

全て `wait_net_condition` に統一され、`poll_and_handle_timeout` と `drain_frames` は削除した。

### timeout=0 の非ブロッキング動作が重要だった

最初の実装では `timeout=0` に対してフォールバックタイムアウト（tcp_accept→100ms、tcp_recv→5000ms）を設定していた。これが大きなバグだった。

telnetd は以下のようなポーリングイベントループを持っている:

```rust
loop {
    // 新規接続チェック（非ブロッキング、timeout=0）
    if let Ok(conn_id) = tcp_accept(port, 0) { ... }
    // 既存接続のデータチェック（非ブロッキング、timeout=0）
    if let Ok(data) = tcp_recv(conn_id, 0) { ... }
    sleep(10);  // 少し待って再ループ
}
```

`timeout=0` が 100ms ブロッキングになると、このイベントループが詰まってしまい、新規接続も既存接続のデータ読み取りもできなくなる。修正後は `timeout_ms == 0` のとき即座にチェック＆復帰するようにした。

### httpd の復帰

パケット競合が解消されたので、`user/src/bin/init.rs` の SERVICES に httpd を追加してデフォルト起動に復帰させた。また `kernel/src/shell.rs` の selftest に `httpd_service` テストを有効化した。

### テスト結果

- カーネル selftest: **47/47 PASSED**（httpd_service, telnetd_service 含む）
- telnet テスト: **5/5 PASSED**（mkdir, ls, selftest_net, hellostd, cat）

httpd と telnetd が同時に動作し、両方のサービステストが通るようになった！

## 学び

### イベント駆動 vs ポーリングの設計

今回の問題は「各 syscall が個別にポーリングする」というアーキテクチャの限界だった。サービスが1つしかない間は動くが、複数サービスが同じリソース（NIC）を共有すると破綻する。

解決策は「受信処理を1箇所に集約し、関心のあるタスクにイベント通知する」というイベント駆動パターン。Linux の NAPI（New API）も同じ発想で、ネットワークドライバがパケットを受信して上位レイヤーに配送する構造になっている。

### 非ブロッキング API の重要性

`timeout=0` は「今すぐチェックして結果がなければ即座に返る」という意味。これをブロッキングにしてしまうと、イベントループベースのサーバー（telnetd, httpd）が正常動作しなくなる。API の契約（timeout=0 → 非ブロッキング）を厳密に守ることが大事。

### wake/sleep パターン

`wait_net_condition` は Linux の `wait_event_timeout` に似た仕組み。条件関数を渡して、条件成立かタイムアウトまで寝て待つ。waiter リストへの登録・解除を確実に行わないとリークする（unregister を忘れると永遠に起床通知が飛ぶ）。

### IPC recv の timeout=0 バグ（潜在バグの発見）

net_poller 導入後、telnetd が接続を accept できなくなった。調査の結果、原因は net_poller ではなく IPC の `recv()` にあった。

`ipc_recv(0)` は telnetd のポーリングループで「メッセージがあれば受け取る、なければすぐ返る」という非ブロッキング呼び出しのつもりだったが、実装上は `timeout_ms=0 → calc_wake_at(0) → u64::MAX → 無期限待ち` になっていた。

以前は `tcp_accept` がパケット処理をインラインで行っていたため、accept のタイミングで接続が取れていた。ipc_recv(0) でブロックしても、次の接続が来たタイミングで tsh からの IPC メッセージが起床させていた。net_poller でパケット処理が非同期になったことで、このレースコンディション的な動作が崩れ、問題が顕在化した。

修正: `recv()`, `recv_from()`, `recv_typed()` で `timeout_ms == 0` の場合は `try_recv()` で即座チェックし、メッセージがなければ即 `Err(Timeout)` を返す。

### テスト結果（最終）

- ユーザーランドテスト（telnet経由）: **5/5 PASSED**
- カーネル selftest: **50/50 PASSED**（httpd_service, telnetd_service 含む）

### 9P ファイルシステム経由の spawn が失敗するバグの修正

`run /9p/user/target/.../selftest_net` のように 9P マウント経由で ELF バイナリを spawn すると失敗するバグを発見・修正した。

#### 原因: walk() の off-by-one バグ

`kernel/src/virtio_9p.rs` の `walk()` 関数で、ループ条件が `while start <= components.len()` になっていた。これは off-by-one エラーで、パスの全コンポーネントを処理した後に余分な 1 イテレーションが走っていた。

9P プロトコルの walk（Twalk）は、パスのコンポーネントを辿って新しい fid（ファイル識別子）を割り当てる操作。1 回の Twalk で最大 16 コンポーネントを辿れる。nwname=0（コンポーネント数ゼロ）で送ると fid のクローン操作になる。

具体例: `walk("user/target/x86_64-unknown-none/debug/selftest_net")` の場合

```
【バグ動作】
イテレーション 1: start=0, components=5個 → 全5コンポーネントを walk → new_fid に割り当て → start=5
イテレーション 2: start=5, 5 <= 5 → true（バグ！）
  → 空チャンク（nwname=0）で同じ new_fid への再 walk を送信
  → サーバーが「この fid は既に使用中」とエラーを返す
  → walk 失敗 → read_file 失敗 → spawn 失敗
```

#### なぜ既存テストで検出されなかったか

カーネル selftest の `9p_read` テストは `list_dir("/9p")` しか行っておらず、VFS 解決後の相対パスは空文字列（""）だった。空パスの場合は `components.is_empty()` チェックで `break` するため、バグが発動しなかった。

つまり、9P のディレクトリ一覧は動くがファイル読み込みは壊れているという状態がずっと隠れていた。

#### 修正内容

1. 空パスのケース（fid クローン）をループ前に分離して早期 return
2. ループ条件を `while start < components.len()` に修正
3. ループ末尾の `components.is_empty()` チェック（空パス用の旧ワークアラウンド）を除去

```rust
// 修正前
while start <= components.len() {  // ← off-by-one!
    ...
    start = end;
    if components.is_empty() { break; }
}

// 修正後
if components.is_empty() {
    // 空パス = root_fid をクローン（専用処理）
    ...
    return Ok((new_fid, last_qid));
}
while start < components.len() {  // ← 正しい条件
    ...
    start = end;
}
```

#### テスト結果

- カーネル selftest: **50/50 PASSED**（9p_read 含む）
- 9P 経由 spawn: **動作するようになった**（selftest_net が起動し `net_addr_types` PASS を確認）

## 変更ファイル

- `kernel/src/netstack.rs` — net_poller, wait_net_condition, waiter 管理、7 syscall の置き換え
- `kernel/src/main.rs` — net_poller タスクの spawn
- `kernel/src/shell.rs` — httpd_service テストの有効化
- `user/src/bin/init.rs` — httpd をデフォルトサービスに追加
- `kernel/src/ipc.rs` — timeout=0 を非ブロッキングに修正（recv, recv_from, recv_typed）
- `kernel/src/syscall.rs` — IPC recv のコメント修正
- `kernel/src/virtio_9p.rs` — walk() の off-by-one バグ修正（9P 経由 spawn が失敗する原因）

---

## Phase 0: 実機対応の基盤整備（ACPI + APIC + PCI改善 + 例外ハンドラ）

午後から実機対応の基盤整備に着手した。SABOS は QEMU 上で安定動作するが、実機では割り込みルーティング（Legacy PIC のみ）やストレージ（virtio-blk のみ）の制約で起動すら困難。Phase 0 として QEMU 上でテスト可能な範囲の改善を行った。

### Step 1: CPU 例外ハンドラの追加

IDT に不足していた 11 個の例外ハンドラを追加した。

| ベクタ | 名称 | 用途 |
|--------|------|------|
| 2 | #NMI | HW ウォッチドッグ、メモリエラー |
| 4 | #OF (Overflow) | INTO 命令 |
| 5 | #BR (Bound Range) | BOUND 命令 |
| 7 | #NM (Device Not Available) | x87 FPU |
| 10 | #TS (Invalid TSS) | タスク切り替え時 TSS 不正 |
| 11 | #NP (Segment Not Present) | セグメントの P ビット 0 |
| 12 | #SS (Stack Segment Fault) | スタックセグメントエラー |
| 16 | #MF (x87 FP Exception) | x87 演算エラー |
| 17 | #AC (Alignment Check) | アライメント違反 |
| 18 | #MC (Machine Check) | 致命的 HW エラー（diverging） |
| 19 | #XM (SIMD FP Exception) | SSE/AVX 演算エラー |

これまでは未登録の例外が発生するとダブルフォルト→トリプルフォルト→無言の再起動になっていた。実機では #NMI（メモリエラー）や #MC（ハードウェア障害）が起きうるので、原因を表示して停止できるようにした。

### Step 2: RSDP アドレスの取得

ACPI テーブルのルート（RSDP: Root System Description Pointer）は UEFI の Configuration Table から取得する。`exit_boot_services()` の後は UEFI サービスが使えなくなるため、その直前に取得して `static AtomicU64` に保存する設計にした。

ACPI 2.0 (ACPI2_GUID) を優先し、なければ ACPI 1.0 (ACPI_GUID) にフォールバックする。QEMU + OVMF では ACPI 2.0 の RSDP が `0xfb7e014` に見つかった。

`uefi::system::with_config_table()` のクロージャが `Fn`（不変参照キャプチャ）なので、ローカル変数への代入ができない。`AtomicU64` に直接 store することで回避した。

### Step 3: ACPI テーブルパース

`acpi` crate (v5.0) を使って RSDP → RSDT/XSDT → MADT（Multiple APIC Description Table）をパースする。SABOS はアイデンティティマッピング（物理アドレス = 仮想アドレス）なので、`AcpiHandler` の `map_physical_region()` / `unmap_physical_region()` は no-op で実装できた。

v6 系は `aml` feature のコンパイルエラーがあったので v5 系を使用。`acpi` crate は `no_std + alloc` に対応しており、ヒープさえあれば使える。

パース結果:
- Local APIC: `0xFEE00000`（x86 の標準アドレス）
- I/O APIC #0: `0xFEC00000`（GSI base 0）
- Legacy PIC: あり

### Step 4: Local APIC + I/O APIC 初期化

ここが今回の核心。PIC (8259) から APIC に割り込みコントローラを移行した。

**APIC とは何か**: APIC (Advanced Programmable Interrupt Controller) は PIC の後継。Local APIC は各 CPU コアに内蔵されたタイマー＋割り込み配送器で、I/O APIC は外部デバイスからの IRQ を Local APIC にルーティングする中継器。PIC は IRQ が 16 本しかないが、APIC は 24+ 本に拡張され、マルチプロセッサや MSI/MSI-X にも対応する。

**初期化手順**:
1. PIC を全マスク（0xFF, 0xFF）して無効化
2. `x2apic` crate の `LocalApicBuilder` で Local APIC を初期化（タイマー vec=32, エラー vec=0xFE, スプリアス vec=0xFF, Periodic モード）
3. I/O APIC を初期化（offset=32、IRQ1=キーボード、IRQ12=マウスを有効化）
4. `IS_APIC_ACTIVE` フラグを true に設定

**EOI の切り替え**: 割り込みハンドラの EOI（End Of Interrupt）送信を `eoi()` 関数に統一した。APIC active なら Local APIC の EOI レジスタ（base+0xB0）に直接書き込み、そうでなければ PIC に送る。割り込みハンドラ内で Mutex を取るとデッドロックの危険があるため、APIC EOI は Mutex なしの直接 MMIO 書き込みにした。

**APIC タイマーの調整で苦労した**: 最初 Div128 / initial=10,000,000（割り込み間隔 1.28秒）にしたら、スケジューリングが遅すぎて telnet テストがタイムアウトした。次に Div16 / initial=625,000（10ms）にしたら、今度は高頻度すぎてオーバーヘッドでユーザーシェルが30秒以内に起動できなくなった。最終的に Div64 / initial=1,000,000（64ms ≈ 15.6Hz）で PIT の 18.2Hz と同等の頻度に落ち着いた。

QEMU のバスクロックが 1GHz という前提で計算しているが、実機ではバスクロックが異なるため PIT キャリブレーションが必要になる（Phase 1 以降の課題）。

### Step 5: PCI マルチバス列挙

既存の `enumerate_bus()` はバス 0 のみスキャンしていた。実機では NVMe コントローラが PCIe ブリッジの配下（バス 1 以降）にあることがある。

`scan_bus()` を再帰実装に変更した。各デバイスのクラスコードを見て PCI-to-PCI ブリッジ（class=0x06, subclass=0x04）を検出すると、ブリッジの secondary bus 番号を読み取って再帰スキャンする。深さ制限は 8 階層（PCI 仕様上バスは 256 本まで）。

その他の追加:
- `read_bar64()`: 64bit BAR 対応（4GB 以上のアドレスを使うデバイス向け）
- `enumerate_capabilities()`: PCI ケイパビリティリスト走査（MSI/MSI-X の検出用、Phase 0 では検出のみ）

### Step 6: selftest 追加と結果

3 テストを追加して **52/52 全 PASS**:

| テスト名 | 検証内容 |
|----------|---------|
| `acpi_detect` | ACPI テーブルから APIC 情報が取得できること |
| `apic_active` | PIC から APIC への移行が完了していること |
| `pci_multibus` | マルチバス列挙がバス 0 のデバイスを包含すること |

ユーザーランドテスト（telnet 経由）も 5/5 PASSED。APIC 移行後もタイマー、キーボード、マウス、ネットワーク全て正常動作。

### 使用した外部クレート

- `acpi` v5.0 — ACPI テーブルパース（no_std + alloc 対応）。`AcpiTables::from_rsdp()` → `PlatformInfo::new()` で MADT の APIC 情報を取得
- `x2apic` v0.5 — Local APIC / I/O APIC 制御（no_std 対応）。`LocalApicBuilder` パターンで初期化、`IoApic::new()` で I/O APIC 設定

## 変更ファイル（Phase 0）

- `kernel/src/interrupts.rs` — 11 個の CPU 例外ハンドラ追加、`eoi()` 関数で PIC/APIC 切り替え
- `kernel/src/main.rs` — RSDP 取得、ACPI 初期化、APIC 初期化の呼び出し
- `kernel/src/acpi.rs` — 新規。ACPI テーブルパース、APIC 情報取得
- `kernel/src/apic.rs` — 新規。Local APIC + I/O APIC 初期化
- `kernel/src/pci.rs` — マルチバス列挙、64bit BAR、ケイパビリティリスト走査
- `kernel/src/shell.rs` — selftest 3 件追加（acpi_detect, apic_active, pci_multibus）
- `kernel/Cargo.toml` — acpi v5.0, x2apic v0.5 追加

## 開発サイクルの振り返り

APIC タイマーの初期カウント値でハマった。「QEMU のバスクロックがいくつか」を知らないまま勘で値を決めたのが原因。今後は PIT を使ったキャリブレーション（PIT のカウンタを使って APIC タイマーの 1tick がどのくらいの時間に相当するか計測する）を実装すれば、QEMU でも実機でも正確な周波数設定ができる。

また、acpi crate の v6 系がコンパイルできない問題に遭遇した。no_std 環境で使うクレートは version pinning が重要。v5 系は安定して動作した。
